# Time Series Analysis {#sec-time-series}

Chapter 3 of @tanimura2021sql is really two chapters.
The first part of the chapter discusses some finer details of dates, date-times, and time stamps.
This material is important, but a little technical and bewildering without some concrete use cases.
This complexity is compounded by the use of DuckDB here and PostgreSQL in @tanimura2021sql, as some details work a little differently from one backend to the other.
Finally, while there are some interesting issues in using `dbplyr` with timestamp data, there probably need data and settings to fully comprehend.
As such, I have put the code related to this part of Chapter 3 of @tanimura2021sql in @sec-dates, part of the appendix.

The second part of the chapter is where @tanimura2021sql really starts to take off.
This is where it starts playing around with real data.

```{r}
#| message: false
library(DBI)
library(tidyverse)
library(dbplyr)
library(knitr)
```

## The Retail Sales Data Set {#sec-retail-data}

We read the data

```{r}
#| eval: true
#| cache: true
retail_sales_local <- read_csv("data/us_retail_sales.csv",
                               show_col_types = FALSE)
```

```{r}
#| eval: false
#| cache: true
#| include: false
url <- paste0("https://raw.githubusercontent.com/",
              "cathytanimura/sql_book/master/",
              "Chapter%203%3A%20Time%20Series%20Analysis/",
              "us_retail_sales.csv")
retail_sales_local <- read_csv(url, show_col_types = FALSE)
```

Now that we have the data in R, a few questions arise.

First, why would we move it to a database?
Second, how can we move it to a database?
Related to the previous question will be: which database to we want to move it to?

Taking these questions in turn, the first one is a good question.
With `retail_sales` as a local data frame, we can run almost all the analyses below with only the slightest modifications.
The modifications needed are to replace every instance of `window_order()` with `arrange()`.^[I requested a tweak to `dplyr` that would have avoided the need to do this, but [my request was denied](https://github.com/tidyverse/dbplyr/issues/627).
Given how awesome `dbplyr`/`dplyr` is, I cannot complain.]
The "almost all" relates to the moving average, which relies on `window_frame()` from `dbplyr`, which has no exact equivalent in `dplyr`.

So the first point is that "almost all" implies an advantage for using `dbplyr`.
On occasion, the SQL engine provided by PostgreSQL will allow us to do some data manipulations more easily than we can in R.
But of course, there are many cases when the opposite is true.
That said, why not have both?
Store data in a database and `collect()` as necessary to do things that R is better at?

Second, performance can be better using SQL.
Compiling a version of this chapter using `dplyr` with a local data frame for the remainder took just under 14 seconds.
Using a PostgreSQL backend, it took 8.5 seconds.
Adding back in the queries with `window_order()` that I removed so that I could compile with `dplyr` and using DuckDB as the backend, the document took 8.3 seconds to compile (this beat out PostgreSQL doing the same in 9.6 seconds).
While these differences are not practically very significant, they could be with more demanding tasks.
Also, a database will not always beat R or Python for performance, but often will and having the option to use a database backend is a good thing.

Third, having the data in a database allows you to interrogate the data using SQL.
If you are more familiar with SQL, or just know how to do a particular task in SQL, this can be beneficial.^[Though I will argue later that transitioning to `dplyr`/`dbplyr` is actually not difficult.]

Fourth, I think there is merit in separating the tasks of acquiring and cleaning data from the task of analysing those data.
Many data analysts have a work flow that entails ingesting and cleaning data for each analysis task.
My experience is that it is often better to do the ingesting and cleaning once and then reuse the cleaned data in subsequent analyses.
A common pattern involves reusing the same data in many analyses and it can be helpful to divide the tasks in a way that using an SQL database encourages.
Also the skills in ingesting and cleaning data can be different from those for analysing those data, so sometimes it makes sense for one person to do one task, push the data to a database, and then have someone else do some or all of the analysis.

Regarding the second question, there are a few options.
But for this chapter we will use `duckdb` 

To install DuckDB, all we have to do is `install.packages("duckdb")`.

Then we can create a connection as follows.

```{r}
db <- dbConnect(duckdb::duckdb())
```

Here we use the default of an in-memory database.
At the end of this chapter, we discuss how we could store the data in a file (say, `legislators.duckdb`) if we want persistent storage.

```{r}
retail_sales <- 
  retail_sales_local %>%
  copy_to(db, ., name = "retail_sales")
```

## Trending the Data

### Simple Trends

```{sql}
#| connection: db
SELECT sales_month, sales
FROM retail_sales
WHERE kind_of_business = 'Retail and food services sales, total'
ORDER BY 1
```

```{r}
retail_sales %>%
    filter(kind_of_business == 'Retail and food services sales, total') %>%
  select(sales_month, sales) %>%
  ggplot(aes(x = sales_month, y = sales)) +
  geom_line()
```

```{sql}
#| connection: db
SELECT date_part('year',sales_month) as sales_year,
    sum(sales) as sales
FROM retail_sales
WHERE kind_of_business = 'Retail and food services sales, total'
GROUP BY 1
;
```

```{r}
retail_sales %>%
  filter(kind_of_business == 'Retail and food services sales, total') %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(sales_year) %>%
  summarize(sales = sum(sales, na.rm = TRUE)) %>%
  ggplot(aes(x = sales_year, y = sales)) +
  geom_line()
```

```{sql}
#| connection: db
SELECT date_part('year',sales_month) as sales_year, 
  kind_of_business, sum(sales) as sales
FROM retail_sales
WHERE kind_of_business IN 
          ('Book stores',
           'Sporting goods stores',
           'Hobby, toy, and game stores')
GROUP BY 1,2
ORDER BY 1;
```

### Comparing Components

```{r}
retail_sales %>%
  filter(kind_of_business %in% 
           c('Book stores',
             'Sporting goods stores',
             'Hobby, toy, and game stores')) %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(sales_year, kind_of_business) %>%
  summarize(sales = sum(sales, na.rm = TRUE), .groups = "drop") %>%
  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +
  geom_line() +
  theme(legend.position = "top")
```

```{sql}
#| connection: db
SELECT sales_month, kind_of_business, sales
FROM retail_sales
WHERE kind_of_business IN ('Men''s clothing stores','Women''s clothing stores')
ORDER BY 1,2;
```

```{r}
retail_sales %>%
  filter(kind_of_business %in% c("Men's clothing stores",
                                 "Women's clothing stores")) %>%
  select(sales_month, kind_of_business, sales) %>%
  ggplot(aes(x = sales_month, y = sales, color = kind_of_business)) +
  geom_line() +
  theme(legend.position = "top")
```

```{sql}
#| connection: db
SELECT date_part('year',sales_month) as sales_year,
  kind_of_business, sum(sales) as sales
FROM retail_sales
WHERE kind_of_business IN 
        ('Men''s clothing stores',
        'Women''s clothing stores')
GROUP BY 1, 2
ORDER BY 1, 2;
```

```{r}
retail_sales %>%
  filter(kind_of_business %in% 
           c("Men's clothing stores",
             "Women's clothing stores")) %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(sales_year, kind_of_business) %>%
  summarize(sales = sum(sales, na.rm = TRUE), .groups = "drop") %>%
  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +
  geom_line() +
  theme(legend.position = "top")
```

```{sql}
#| connection: db
SELECT date_part('year', sales_month) AS sales_year,
  sum(CASE WHEN kind_of_business = 'Women''s clothing stores' 
          then sales 
          END) AS womens_sales,
  sum(CASE WHEN kind_of_business = 'Men''s clothing stores' 
          then sales 
          END) AS mens_sales
FROM retail_sales
WHERE kind_of_business IN 
   ('Men''s clothing stores',
    'Women''s clothing stores')
GROUP BY 1
ORDER BY 1;
```

```{r}
pivoted_sales <-
  retail_sales %>%
  filter(kind_of_business %in% 
           c("Men's clothing stores",
             "Women's clothing stores")) %>%
  mutate(kind_of_business = if_else(kind_of_business == "Women's clothing stores",
                                    "womens", "mens"),
         sales_year = year(sales_month)) %>%
  group_by(sales_year, kind_of_business) %>%
  summarize(sales = sum(sales, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(id_cols = "sales_year",
              names_from = "kind_of_business",
              names_glue = "{kind_of_business}_{.value}",
              values_from = "sales")  

pivoted_sales %>%
  show_query()

pivoted_sales %>%
  arrange(sales_year) %>%
  collect(n = 10) %>%
  kable()
```

```{r}
pivoted_sales %>%
  filter(sales_year <= 2019) %>%
  group_by(sales_year) %>%
  mutate(womens_minus_mens = womens_sales - mens_sales,
         mens_minus_womens = mens_sales - womens_sales) %>%
  select(sales_year, womens_minus_mens, mens_minus_womens) %>%
  ggplot(aes(y = womens_minus_mens, x = sales_year)) +
  geom_line()
```

```{r}
pivoted_sales %>%
  filter(sales_year <= 2019) %>%
  group_by(sales_year) %>%
  mutate(womens_times_of_mens = womens_sales / mens_sales) %>%
  ggplot(aes(y = womens_times_of_mens, x = sales_year)) +
  geom_line()
```

```{r}
pivoted_sales %>%
  filter(sales_year <= 2019) %>%
  group_by(sales_year) %>%
  mutate(womens_pct_of_mens = (womens_sales / mens_sales - 1) * 100) %>%
  ggplot(aes(y = womens_pct_of_mens, x = sales_year)) +
  geom_line()
```

### Percent of Total Calculations

```{r}
retail_sales %>%
  filter(kind_of_business %in% 
           c("Men's clothing stores",
             "Women's clothing stores")) %>%
  group_by(sales_month) %>%
  mutate(total_sales = sum(sales, na.rm = TRUE))  %>%
  ungroup() %>%
  mutate(pct_total_sales = sales * 100 / total_sales) %>%
  select(sales_month, kind_of_business, pct_total_sales) %>%
  collect(n = 3) %>%
  kable()
```

```{r}
retail_sales %>%
  filter(kind_of_business %in% 
           c("Men's clothing stores",
             "Women's clothing stores")) %>%
  group_by(sales_month) %>%
  mutate(total_sales = sum(sales, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(pct_total_sales = sales * 100 / total_sales) %>%
  show_query()
```

```{r}
retail_sales %>%
  filter(kind_of_business %in% 
           c("Men's clothing stores",
             "Women's clothing stores")) %>%
  group_by(sales_month) %>%
  mutate(total_sales = sum(sales, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(pct_total_sales = sales * 100 / total_sales) %>%
  ggplot(aes(y = pct_total_sales, x = sales_month, color = kind_of_business)) +
  geom_line()
```

### Indexing to See Percent Change over Time

```{r}
retail_sales %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(sales_year) %>%
  summarize(sales = sum(sales, na.rm = TRUE)) %>%
  ungroup() %>%
  window_order(sales_year) %>%
  mutate(index_sales = first(sales),
         pct_from_index = (sales/index_sales - 1) * 100) %>%
  collect(n = 3) %>%
  kable()
```

```{r}
retail_sales %>%
  filter(kind_of_business %in% c("Women's clothing stores",
                                 "Men's clothing stores"),
         sales_month <= '2019-12-31') %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(kind_of_business, sales_year) %>%
  summarize(sales = sum(sales, na.rm = TRUE), .groups = "drop") %>%
  group_by(kind_of_business) %>%
  window_order(sales_year) %>%
  mutate(index_sales = first(sales),
         pct_from_index = (sales/index_sales - 1) * 100) %>%
  ungroup() %>%
  ggplot(aes(y = pct_from_index, x = sales_year, color = kind_of_business)) +
  geom_line()
```

## Rolling Time Windows

### Calculating Rolling Time Windows

```{r}
mvg_avg <-
  retail_sales %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  window_order(sales_month) %>%
  window_frame(-11, 0) %>%
  mutate(moving_avg = mean(sales, na.rm = TRUE),
         records_count = n()) %>%
  filter(sales_month >= '1993-01-01') 

mvg_avg %>%
  select(sales_month, moving_avg, records_count) %>%
  collect(n = 3) %>%
  kable(digits = 2)
```

```{r}
mvg_avg %>%
  ggplot(aes(x = sales_month)) +
  geom_line(aes(y = sales, colour = "Sales")) +
  geom_line(aes(y = moving_avg, colour = "Moving average")) 
```


```{sql}
#| connection: db
SELECT 
  sales_month,
  avg(sales) over w AS moving_avg,
  count(sales) over w AS records_count
FROM retail_sales
WHERE kind_of_business = 'Women''s clothing stores'
WINDOW w AS (order by sales_month 
             rows between 11 preceding and current row)
```

```{r}
#| eval: false
retail_sales %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  window_order(sales_month) %>%
  window_frame(-11, 0) %>%
  mutate(moving_avg = mean(sales, na.rm = TRUE),
         records_count = n()) %>%
  select(sales_month, moving_avg, records_count) %>%
  collect(n = 10) %>%
  kable()
```

```{r}
date_dim <-
  tibble(date = seq(as.Date('1993-01-01'), 
                    as.Date('2020-12-01'), 
                    by = "1 month")) %>%
  copy_to(db, ., overwrite = TRUE, name = "date_dim")
```

```{sql}
#| connection: db
WITH jan_jul AS (
  SELECT sales_month, sales
  FROM retail_sales 
  WHERE kind_of_business = 'Women''s clothing stores'
     AND date_part('month', sales_month) IN (1, 7))
     
SELECT a.date, b.sales_month, b.sales
FROM date_dim a
INNER JOIN jan_jul b 
ON b.sales_month BETWEEN a.date - interval '11 months' AND a.date
WHERE a.date BETWEEN '1993-01-01' AND '2020-12-01';
```

```{r}
jan_jul <-
  retail_sales %>%
  filter(kind_of_business == "Women's clothing stores",
         month(sales_month) %in% c(1, 7)) %>%
  select(sales_month, sales)

date_dim %>%
  mutate(date_start = date - months(11)) %>%
  inner_join(jan_jul, 
             join_by(between(y$sales_month, x$date_start, x$date))) %>%
  select(date, sales_month, sales) %>%
  collect(n = 3) %>%
  kable()
```

```{r}
date_dim %>%
  mutate(date_start = date - months(11)) %>%
  inner_join(jan_jul, 
             join_by(between(y$sales_month, x$date_start, x$date))) %>%
  group_by(date) %>%
  summarize(moving_avg = mean(sales, na.rm = TRUE),
            records = n()) %>%
  collect(n = 3) %>%
  kable()
```

```{sql}
#| connection: db
WITH sales_months AS (
  SELECT distinct sales_month
  FROM retail_sales
  WHERE sales_month between '1993-01-01' and '2020-12-01')

SELECT a.sales_month, avg(b.sales) as moving_avg
FROM sales_months a
JOIN retail_sales b 
on b.sales_month between 
    a.sales_month - interval '11 months' and a.sales_month
  and b.kind_of_business = 'Women''s clothing stores' 
GROUP BY 1
ORDER BY 1
LIMIT 3;
```

```{r}
sales_months <-
  retail_sales %>%
  filter(between(sales_month, 
                 as.Date('1993-01-01'), 
                 as.Date('2020-12-01'))) %>%
  distinct(sales_month)

sales_months %>%
  mutate(month_start = sales_month - months(11)) %>%
  inner_join(retail_sales, 
             join_by(between(y$sales_month, x$month_start, x$sales_month)),
                     suffix = c("", "_y")) %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  group_by(sales_month) %>%
  summarize(moving_avg = mean(sales, na.rm = TRUE)) %>%
  arrange(sales_month) %>%
  collect(n = 3) %>%
  kable()
``` 

### Calculating Cumulative Values

```{sql}
#| connection: db
SELECT sales_month, sales,
  sum(sales) OVER w AS sales_ytd
FROM retail_sales
WHERE kind_of_business = 'Women''s clothing stores'
WINDOW w AS (PARTITION BY date_part('year', sales_month) 
             ORDER BY sales_month)
LIMIT 3;
```

```{r}
ytd_sales <-
  retail_sales %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  mutate(year = year(sales_month)) %>%
  group_by(year) %>%
  window_order(sales_month) %>%
  mutate(sales_ytd = cumsum(sales)) %>%
  ungroup() %>%
  select(sales_month, sales, sales_ytd) 

ytd_sales %>%
  filter(month(sales_month) %in% c(1:3, 12)) %>%
  collect(n = 6) %>%
  kable()
```

```{r}
factor <- 40/4.5

ytd_sales %>%
  filter(year(sales_month) %in% 2016:2020) %>%
  ggplot(aes(x = sales_month, y = sales_ytd)) +
  geom_bar(stat = "identity") +
  geom_line(aes(y = sales * factor, colour = I("blue"))) +
  scale_y_continuous(
    "Sales YTD", 
    sec.axis = sec_axis(~ . / factor, name = "Monthly Sales")
  )
```
             
```{sql}
#| connection: db
SELECT a.sales_month, a.sales,
  sum(b.sales) AS sales_ytd
FROM retail_sales a
INNER JOIN retail_sales b ON 
 date_part('year',a.sales_month) = date_part('year',b.sales_month)
 AND b.sales_month <= a.sales_month
 AND b.kind_of_business = 'Women''s clothing stores'
WHERE a.kind_of_business = 'Women''s clothing stores'
GROUP BY 1,2;
```

```{r}
retail_sales_yr <-
  retail_sales %>%
  mutate(year = year(sales_month))
  
retail_sales_yr %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  inner_join(retail_sales_yr, 
             join_by(year, kind_of_business,
                     sales_month >= sales_month),
             suffix = c("", "_y")) %>%
  group_by(sales_month, sales) %>%
  summarize(sales_ytd = sum(sales_y, na.rm = TRUE),
            .groups = "drop") %>%
  filter(month(sales_month) %in% c(1:3, 12)) %>%
  collect(n = 6) %>%
  kable()
```

## Analyzing with Seasonality

```{r}
retail_sales %>%
  filter(kind_of_business %in% c("Jewelry stores",
                                 "Book stores",
                                 "Grocery stores")) %>%
  ggplot(aes(x = sales_month, y = sales)) +
  geom_line() +
  facet_wrap(vars(kind_of_business), nrow = 3, scales = "free")
```

### Period-over-Period Comparisons: YoY and MoM

```{sql}
#| connection: db
SELECT kind_of_business, sales_month, sales,
  lag(sales_month) OVER w AS prev_month,
  lag(sales) OVER w AS prev_month_sales
FROM retail_sales
WHERE kind_of_business = 'Book stores'
WINDOW w AS (PARTITION BY kind_of_business ORDER BY sales_month)
```

```{r}
books_w_lag <-
  retail_sales %>%
  filter(kind_of_business == 'Book stores') %>%
  group_by(kind_of_business) %>%
  window_order(sales_month) %>%
  mutate(prev_month = lag(sales_month),
         prev_month_sales = lag(sales)) %>%
  select(kind_of_business,
         sales_month, sales,
         prev_month, prev_month_sales)

books_w_lag %>%
  collect(n = 3) %>%
  kable()
```

```{r}
books_monthly <-
  books_w_lag %>%
  mutate(pct_growth = (sales / prev_month_sales - 1) * 100) %>%
  select(-prev_month, -prev_month_sales)
```

```{r}
books_yearly <- 
  retail_sales %>%
  filter(kind_of_business == 'Book stores') %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(sales_year) %>%
  summarize(yearly_sales = sum(sales, na.rm = TRUE),
            .groups = "drop") %>%
  window_order(sales_year) %>%
  mutate(prev_year_sales = lag(yearly_sales),
         pct_growth = (yearly_sales/prev_year_sales - 1) * 100)

books_yearly %>%
  collect(n = 3) %>%
  kable(digits = 2)
```

```{r}
books_monthly %>%
  filter(!is.na(pct_growth)) %>%
  ggplot(aes(x = sales_month, y = pct_growth)) +
  geom_line()
```

### Period-over-Period Comparisons: Same Month Versus Last Year

```{r}
books_lagged_year_month <-
  retail_sales %>%
  filter(kind_of_business == 'Book stores') %>%
  mutate(month = month(sales_month)) %>%
  group_by(month) %>%
  window_order(sales_month) %>%
  mutate(prev_year_month = lag(sales_month),
         prev_year_sales = lag(sales)) %>%
  ungroup() %>%
  select(sales_month, sales, prev_year_month, prev_year_sales)

books_lagged_year_month %>%
  filter(month(sales_month) <= 2, 
         year(sales_month) <= 1994) %>%
  arrange(sales_month) %>%
  collect(n = 6) %>%
  kable()
```

```{r}
books_lagged_year_month %>%
  mutate(dollar_diff = sales - prev_year_sales,
         pct_diff = dollar_diff/prev_year_sales * 100) %>%
  select(-prev_year_month, -prev_year_sales) %>%
  filter(month(sales_month) == 1) %>%
  collect(n = 3) %>%
  kable(digits = 2)
```
```{r}
books_lagged_year_month %>%
  filter(!is.na(prev_year_sales)) %>%
  mutate(dollar_diff = sales - prev_year_sales,
         pct_diff = dollar_diff/prev_year_sales * 100) %>%
  select(sales_month, sales, dollar_diff, pct_diff) %>%
  pivot_longer(-sales_month) %>%
  collect() %>%
  mutate(name = fct_inorder(name)) %>%
  ggplot(aes(x = sales_month, y = value)) +
  geom_line() +
  facet_wrap(. ~ name, nrow = 3, scales = "free")
```

With PostgreSQL, we would use `to_char(sales_month,'Month')`; with DuckDB, the equivalent is `monthname(sales_month)`.
To use an approach that works with either backend, we draw on arguments to the `lubridate` function `month()`.

```{r}
sales_92_94 <-
  retail_sales %>%
  filter(kind_of_business == 'Book stores',
         year(sales_month) %in% 1992:1994) %>%
  select(sales_month, sales) %>%
  mutate(month_number = month(sales_month),
         month_name = month(sales_month, 
                            label = TRUE, abbr = FALSE),
         year = as.integer(year(sales_month)))

sales_92_94 %>%
  pivot_wider(id_cols = c(month_number, month_name),
              names_from = year,
              names_prefix = "sales_",
              values_from = sales) %>%
  kable()
```

```{r}
sales_92_94 %>%
  collect() %>%
  mutate(year = factor(year),
         month_name = month(month_number, label = TRUE)) %>%
  ggplot(aes(x = month_name, y = sales, 
             group = year, colour = year)) +
  geom_line()
```

### Comparing to Multiple Prior Periods

```{r}
prev_three <-
  retail_sales %>%
  filter(kind_of_business == 'Book stores') %>%
  mutate(month = month(sales_month)) %>%
  group_by(month) %>%
  window_order(sales_month) %>%
  mutate(prev_sales_1 = lag(sales, 1),
         prev_sales_2 = lag(sales, 2),
         prev_sales_3 = lag(sales, 3)) %>%
  ungroup()

prev_three %>%
  filter(month == 1) %>%
  select(sales_month, sales, starts_with("prev_sales")) %>%
  collect(n = 5) %>%
  kable()
```

```{r}
prev_three %>%
  mutate(avg_prev_three = (prev_sales_1 + 
                         prev_sales_2 + 
                         prev_sales_3)/3,
         pct_of_prev_3 = 100 * sales/avg_prev_three) %>%
  select(sales_month, sales, pct_of_prev_3) %>%
  filter(month(sales_month) == 1,
         year(sales_month) %in% c(1995:1997, 2017:2019)) %>%
  collect(n = 10) %>%
  kable(digits = 2)
```

```{r}
prev_three_win <-
  retail_sales %>%
  filter(kind_of_business == 'Book stores') %>%
  mutate(month = month(sales_month)) %>%
  group_by(month) %>%
  window_order(sales_month) %>%
  window_frame(-3, -1) %>%
  mutate(avg_prev_three = mean(sales, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(pct_of_prev_3 = 100 * sales/avg_prev_three)
```

```{r}
prev_three_win %>%
  select(sales_month, sales, pct_of_prev_3) %>%
  filter(month(sales_month) == 1,
         year(sales_month) %in% c(1995:1997, 2017:2019)) %>%
  collect(n = 10) %>%
  kable(digits = 2)
```

## Persistent storage

Having created a database connection, we can write the local data frame to the database using (say) `copy_to()`.

```{r}
#| include: false
#| eval: false
retail_sales <- copy_to(db, retail_sales_local, 
                        name = "retail_sales", overwrite = TRUE)
```

We could specify `temporary = FALSE` if we wanted the data to be there permanently.^[Obviously this would not make sense if `db` is a connection to an in-memory database.]

```{r}
#| warning: false
#| eval: false
#| include: false
# Code to put data in sql_book.duckdb.
db <- dbConnect(duckdb::duckdb(), "sql_book.duckdb")
url <- paste0("https://raw.githubusercontent.com/",
              "cathytanimura/sql_book/master/",
              "Chapter%203%3A%20Time%20Series%20Analysis/",
              "us_retail_sales.csv")
df <- read_csv(url, show_col_types = FALSE)
dbWriteTable(db, "retail_sales", df,
             overwrite = TRUE, row.names = FALSE)
dbDisconnect(db, shutdown = TRUE)
```

### Using PostgreSQL

```{r}
#| eval: false
#| include: false
# First, if we have a PostgreSQL database, we could connect to that.
db <- dbConnect(RPostgres::Postgres(), 
                bigint = "integer",
                check_interrupts = TRUE)
```

### Read-only databases

In some cases, you will have access to a database, but no write privileges for that database.
In such a case, `copy_inline()` can be useful.^[I [requested](https://github.com/tidyverse/dbplyr/issues/628) this function for a common use case I have.
Thank you to the `dbplyr` team for making it happen.]
Note that it seems you cannot interrogate a table created using `copy_inline()` using SQL, though it will behave in most respects just like a table created using `copy_to()` when using `dbplyr`.
It is useful to note that `copy_inline()` is probably not a good solution if your data are hundreds of thousands of rows or more because the table is effectively turned into literal SQL.

```{r}
#| eval: false
retail_sales_alt <- copy_inline(db, retail_sales_local)
```

### Closing the database connection

```{r}
dbDisconnect(db, shutdown=TRUE)
```
