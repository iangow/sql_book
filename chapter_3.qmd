# Time Series Analysis {#sec-time-series}

```{r}
#| message: false
library(DBI)
library(tidyverse)
library(dbplyr)
library(knitr)
```

```{r}
#| output: false
#| include: false
#| eval: false
db <- dbConnect(RPostgres::Postgres(), 
                bigint = "integer",
                check_interrupts = TRUE)
```

```{r}
#| include: false
#| eval: false
dbExecute(db, "SET search_path TO sql_book")
```

```{r}
db <- dbConnect(duckdb::duckdb())
```


## Date, Datetime, and Time Manipulations

```{sql}
#| connection: db
#| output: false
#| warning  : false
INSTALL 'icu';
LOAD 'icu';
```

### Time Zone Conversions

@tanimura2021sql points out that often "timestamps in the database are not encoded with the time zone, and you will need to consult with the source or developer to figure out how your data was stored."
When pushing data to a PostgreSQL database, I use the `timestamp with time zone` type as much as possible.

@tanimura2021sql provides the following example, which is interesting because the west coast of the United States would not be on the PST time zone at that time of year.
Instead, it would be on PDT.

```{sql}
#| connection: db
SELECT timestamptz '2020-09-01 00:00:00 -0' AT TIME ZONE 'PST' AS time;
```
In PostgreSQL, we would get a different answer with the following query, but in DuckDB it seems that `PDT` is not recognized as a time zone abbreviation at all, so we just get the original UTC timestamp back.

```{sql}
#| connection: db
SELECT timestamptz '2020-09-01 00:00:00 -0' AT TIME ZONE 'PDT' AS time;
```

I think most people barely know the difference between PST and PDT and even fewer would know the exact dates that one switches from one to the other.
A better approach is to use a time zone that encodes information about when PDT is used and when PST is used.
In PostgreSQL and DuckDB, the function `pg_timezone_names()` returns a table with the information that we need.

However, it seems that there are inconsistencies between PostgreSQL and DuckDB in terms of the abbreviations used.
As such it's probably safer to use the `name` form (e.g., `US/Pacific`) whenever possible.
Given the widespread confusion about the meaning of terms like `EST`, `EDT`, and so on, just use `US/Eastern`, etc.

```{sql}
#| connection: db
SELECT name, abbrev
FROM pg_timezone_names()
WHERE regexp_matches(name, '^US/');
-- use WHERE name ~ '^US/'; in PostgreSQL
```

The following queries demonstrate that daylight savings information is encoded in the database.

```{sql}
#| connection: db
SELECT '2020-09-01 17:00:01 US/Pacific'::timestamptz AS t1,
       '2020-09-02 10:00:01 Australia/Melbourne'::timestamptz  AS t2;
```

```{sql}
#| connection: db
SELECT '2020-12-01 16:00:01 US/Pacific'::timestamptz AS t1,
       '2020-12-02 11:00:01 Australia/Melbourne'::timestamptz  AS t2;
```

```{r}
sql <-
  "(SELECT     
    '2020-12-01 16:00:01 US/Pacific'::timestamptz AS t1)"

a_time <- tbl(db, sql(sql))
a_time %>%
  kable()
```

```{r}
a_time_r <-
  a_time %>%
  select(t1) %>%
  pull()

print(a_time_r, tz = "UTC")
print(a_time_r, tz = "US/Pacific")
Sys.timezone()
print(a_time_r, tz = Sys.timezone())
```
The above examples illustrate a few key ideas.

First, while we supply the literal form `'2020-09-01 17:00:01 US/Pacific'::timestamptz`, it seems that once a variable has been encoded as `TIMESTAMP WITH TIME ZONE`, it behaves as though it is actually being *stored* as a timestamp in the UTC time zone, just with the *displayed* time perhaps being different.

Second, columns of type `TIMESTAMP WITH TIME ZONE` come into R with the associated time-zone information, which is what we want (especially if we will later put timestamp data back into PostgreSQL).

Third, we can see that we can choose to *display* information in a different time zone without changing the underlying data.

Some care is needed with timestamp data.
I think the `AT TIME ZONE` queries provided in @tanimura2021sql are actually pretty dangerous, as can be seen in the following query.
While we supply `2020-09-01 00:00:01` as UTC and then render it `AT TIME ZONE 'US/Pacific'`, it turns out that the returned value is interpreted as a `TIMESTAMP WITHOUT TIME ZONE` and subsequent queries lead to confusing behaviour.
In the query below, the second application of `AT TIME ZONE` interprets the `TIMESTAMP WITHOUT TIME ZONE` as though it came from the stated time zone and the results seem to have `AT TIME ZONE` doing the opposite of what it did when given a `TIMESTAMP WITH TIME ZONE` (as in the initial literal `'2020-09-01 00:00:01 -0'`). 

```{sql}
#| connection: db
WITH q1 AS
 (SELECT timestamptz '2020-09-01 00:00:01-00' AT TIME ZONE 'US/Pacific' AS t1,
         timestamp '2020-09-01 00:00:01' AT TIME ZONE 'US/Pacific' AS t2)
 
SELECT 
  t1,
  t1::varchar AS t1_char,
  t1 AT TIME ZONE 'UTC' AS t3,
  t2 AT TIME ZONE 'UTC' AS t4,
  typeof(t1),
  typeof(t2)
FROM q1
```
It seems that `TIMESTAMP WITHOUT TIME ZONE` values should be converted to a time zone as quickly as possible to avoid confusion and that care is needed with `AT TIME ZONE` given that it does very different (essentially opposite) things according to the supplied data type.

```{sql}
#| connection: db
WITH q1 AS
 (SELECT '2020-09-01 00:00:01-00'::timestamptz AS t1)
 
SELECT t1,
  t1::varchar AS t2,
  typeof(t1)
FROM q1
```
Strange behaviour can result from values stored as `TIMESTAMP WITHOUT TIME ZONE`.
Below we see that `t1` is printed as UTC no matter what, while the behaviour of `t2` seems easier to understand.

```{r}
sql <-
  "(SELECT     
    '2020-12-01 00:00:01-00' AS t1,
    '2020-12-01 00:00:01-00'::timestamptz AS t2)"

two_times_notz <- tbl(db, sql(sql))
two_times_notz %>% kable()
```

```{r}
two_times_notz_r <-
  collect(two_times_notz)
  
print(two_times_notz_r$t1)
Sys.timezone()
print(two_times_notz_r$t1, tz = Sys.timezone())

print(two_times_notz_r$t2)
Sys.timezone()
print(two_times_notz_r$t2, tz = Sys.timezone())
```
As pointed out by @tanimura2021sql, one drawback to storing information as UTC is that localtime information may be lost.
But it seems it would be more prudent to store information as `TIMESTAMP WITH TIME ZONE` and keep local time zone information as a separate column to avoid confusion.
For example, if the `orders` table is stored as `TIMESTAMP WITHOUT TIME ZONE` based on the local time of the customer, which might be `Australia/Melbourne` and the `shipping` table uses `TIMESTAMP WITH TIME ZONE`, then an analyst of time-to-ship data would be confused by orders apparently being shipped before they are made.
If `shipping` table uses `TIMESTAMP WITH TIME ZONE` using timestamps in the time zone of the East Bay warehouse (so `US/Pacific`), things would be even worse.

I think that fully fleshing out the issues here would require a separate chapter.
In fact, nothing in the core part of Chapter 3 of @tanimura2021sql (which focuses on the `retail_sales` table) really uses timestamp information, so we can put these issues aside for now.

### Date and Timestamp Format Conversions

As discussed in @tanimura2021sql, PostgreSQL has a rich array of functions for converting dates and times and extracting such information as months and days of the week.

```{sql}
#| connection: db
SELECT date_trunc('month','2020-10-04 12:33:35 -00'::timestamptz);
```

One such function 

```{r}
a_time_df <- tbl(db, sql("(SELECT '2020-10-04 12:33:35'::timestamp AS a_time)"))

a_time_df %>% 
  mutate(a_trunced_time = date_trunc('month', a_time))

a_time_df %>% 
  mutate(a_trunced_time = date_trunc('month', a_time)) %>%
  show_query()

a_time_df %>%
  collect()
```

### Date Math

### Time Math

```{r}
a_time_df <- tbl(db, sql("(SELECT '2020-10-04 12:33:35 US/Pacific'::timestamptz AS a_time)"))

a_time_df %>% 
  mutate(a_trunced_time = date_trunc('month', a_time)) 

a_time_df %>% 
  mutate(a_trunced_time = date_trunc('month', a_time)) %>%
  show_query()

a_time_df %>%
  collect()
```

```{r}
a_time_df %>%
  mutate(new_time = a_time + sql("interval '3 hours'")) %>%
  collect()
```

```{r}
#| include: false
dbDisconnect(db, shutdown=TRUE)
```

### Joining Data from Different Sources

## The Retail Sales Data Set

As discussed in @tanimura2021sql, the data set used in this chapter comes from the website of the US Census Bureau.
The data set is a little messy, but not too large, so we can easily grab it directly from the website and clean it up in much the same way that Cathy has done for us.

```{r}
#| cache: true  
library(tidyverse)
library(readxl)

# Use tmpdir = "." or known directory if you have trouble with 
# this part.
mrtssales <- tempfile(fileext = ".xlsx")
url <- paste0("https://www.census.gov/retail/mrts/www/",
              "mrtssales92-present.xlsx")
download.file(url, mrtssales)

read_tab <- function(year) {
  
  # Initially we read all columns as text, as we want to process
  # more precise than read_excel() would do unsupervised.
  temp <- read_excel(mrtssales,
                     range = "A4:N71", 
                     sheet = as.character(year),
                     col_types = "text",
                     col_names = paste0("v", 1:14))
  
  # The third row has the dates for columns 3:14
  names(temp) <- c("naics_code", "kind_of_business",
                   as.character(temp[2, 3:14]))
  
  # The actual data are found after row 3
  temp <- temp[-1:-3, ]
  
  # Now pivot the data and convert sales to numeric values.
  # Also convert sales_month to dates (start of respective month).
  df <-
    temp %>%
    pivot_longer(names_to = "sales_month",
                 values_to = "sales",
                 cols = -1:-2) %>%
    mutate(sales_month = paste("01", str_remove(sales_month, "\\.")),
           sales_month = as.Date(sales_month, "%d %b %Y")) %>%
    mutate(reason_for_null = case_when(sales == "(NA)" ~ "Not Available",
                                       sales == "(S)" ~ "Supressed",
                                       TRUE ~ NA),
           sales = case_when(sales == "(NA)" ~ NA,
                             sales == "(S)" ~ NA,
                             TRUE ~ sales)) %>%
    mutate(sales = as.double(sales)) %>%
    select(sales_month, naics_code, kind_of_business,
           reason_for_null, sales)
  df
}

retail_sales_local <- bind_rows(lapply(1992:2020, read_tab)) 
```

```{r}
#| warning: false
#| eval: false
db <- dbConnect(duckdb::duckdb(), "sql_book.duckdb")
df <- read_csv("data/us_retail_sales.csv.gz", show_col_types = FALSE)
dbWriteTable(db, "retail_sales", df,
             overwrite = TRUE, row.names = FALSE)
dbDisconnect(db, shutdown = TRUE)
```

Now that we have the data in R, a few questions arise.

First, why would we move it to a database?
Second, how can we move it to a database?
Related to the previous question will be: which database to we want to move it to?

Taking these questions in turn, the first one is a good question.
With `retail_sales` as a local data frame, we can run almost all the analyses below with only the slightest modifications.
The modifications needed are to replace every instance of `window_order()` with `arrange()`.^[I requested a tweak to `dplyr` that would have avoided the need to do this, but my request was denied.
Given how awesome `dbplyr`/`dplyr` is, I cannot complain.]
The "almost all" relates to the moving average, which relies on `window_frame()` from `dbplyr`, which has no exact equivalent in `dplyr`.

So the first point is that "almost all" implies an advantage for using `dbplyr`.
On occasion, the SQL engine provided by PostgreSQL will allow us to do some data manipulations more easily than we can in R.
But of course, there are many cases when the opposite is true.
That said, why not have both?
Store data in a database and `collect()` as necessary to do things that R is better at?

Second, performance can be better using SQL.
Compiling a version of this chapter using `dplyr` with a local data frame for the remainder took just under 14 seconds.
Using a PostgreSQL backend, it took 8.5 seconds.
Adding back in the queries with `window_order()` that I removed so that I could compile with `dplyr` and using DuckDB as the backend, the document took 8.3 seconds to compile (this beat out PostgreSQL doing the same in 9.6 seconds).
While these differences are not practically very significant, they could be with more demanding tasks.
Also, a database will not always beat R or Python for performance, but often will and having the option to use a database backend is a good thing.

Third, having the data in a database allows you to interrogate the data using SQL.
If you are more familiar with SQL, or just know how to do a particular task in SQL, this can be beneficial.^[Though I will argue later that transitioning to `dplyr`/`dbplyr` is actually not difficult.]

Fourth, I think there is merit in separating the tasks of acquiring and cleaning data from the task of analysing those data.
Many data analysts have a work flow that entails ingesting and cleaning data for each analysis task.
My experience is that it is often better to do the ingesting and cleaning once and then reuse the cleaned data in subsequent analyses.
A common pattern involves reusing the same data in many analyses and it can be helpful to divide the tasks in a way that using an SQL database encourages.
Also the skills in ingesting and cleaning data can be different from those for analysing those data, so sometimes it makes sense for one person to do one task, push the data to a database, and then have someone else do some or all of the analysis.

Now, what about the second question.
Here there are a few options.

First, if we have a PostgreSQL database, we could connect to that, much as we did at the start of the chapter:

```{r}
#| eval: false
db <- dbConnect(RPostgres::Postgres(), 
                bigint = "integer",
                check_interrupts = TRUE)
```

```{r}
db <- dbConnect(duckdb::duckdb(), read_only = TRUE)
retail_sales <- copy_to(db, retail_sales_local, "retail_sales")
```

Alternatively, we could use DuckDB.
To install DuckDB, all we have to do is `install.packages("duckdb")`.
Then we can create a connection as follows:

```{r}
#| eval: false
db <- dbConnect(duckdb::duckdb())

```

By default, the `duckdb` will use an in-memory database, though we could easily specify a file (say, `sql_book.duckdb`) if we want persistent storage or are using an existing database.

Having created a database connection, we can write the local data frame to the database using (say) `copy_to()`.

```{r}
#| include: false
#| eval: false
retail_sales <- copy_to(db, retail_sales_local, 
                        name = "retail_sales", overwrite = TRUE)
```

We could specify `temporary = FALSE` if we wanted the data to be there permanently.^[Obviously this would not make sense if `db` is a connection to an in-memory database.]

In some cases, you will have access to a database, but no write privileges for that database.
In such a case, `copy_inline()` can be useful.
Note that it seems you cannot interrogate a table created using `copy_inline()` using SQL, though it will behave in most respects just like a table created using `copy_to()` when using `dbplyr`.
It is useful to note that `copy_inline()` is probably not a good solution if your data are hundreds of thousands of rows or more because the table is effectively turned into literal SQL.

```{r}
#| eval: false
retail_sales_alt <- copy_inline(db, retail_sales_local)
```

```{r}
retail_sales <- tbl(db, "retail_sales")
```

## Trending the Data

### Simple Trends

```{sql}
#| connection: db
SELECT sales_month, sales
FROM retail_sales
WHERE kind_of_business = 'Retail and food services sales, total'
ORDER BY 1
```

```{r}
retail_sales %>%
  filter(kind_of_business == 'Retail and food services sales, total') %>%
  select(sales_month, sales) %>%
  ggplot(aes(x = sales_month, y = sales)) +
  geom_line()
```

```{sql}
#| connection: db
SELECT date_part('year',sales_month) as sales_year,
    sum(sales) as sales
FROM retail_sales
WHERE kind_of_business = 'Retail and food services sales, total'
GROUP BY 1
;
```

```{r}
retail_sales %>%
  filter(kind_of_business == 'Retail and food services sales, total') %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(sales_year) %>%
  summarize(sales = sum(sales, na.rm = TRUE)) %>%
  ggplot(aes(x = sales_year, y = sales)) +
  geom_line()
```

```{sql}
#| connection: db
SELECT date_part('year',sales_month) as sales_year, 
  kind_of_business, sum(sales) as sales
FROM retail_sales
WHERE kind_of_business IN 
          ('Book stores',
           'Sporting goods stores',
           'Hobby, toy, and game stores')
GROUP BY 1,2
ORDER BY 1;
```

### Comparing Components

```{r}
retail_sales %>%
  filter(kind_of_business %in% 
           c('Book stores',
             'Sporting goods stores',
             'Hobby, toy, and game stores')) %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(sales_year, kind_of_business) %>%
  summarize(sales = sum(sales, na.rm = TRUE), .groups = "drop") %>%
  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +
  geom_line() +
  theme(legend.position = "top")
```

```{sql}
#| connection: db
SELECT sales_month, kind_of_business, sales
FROM retail_sales
WHERE kind_of_business IN ('Men''s clothing stores','Women''s clothing stores')
ORDER BY 1,2;
```

```{r}
retail_sales %>%
  filter(kind_of_business %in% c("Men's clothing stores",
                                 "Women's clothing stores")) %>%
  select(sales_month, kind_of_business, sales) %>%
  ggplot(aes(x = sales_month, y = sales, color = kind_of_business)) +
  geom_line() +
  theme(legend.position = "top")
```

```{sql}
#| connection: db
SELECT date_part('year',sales_month) as sales_year,
  kind_of_business, sum(sales) as sales
FROM retail_sales
WHERE kind_of_business IN 
        ('Men''s clothing stores',
        'Women''s clothing stores')
GROUP BY 1, 2
ORDER BY 1, 2;
```

```{r}
retail_sales %>%
  filter(kind_of_business %in% 
           c("Men's clothing stores",
             "Women's clothing stores")) %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(sales_year, kind_of_business) %>%
  summarize(sales = sum(sales, na.rm = TRUE), .groups = "drop") %>%
  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +
  geom_line() +
  theme(legend.position = "top")
```

```{sql}
#| connection: db
SELECT date_part('year', sales_month) AS sales_year,
  sum(CASE WHEN kind_of_business = 'Women''s clothing stores' 
          then sales 
          END) AS womens_sales,
  sum(CASE WHEN kind_of_business = 'Men''s clothing stores' 
          then sales 
          END) AS mens_sales
FROM retail_sales
WHERE kind_of_business IN 
   ('Men''s clothing stores',
    'Women''s clothing stores')
GROUP BY 1
ORDER BY 1;
```

```{r}
pivoted_sales <-
  retail_sales %>%
  filter(kind_of_business %in% 
           c("Men's clothing stores",
             "Women's clothing stores")) %>%
  mutate(kind_of_business = if_else(kind_of_business == "Women's clothing stores",
                                    "womens", "mens"),
         sales_year = year(sales_month)) %>%
  group_by(sales_year, kind_of_business) %>%
  summarize(sales = sum(sales, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(id_cols = "sales_year",
              names_from = "kind_of_business",
              names_glue = "{kind_of_business}_{.value}",
              values_from = "sales")  

pivoted_sales %>%
  show_query()

pivoted_sales %>%
  arrange(sales_year) %>%
  collect(n = 10) %>%
  kable()
```

```{r}
pivoted_sales %>%
  filter(sales_year <= 2019) %>%
  group_by(sales_year) %>%
  mutate(womens_minus_mens = womens_sales - mens_sales,
         mens_minus_womens = mens_sales - womens_sales) %>%
  select(sales_year, womens_minus_mens, mens_minus_womens) %>%
  ggplot(aes(y = womens_minus_mens, x = sales_year)) +
  geom_line()
```

```{r}
pivoted_sales %>%
  filter(sales_year <= 2019) %>%
  group_by(sales_year) %>%
  mutate(womens_times_of_mens = womens_sales / mens_sales) %>%
  ggplot(aes(y = womens_times_of_mens, x = sales_year)) +
  geom_line()
```

```{r}
pivoted_sales %>%
  filter(sales_year <= 2019) %>%
  group_by(sales_year) %>%
  mutate(womens_pct_of_mens = (womens_sales / mens_sales - 1) * 100) %>%
  ggplot(aes(y = womens_pct_of_mens, x = sales_year)) +
  geom_line()
```

### Percent of Total Calculations

```{r}
retail_sales %>%
  filter(kind_of_business %in% 
           c("Men's clothing stores",
             "Women's clothing stores")) %>%
  group_by(sales_month) %>%
  mutate(total_sales = sum(sales)) %>%
  ungroup() %>%
  mutate(pct_total_sales = sales * 100 / total_sales) %>%
  select(sales_month, kind_of_business, pct_total_sales) %>%
  collect(n = 3)
```

```{r}
retail_sales %>%
  filter(kind_of_business %in% 
           c("Men's clothing stores",
             "Women's clothing stores")) %>%
  group_by(sales_month) %>%
  mutate(total_sales = sum(sales)) %>%
  ungroup() %>%
  mutate(pct_total_sales = sales * 100 / total_sales) %>%
  show_query()
```

```{r}
retail_sales %>%
  filter(kind_of_business %in% 
           c("Men's clothing stores",
             "Women's clothing stores")) %>%
  group_by(sales_month) %>%
  mutate(total_sales = sum(sales)) %>%
  ungroup() %>%
  mutate(pct_total_sales = sales * 100 / total_sales) %>%
  ggplot(aes(y = pct_total_sales, x = sales_month, color = kind_of_business)) +
  geom_line()
```

### Indexing to See Percent Change over Time

```{r}
retail_sales %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(sales_year) %>%
  summarize(sales = sum(sales, na.rm = TRUE)) %>%
  ungroup() %>%
  window_order(sales_year) %>%
  mutate(index_sales = first(sales),
         pct_from_index = (sales/index_sales - 1) * 100)
```

```{r}
retail_sales %>%
  filter(kind_of_business %in% c("Women's clothing stores",
                                 "Men's clothing stores"),
         sales_month <= '2019-12-31') %>%
  mutate(sales_year = year(sales_month)) %>%
  group_by(kind_of_business, sales_year) %>%
  summarize(sales = sum(sales, na.rm = TRUE), .groups = "drop") %>%
  group_by(kind_of_business) %>%
  window_order(sales_year) %>%
  mutate(index_sales = first(sales),
         pct_from_index = (sales/index_sales - 1) * 100) %>%
  ungroup() %>%
  ggplot(aes(y = pct_from_index, x = sales_year, color = kind_of_business)) +
  geom_line()
```

## Rolling Time Windows

### Calculating Rolling Time Windows

```{r}
mvg_avg <-
  retail_sales %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  window_order(sales_month) %>%
  window_frame(-11, 0) %>%
  mutate(moving_avg = mean(sales, na.rm = TRUE),
         records_count = n()) %>%
  filter(sales_month >= '1993-01-01') 

mvg_avg %>%
  select(sales_month, moving_avg, records_count) %>%
  collect(n = 3) %>%
  kable(digits = 2)
```

```{r}
mvg_avg %>%
  ggplot(aes(x = sales_month)) +
  geom_line(aes(y = sales, colour = "Sales")) +
  geom_line(aes(y = moving_avg, colour = "Moving average")) 
```


```{sql}
#| connection: db
SELECT 
  sales_month,
  avg(sales) over w AS moving_avg,
  count(sales) over w AS records_count
FROM retail_sales
WHERE kind_of_business = 'Women''s clothing stores'
WINDOW w AS (order by sales_month 
             rows between 11 preceding and current row)
```

```{r}
#| eval: false
retail_sales %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  window_order(sales_month) %>%
  window_frame(-11, 0) %>%
  mutate(moving_avg = mean(sales, na.rm = TRUE),
         records_count = n()) %>%
  select(sales_month, moving_avg, records_count) %>%
  collect(n = 10) %>%
  kable()
```

```{r}
date_dim <-
  tibble(date = seq(as.Date('1993-01-01'), 
                    as.Date('2020-12-01'), 
                    by = "1 month")) %>%
  copy_to(db, ., overwrite = TRUE, name = "date_dim")
```

```{sql}
#| connection: db
WITH jan_jul AS (
  SELECT sales_month, sales
  FROM retail_sales 
  WHERE kind_of_business = 'Women''s clothing stores'
     AND date_part('month', sales_month) IN (1, 7))
     
SELECT a.date, b.sales_month, b.sales
FROM date_dim a
INNER JOIN jan_jul b 
ON b.sales_month BETWEEN a.date - interval '11 months' AND a.date
WHERE a.date BETWEEN '1993-01-01' AND '2020-12-01';
```

```{r}
jan_jul <-
  retail_sales %>%
  filter(kind_of_business == "Women's clothing stores",
         month(sales_month) %in% c(1, 7)) %>%
  select(sales_month, sales)

date_dim %>%
  mutate(date_start = date - months(11)) %>%
  inner_join(jan_jul, 
             join_by(between(y$sales_month, x$date_start, x$date))) %>%
  select(date, sales_month, sales)
```

```{r}
date_dim %>%
  mutate(date_start = date - months(11)) %>%
  inner_join(jan_jul, 
             join_by(between(y$sales_month, x$date_start, x$date))) %>%
  group_by(date) %>%
  summarize(moving_avg = mean(sales, na.rm = TRUE),
            records = n())
```

```{sql}
#| connection: db
WITH sales_months AS (
  SELECT distinct sales_month
  FROM retail_sales
  WHERE sales_month between '1993-01-01' and '2020-12-01')

SELECT a.sales_month, avg(b.sales) as moving_avg
FROM sales_months a
JOIN retail_sales b 
on b.sales_month between 
    a.sales_month - interval '11 months' and a.sales_month
  and b.kind_of_business = 'Women''s clothing stores' 
GROUP BY 1
ORDER BY 1
LIMIT 3;
```

```{r}
sales_months <-
  retail_sales %>%
  filter(between(sales_month, 
                 as.Date('1993-01-01'), 
                 as.Date('2020-12-01'))) %>%
  distinct(sales_month)

sales_months %>%
  mutate(month_start = sales_month - months(11)) %>%
  inner_join(retail_sales, 
             join_by(between(y$sales_month, x$month_start, x$sales_month)),
                     suffix = c("", "_y")) %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  group_by(sales_month) %>%
  summarize(moving_avg = mean(sales, na.rm = TRUE)) %>%
  arrange(sales_month) %>%
  collect(n = 3) %>%
  kable()
``` 

### Calculating Cumulative Values

```{sql}
#| connection: db
SELECT sales_month, sales,
  sum(sales) over w as sales_ytd
FROM retail_sales
WHERE kind_of_business = 'Women''s clothing stores'
WINDOW w AS (partition by date_part('year',sales_month) 
             order by sales_month)
LIMIT 3
```

```{r}
ytd_sales <-
  retail_sales %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  mutate(year = year(sales_month)) %>%
  group_by(year) %>%
  window_order(sales_month) %>%
  mutate(sales_ytd = cumsum(sales)) %>%
  ungroup() %>%
  select(sales_month, sales, sales_ytd) 

ytd_sales %>%
  filter(month(sales_month) %in% c(1:3, 12)) %>%
  collect(n = 6) %>%
  kable()
```

```{r}
factor <- 40/4.5

ytd_sales %>%
  filter(year(sales_month) %in% 2016:2020) %>%
  ggplot(aes(x = sales_month, y = sales_ytd)) +
  geom_bar(stat = "identity") +
  geom_line(aes(y = sales * factor, colour = I("blue"))) +
  scale_y_continuous(
    "Sales YTD", 
    sec.axis = sec_axis(~ . / factor, name = "Monthly Sales")
  )
```
             
```{sql}
#| connection: db
SELECT a.sales_month, a.sales
,sum(b.sales) as sales_ytd
FROM retail_sales a
JOIN retail_sales b on 
 date_part('year',a.sales_month) = date_part('year',b.sales_month)
 and b.sales_month <= a.sales_month
 and b.kind_of_business = 'Women''s clothing stores'
WHERE a.kind_of_business = 'Women''s clothing stores'
GROUP BY 1,2;
```

```{r}
retail_sales_yr <-
  retail_sales %>%
  mutate(year = year(sales_month))
  
retail_sales_yr %>%
  filter(kind_of_business == "Women's clothing stores") %>%
  inner_join(retail_sales_yr, 
             join_by(year, kind_of_business, sales_month >= sales_month),
             suffix = c("", "_y")) %>%
  group_by(sales_month, sales) %>%
  summarize(sales_ytd = sum(sales_y), .groups = "drop") %>%
  filter(month(sales_month) %in% c(1:3, 12)) %>%
  collect(n = 6) %>%
  kable()
```

## Analyzing with Seasonality

### Period-over-Period Comparisons: YoY and MoM

### Period-over-Period Comparisons: Same Month Versus Last Year

### Comparing to Multiple Prior Periods



```{r}
#| include: false
dbDisconnect(db, shutdown=TRUE)
```