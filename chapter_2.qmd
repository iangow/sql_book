# Preparing Data for Analysis

Chapter 2 of @tanimura2021sql provides a good foundation discussion of issues related to preparing data for analysis.
While the discussion is couched in terms of SQL, in reality the issues are not specific to SQL or databases.
For this reason, I recommend that you read the chapter.

While Chapter 2 of @tanimura2021sql contains many code snippets, few of these seem to be intended for users to run (in part because they assume a database set-up that users would not have).
For this reason, I do not attempt to provide `dplyr` equivalents to the code there except for a couple of exceptions that I discuss below.

In my view, some of the material in Chapter 2 of @tanimura2021sql would be better placed later in the book where there is more context for some of the issues discussed.
Chapter 2 of @tanimura2021sql contains some example code, but no data, so the reader can merely read.
As such, there isn't much for me to parallel here.
Nonetheless, I recommend you go read Chapter 2 before coming back here.

## Types of Data

### Database Data Types

R has all the types listed in Chapter 2.
PostgreSQL has a richer set of data types than base R has.^[Some packages offer additional data types, so it is difficult to be definitive where R has fewer types once packages are included.]

In general, data retains the equivalent type when transferred from R to PostgreSQL or vice versa.
Sometimes data makes round trips (e.g., data is pulled from PostgreSQL to R for computations and then sent back to R) and it is important to check that data types are retained in the process (e.g., that timestamps don't shift to a different time zone).

### Structured versus Unstructured

### Quantitative versus Qualitative Data

### First-, Second-, and Third-Party Data

### Sparse Data

## SQL Query Structure

## Profiling: Distributions

### Histograms and Frequencies

### Binning

### n-Tiles

## Profiling: Data Quality

### Detecting Duplicates

### Deduplication with GROUP BY and DISTINCT

## Preparing: Data Cleaning

### Cleaning Data with CASE Transformations

### Type Conversions and Casting

### Dealing with Nulls: coalesce, nullif, nvl Functions

### Missing data

The SQL in the book generally uses the form `x::date` rather than the more standard SQL `CAST(x AS DATE)`.
In `dbplyr`, we would use `as.Date(x)` and `dbplyr` would translate as `CAST(x AS DATE)`.
The following code and output demonstrates how `dbplyr` translated from `dplyr` to SQL.

The table stored in `dates_processed` below is equivalent to that created and stored in the database as `date_dim` in the code supplied with book.
This `date_dim` table is only used in #sec-time-series of the book and we will not even use it there (for reasons to be explained).

```{r}
#| message: false
library(tidyverse)
library(DBI)
library(knitr)
```

```{r}
#| output: false
pg <- dbConnect(RPostgres::Postgres(), 
                bigint = "integer",
                check_interrupts = TRUE)
```

```{r}
#| include: false
dbExecute(pg, "SET search_path TO sql_book")
```

```{r}
dates_sql <-
   "SELECT * 
    FROM generate_series('2000-01-01'::timestamp,
                         '2030-12-31', '1 day')"

dates <- 
  tbl(pg, sql(dates_sql)) %>%
  rename(date = generate_series)

dates_processed <-
  dates %>%
  mutate(date_key = as.integer(to_char(date, 'yyyymmdd')),
         day_of_month = as.integer(date_part('day',date)),
         day_of_year = as.integer(date_part('doy', date)),
         day_of_week = as.integer(date_part('dow', date)),
         day_name  = trim(to_char(date, 'Day')),
         day_short_name = trim(to_char(date, 'Dy')),
         week_number = as.integer(date_part('week', date)),
         week_of_month = as.integer(to_char(date, 'W')),
         week = as.Date(date_trunc('week', date)),
         month_number = as.integer(date_part('month',date)),
         month_name = trim(to_char(date, 'Month')),
         month_short_name = trim(to_char(date, 'Mon')),
         first_day_of_month = as.Date(date_trunc('month', date)),
         last_day_of_month = as.Date(date_trunc('month', date) +
                                       months(1) - days(1)),
         quarter_number = as.integer(date_part('quarter', date)),
         quarter_name = trim('Q' %||% as.integer(date_part('quarter', date))),
         first_day_of_quarter = as.Date(date_trunc('quarter', date)),
         last_day_of_quarter = as.Date(date_trunc('quarter', date) + 
                                         months(3) - days(1)),
         year = as.integer(year(date)),
         decade = as.integer(date_part('decade', date)) * 10,
         century = as.integer(date_part('century', date)))

dates_processed %>%
  collect(n = 10)

dates_processed %>%
  show_query()
```

## Preparing: Shaping Data

### For Which Output: BI, Visualization, Statistics, ML

### Pivoting with CASE Statements

### Unpivoting with UNION Statements

A user of `dplyr` has access to the functions `pivot_wider` and `pivot_longer`, which make it much easier to "pivot" and "unpivot" tables than using `CASE` statements, which could become long and tedious.

To illustrate the `dplyr` way of doing things, I will create `ctry_pops` to match the data discussed in Chapter 2.
First, I create the data set using the `tribble()` function from `dplyr`.

```{r}
ctry_pops <-
  tribble(
  ~country, ~year_1980,  ~year_1990, ~year_2000, ~year_2010,
  "Canada", 24593, 27791, 31100, 34207,
  "Mexico", 68347, 84634, 99775, 114061,
  "United States", 227225, 249623, 282162, 309326
)
```

Second, I pivot the local data frame using `pivot_longer`.

```{r}
ctry_pops_long <-
  ctry_pops %>%
  pivot_longer(cols = -country, 
               names_to = "year",
               names_prefix = "year_",
               values_ptypes = integer(),
               values_to = "population") 
ctry_pops_long %>%
  kable()
```

Next, I copy the data to PostgreSQL, so that it's a (temporary) table inside the database.^["Temporary" here means that it will disappear once we close our connection to the database.]

```{r}
ctry_pops_db <- copy_to(pg, ctry_pops)
ctry_pops_db
```

```{r}
ctry_pops_db_long <-
  ctry_pops_db %>%
  pivot_longer(cols = -country, 
               names_to = "year",
               names_prefix = "year_",
               values_to = "population") 
```

From the output below, we can see that `dbplyr` has taken care of the tedious business of constructing several statements for us.

```{r}
ctry_pops_db_long %>%
  show_query()
```

And from the following, we can see that the result is the same as it was when using `dplyr` on a local data frame.

```{r}
ctry_pops_db_long %>%
  kable()
```

And we can reverse the `pivot_longer()` using `pivot_wider()`.

```{r}
ctry_pops_db_long %>%
  compute() %>%
  pivot_wider(names_from = year, 
              values_from = population, 
              names_prefix = "year_") %>%
  kable()
```

### pivot and unpivot Functions

