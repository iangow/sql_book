# Experiment Analysis

## Strengths and Limits of Experiment Analysis with SQL

I would reframe this discussion.
We might be using a database because the results of an experiment are stored there.

An alternative approach might have the results of an experiment are extracted and exported to a CSV or Excel file and attached to an email and sent to you for analysis.^[Ew!]
But extracted from where?
If the data are in a database, it would be better to cut out the middleman and just get the data directly.

The limitations of SQL essentially vanish when we access the data using `dbplyr`.
R can do any statistical calculation we can think of, so we have no need of an online statistical calculator (though such calculators can help us frame our analyses and check our results).


Cathy does say that "many databases allow developers to extend SQL functionality with *user-defined functions* (UDFs) ... but they are beyond the scope of this book."
For PostgreSQL there are [PL/Python](https://www.postgresql.org/docs/current/plpython.html) and [PL/R](https://github.com/postgres-plr/plr), which allow creation of functions in Python and R, respectively.
When I first started to use PostgreSQL, these extensions seemed pretty exciting and, because I was maintaining my own databases, I could use them.
But over time, I found maintaining UDFs to be more effort than could be justified and I no longer use them.
Instead, if I need to do analysis in Python or R, I will extract data from the database, do the analysis, then write data back to the database.
While this likely partly reflects the kinds of analysis I do, I think that UDFs are likely to be off limits to most users because of the additional complexity of turning code into UDFs and because many users would lack sufficient database privileges to create these.

## The Data Set

As discussed in @tanimura2021sql, there are four tables used in this chapter.
Here we set up references to each of these tables.

```{r}
#| message: false
library(DBI)
library(tidyverse)
library(dbplyr)
library(knitr)
library(flextable)
library(janitor)
```

```{r}
#| output: false
pg <- dbConnect(RPostgres::Postgres(), 
                bigint = "integer",
                check_interrupts = TRUE)
dbExecute(pg, "SET search_path TO sql_book")
```

```{r}
game_users <- tbl(pg, "game_users")
game_actions <- tbl(pg, "game_actions")
game_purchases <- tbl(pg, "game_purchases")
exp_assignment <- tbl(pg, "exp_assignment")
```

## Types of Experiments

I would reword the first paragraph here to the following for clarity (edits in *italics*):

> There is a wide range of experiments,
> If you can change something that a user, customer, constituent, or other entity experiences, you can in theory test *the effect of* that change *on some outcome*.

### Experiments wtih Binary Outcomes: The Chi-Squared Test

To better match the approach of the book, I essentially create the contingency table in the database.
An alternative approach would have been to `collect()` after `summarize()` and then do the statistical analysis in R.
In fact, many of the functions in R are better set-up for this approach.
However, this means bring more data into R and doing more calculation in R.
If the experiment is very large or you would rather the database server do more of the work, then the approach below may be preferred.

```{r}
cont_tbl <-
  exp_assignment %>%
  left_join(game_actions, by = "user_id") %>%
  group_by(variant, user_id) %>%
  summarize(completed = 
              coalesce(any(action == "onboarding complete", na.rm = TRUE),
                       FALSE),
            .groups = "drop") %>%
  count(variant, completed) %>%
  mutate(completed = if_else(completed, "Yes", "No")) %>%
  pivot_wider(names_from = completed, values_from = n) %>%
  collect()
```

Using the packages `janitor` and `flextable`, I mimic the nicely formatted output shown in the book:

```{r}
cont_tbl %>%
  adorn_totals(c("row", "col")) %>%
  mutate(`% Complete` = prettyNum(Yes/Total * 100, digits = 4)) %>%
  flextable() %>%
  add_header_row(values=c("", "Completed onboarding", ""),
                 colwidths = c(1, 2, 2))
```
Now I can do the Chi-squared test.
I need to turn `variant` into the row names of the contingency table so that we want a simple $2 \times 2$ numeric table as the input to our statistical test and I use `column_to_rownames()` to this end.
I then pipe the result into the base R function `chisq.test()`.
I specified `correct = FALSE` so that my result matched what I got from the [online calculator](https://www.socscistatistics.com/tests/chisquare/default.aspx) I found.
I then display the Chi-squared statistic and the $p$-value.

```{r}
res <- 
  cont_tbl %>%
  column_to_rownames(var = "variant") %>%
  chisq.test(correct = FALSE)

res$statistic
res$p.value
```

### Experiments with Continuous Outcomes: The t-Test

```{r}
amounts <- 
  exp_assignment %>%
  filter(exp_name == 'Onboarding') %>%
  left_join(game_purchases, by = "user_id") %>%
  group_by(variant, user_id) %>%
  summarize(amount = sum(coalesce(amount, 0), na.rm = TRUE),
            .groups = "drop")

t_test_stats <-
  amounts %>%
  group_by(variant) %>%
  summarize(n = n(),
            mean = mean(amount, na.rm = TRUE),
            sd = sd(amount, na.rm = TRUE)) %>%
  collect()

t_test_stats %>%
  kable(digits = 3)
```

We can make a small function that we can pass a data frame to as `df`.
The calculations assume that `df` contains two rows (one for each group) and columns named `mean`, `sd`, and `n` for the mean, standard deviation, and number of observations, respectively, in each group.

```{r}
t_test <- function(df) {
  mean_diff = abs(df$mean[1] - df$mean[2])
  se_diff <- sqrt(sum(df$sd^2 / df$n))
  t_stat <- mean_diff / se_diff
  p <- pt(t_stat, df = sum(df$n))
  p_val <- 2 * min(p, 1 - p)
  return(list("statistic" = t_stat, "p-value" = p_val))
}

t_test(t_test_stats)
```
These values line up with those obtained from the [online calculator](https://www.omnicalculator.com/statistics/t-test) I found.

An alternative approach would be to `collect()` the underlying data and do the $t$-test in R.

```{r}
t_test_data <-
  amounts %>%
  select(variant, amount) %>%
  collect()

t.test(formula = amount ~ variant, data = t_test_data)
```

```{r}
t_test_stats_2 <-
  amounts %>%
  inner_join(game_actions, by = "user_id") %>%
  filter(action == "onboarding complete") %>%
  group_by(variant) %>%
  summarize(n = n(),
            mean = mean(amount, na.rm = TRUE),
            sd = sd(amount, na.rm = TRUE)) %>%
  collect()

t_test_stats_2 %>%
  kable(digits = 3)
```

```{r}
t_test(t_test_stats_2)
```

## Challenges with Experiments and Options for Rescuing Flawed Experiments

### Variant Assignment

### Outliers

```{r}
exp_assignment %>%
  left_join(game_purchases, by = "user_id", keep = TRUE,
            suffix = c("", ".y")) %>%
  inner_join(game_actions, by = "user_id") %>%
  filter(action == "onboarding complete",
         exp_name == 'Onboarding') %>%
  group_by(variant) %>%
  summarize(total_cohorted = n_distinct(user_id),
            purchasers = n_distinct(user_id.y),
            .groups = "drop") %>%
  mutate(pct_purchased = purchasers * 100.0 / total_cohorted) %>%
  kable(digits = 2)
```

### Time Boxing

```{r}
amounts_boxed <- 
  exp_assignment %>%
  filter(exp_name == 'Onboarding') %>%
  mutate(exp_end = exp_date + days(7)) %>%
  left_join(game_purchases, 
            by = join_by(user_id, exp_end >= purch_date)) %>%
  group_by(variant, user_id) %>%
  summarize(amount = sum(coalesce(amount, 0), na.rm = TRUE),
            .groups = "drop")


t_test_stats_boxed <-
  amounts_boxed %>%
  group_by(variant) %>%
  summarize(n = n(),
            mean = mean(amount, na.rm = TRUE),
            sd = sd(amount, na.rm = TRUE)) %>%
  collect()

t_test_stats_boxed %>%
  kable(digits = 3)
```

```{r}
t_test(t_test_stats_boxed)
```
