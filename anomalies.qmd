# Anomaly Detection

```{r}
#| message: false
library(DBI)
library(tidyverse)
library(dbplyr)
library(knitr)
```


```{r}
#| warning: false
#| eval: false 
db <- dbConnect(duckdb::duckdb())

csv_to_db <- function(file) {
  df <- read_csv(file, show_col_types = FALSE)
  dbWriteTable(db, df, name = "earthquakes", append = TRUE)
}

files <- list.files("data", "earthquakes.*csv",
                    full.names = TRUE)
res <- lapply(files, csv_to_db)
```

```{r}
#| warning: false
#| eval: false 
#| include: false
db <- dbConnect(duckdb::duckdb(), dbdir = "data/earthquakes.duckdb")

csv_to_db <- function(file) {
  df <- read_csv(file, show_col_types = FALSE)
  dbWriteTable(db, df, name = "earthquakes", append = TRUE)
}

files <- list.files("data", "earthquakes.*csv",
                    full.names = TRUE)
res <- lapply(files, csv_to_db)

dbDisconnect(db, shutdown = TRUE)
```

```{r}
#| eval: true
#| include: false
db <- dbConnect(duckdb::duckdb(), 
                dbdir = "earthquakes.duckdb",
                read_only = TRUE)
```

```{r}
earthquakes <- tbl(db, "earthquakes")
```

```{r}
earthquakes |>
  filter(!is.na(mag)) |>
  group_by(mag) |>
  summarize(earthquakes = n()) |>
  mutate(pct_earthquakes = 1.0 * earthquakes / 
           sum(earthquakes, na.rm = TRUE)) |>
  arrange(desc(mag)) |>
  collect(n = 5) |>
  kable()
```

```{r}
earthquakes |>
  filter(!is.na(mag)) |>
  group_by(mag) |>
  summarize(earthquakes = n()) |>
  mutate(pct_earthquakes = earthquakes/sum(earthquakes, 
                                           na.rm = TRUE)) |>
  arrange(mag) |>
  collect(n = 5) |>
  kable()
```

```{r}
norcal <-
  earthquakes |>
  filter(!is.na(mag), place == 'Northern California') |>
  group_by(place, mag) |>
  summarize(count = n(), .groups = "drop") 

norcal |> 
  arrange(place, desc(mag)) |> 
  collect(n = 3) |>
  kable()

norcal |> 
  arrange(place, mag) |> 
  collect(n = 3) |>
  kable()
```

```{r}
earthquakes |>
  filter(!is.na(mag), place == 'Northern California') |>
  group_by(place) |>
  window_order(mag) |>
  mutate(percentile = percent_rank()) |>
  group_by(place, mag, percentile) |>
  summarize(count = n(), .groups = "drop") |>
  arrange(place, desc(mag)) |>
  collect(n = 6) |>
  kable()
```
```{r}
earthquakes |>
  filter(!is.na(mag), place == 'Central Alaska') |>
  select(place, mag) |>
  mutate(percentile = ntile(100, order_by = "mag")) |>
  arrange(place, desc(mag)) |>
  collect(n = 5) |>
  kable()
```

```{r}
earthquakes |>
  filter(!is.na(mag), place == 'Central Alaska') |>
  select(place, mag) |>
  group_by(place) |>
  window_order(mag) |>
  mutate(percentile = round(percent_rank() * 100, 0)) |>
  arrange(place, desc(mag)) |>
  show_query()
```

The `ntile()` offered by `dbplyr` works a little differently from other window functions.
Rather than preceding the `mutate` with a `window_order()`, we need to specify an `order_by` argument.^[The reason for this is not clear and I have filed [a feature request](https://github.com/tidyverse/dbplyr/issues/1235) to make the handling of `ntile()` similar to that of other window functions.]

```{r}
cen_ak_quartiles <-
  earthquakes |>
  filter(!is.na(mag), place == 'Central Alaska') |>
  select(place, mag) |>
  group_by(place) |>
  mutate(ntile = ntile(4, order_by = "mag"))

cen_ak_quartiles |>
  group_by(place, ntile) |>
  summarize(maximum = max(mag, na.rm = TRUE),
            minimum = min(mag, na.rm = TRUE),
            .groups = "drop") |>
  arrange(place, desc(ntile)) |>
  kable()
```

```{r}
earthquakes |>
  filter(!is.na(mag), place == 'Central Alaska') |>
  select(place, mag) |>
  group_by(place) |>
  summarize(pct_25 = quantile(mag, probs = 0.25, na.rm = TRUE),
            pct_50 = quantile(mag, probs = 0.50, na.rm = TRUE),
            pct_75 = quantile(mag, probs = 0.75, na.rm = TRUE)) |>
  kable()
```

```{r}
earthquakes |>
  filter(!is.na(mag), place == 'Central Alaska') |>
  summarize(pct_25_mag = quantile(mag, probs = 0.25, na.rm = TRUE),
            pct_25_depth = quantile(depth, probs = 0.25, na.rm = TRUE)) |>
  kable()
```

```{r}
earthquakes |>
  filter(!is.na(mag), 
         place %in% c('Central Alaska', 'Southern Alaska')) |>
  group_by(place) |>
  summarize(pct_25_mag = quantile(mag, probs = 0.25, na.rm = TRUE),
            pct_25_depth = quantile(depth, probs = 0.25, na.rm = TRUE)) |>
  kable()
```

```{r}
earthquakes |>
  summarize(sd_mag = sd(mag, na.rm = TRUE),
            stddev_samp_mag = stddev_samp(mag),
            stddev_pop_mag = stddev_pop(mag)) |>
  mutate(diff_samp = sd_mag - stddev_samp_mag,
         diff_pop = sd_mag - stddev_pop_mag) |>
  collect() |>
  kable()
```

Below Cathy uses `INNER JOIN` with `ON 1 = 1`.

```{sql}
#| eval: false
SELECT * 
FROM earthquakes a
INNER JOIN b ON 1 = 1
```

Instead, I use `CROSS JOIN` (this is `cross_join` in `dplyr`).

```{sql}
#| eval: false
SELECT * 
FROM earthquakes a
CROSS JOIN b
```

The output in the book differs from one gets from running the code, 
so I add `!(mag %in% c(-9, -9.99))` to get closer to the book's output.

Note that in constructing `mag_stats`, I follow the book in using `avg(mag)` and `stddev_pop(mag)`.
In practice, I would probably lean more to using R-compatible `mean(mag, na.rm = TRUE)` and `sd(mag, na.rm = TRUE)`, respectively.
This makes little differ in practice---the only difference is that `sd` is translated into `stddev_samp` instead of `stddev_pop`, which is barely different in this case---but I believe it is helpful to be consistent where possible.
Often I find myself moving the data processing from PosrtgreSQL to R or vice versa and this is much easier if the `dbplyr` code is consistent with the `dplyr` equivalent.

```{r}
mag_stats <-
  earthquakes |>
  filter(!is.na(mag)) |>
  summarize(avg_mag = avg(mag),
            std_dev = stddev_pop(mag))

z_scores <-
  earthquakes |>
  filter(!is.na(mag), !(mag %in% c(-9, -9.99))) |>
  select(place, mag) |>
  cross_join(mag_stats) |>
  mutate(z_score = (mag - avg_mag) / std_dev) 

z_scores |>
  arrange(desc(mag)) |>
  collect(n = 3) |>
  kable()

z_scores |>
  arrange(mag) |>
  collect(n = 3) |>
  kable()
```

## Graphing to find anomalies visually

```{r}
earthquakes |>
  filter(!is.na(mag)) |>
  ggplot(aes(x = mag)) +
  geom_histogram(breaks = seq(-10, 10, 0.1))
```

```{r}
earthquakes |>
  filter(!is.na(mag),
         between(mag, 7.2, 9.5)) |>
  ggplot(aes(x = mag)) +
  geom_histogram(binwidth = 0.1)
```

```{r}
earthquakes |>
  filter(!is.na(mag),
         between(mag, 7.2, 9.5)) |>
  ggplot(aes(x = mag)) +
  geom_bar() +
  scale_x_binned(breaks = seq(7.2, 9.5, 0.1))
```

```{r}
earthquakes |>
  filter(!is.na(mag), !is.na(depth)) |>
  distinct(mag, depth) |>
  ggplot(aes(x = mag, y = depth)) +
  geom_point(size = 0.1, colour = "blue")
```

```{r}
earthquakes |>
  filter(!is.na(mag), !is.na(depth)) |>
  filter(between(mag, 4, 7), depth <= 50) |>
  ggplot(aes(x = mag, y = depth)) +
  geom_count(color = "blue")
```

```{r}
japan_quakes <-
  earthquakes |>
  filter(!is.na(mag), !is.na(depth)) |>
  filter(grepl("Japan", place)) 

japan_quakes |>
  ggplot(aes(y = mag)) +
  geom_boxplot(width = 0.5)
```

```{r}
japan_quakes |>
  summarize(p25 = quantile(mag, probs = 0.25, na.rm = TRUE),
            p50 = quantile(mag, probs = 0.50, na.rm = TRUE),
            p75 = quantile(mag, probs = 0.75, na.rm = TRUE)) |>
  mutate(iqr = (p75 - p25) * 1.5,
         lower_whisker = p25 - (p75 - p25) * 1.5,
         upper_whisker = p75 + (p75 - p25) * 1.5) |>
  kable()
```

```{r}
japan_quakes |>
  select(mag, time) |>
  collect() |>
  mutate(year = as.factor(year(time))) |>
  ggplot(aes(y = mag, x = year, group = year)) +
  geom_boxplot()
```

## Forms of Anomalies

### Anomalous Values

```{r}
earthquakes |>
  filter(mag >= 1.08) |>
  group_by(mag) |>
  summarize(count = n()) |>
  arrange(mag) |>
  collect(n = 5) |>
  kable()
```

```{r}
earthquakes |>
  filter(depth > 600) |>
  group_by(net) |>
  summarize(count = n()) |>
  arrange(net) |>
  collect(n = 5) |>
  kable()
```
```{r}
earthquakes |>
  filter(depth > 600) |>
  group_by(place) |>
  summarize(count = n()) |>
  arrange(place) |>
  collect(n = 5) |>
  kable()
```

```{r}
earthquakes |>
  filter(depth > 600) |>
  mutate(place_name = case_when(grepl(' of ', place) ~
                                  split_part(place, ' of ', 2L),
                                TRUE ~ place)) |>
  group_by(place_name) |>
  summarize(count = n()) |>
  arrange(desc(count)) |>
  collect(n = 3) |>
  kable()
```

```{r}
earthquakes |>
  summarize(distinct_types = n_distinct(type),
            distinct_lower = n_distinct(lower(type))) |>
  kable()
```

### Anomalous Counts or Frequencies

Why use `date_trunc('year',time)::date as earthquake_year`?

```{sql}
#| connection: db
SELECT EXTRACT(year FROM time) AS earthquake_year, 
  COUNT(*) AS earthquakes
FROM earthquakes
GROUP BY 1
ORDER BY 1;
```

```{r}
earthquakes |>
  mutate(earthquake_year = as.character(year(time))) |>
  group_by(earthquake_year) |>
  summarize(earthquakes = n()) |>
  ggplot(aes(x = earthquake_year, y = earthquakes)) +
  geom_bar(stat = "identity")
```

^[Without the `select(earthquake_year)`, the following plot would take 4 seconds to produce---versus 0.4 seconds needed here. 
This suggests that `ggplot` is triggering a `collect()` on fields that it does not need to make the plot.]

```{r}
#| eval: false
earthquakes |>
  mutate(earthquake_year = as.character(year(time))) |>
  select(earthquake_year) |>
  ggplot(aes(x = earthquake_year)) +
  geom_bar()
```
```{r}
earthquakes |>
  mutate(earthquake_month = floor_date(time, "month")) |>
  group_by(earthquake_month) |>
  summarize(earthquakes = n(), .groups = "drop") |>
  ggplot(aes(x = earthquake_month, y = earthquakes)) +
  geom_line()
```

From the book, "it turns out that the increase in earthquakes starting in 2017 can be at least partially explained by the status field. The status indicates whether the event has been reviewed by a human ('reviewed') or was directly posted by a system without review ('automatic')."
This can be seen in the following plot.^[Unlike the plot in the book, I leave observations with "manual" status in the plot, as they are in the SQL query.]

```{r}
earthquakes |>
  mutate(earthquake_month = floor_date(time, "month")) |>
  group_by(earthquake_month, status) |>
  summarize(earthquakes = n(), .groups = "drop") |>
  ggplot(aes(x = earthquake_month, y = earthquakes, color = status)) +
  geom_line()
```


```{r}
earthquakes |>
  filter(mag >= 6) |>
  group_by(place) |>
  summarize(earthquakes = n(), .groups = "drop") |>
  arrange(desc(earthquakes)) |>
  collect(n = 3) |>
  kable()
```

For the next query, it seems easy enough to just put the result in a plot.

```{r}
earthquakes |>
  filter(mag >= 6) |>
  mutate(place = if_else(grepl(' of ', place),
                         split_part(place, ' of ', 2L), 
                         place)) |>
  count(place, name = "earthquakes") |>
  arrange(desc(earthquakes)) |>
  collect(n = 10) |>
  ggplot(aes(y = fct_inorder(place), 
             x = earthquakes)) +
  geom_bar(stat = "identity")
```  
### Anomalies from the Absence of Data

## Handling Anomalies

### Investigation

### Removal

```{r}
earthquakes |>
  filter(!mag %in% c(-9,-9.99)) |>
  select(time, mag, type) |>
  collect(n = 10) |>
  kable()
```

```{r}
earthquakes |>
  summarize(avg_mag = avg(mag),
            avg_mag_adjusted = avg(if_else(mag > -9, mag, NA))) |>
  kable()
```

```{r}
earthquakes |>
  filter(place == 'Yellowstone National Park, Wyoming') |>
  summarize(avg_mag = avg(mag),
            avg_mag_adjusted = avg(if_else(mag > -9, mag, NA))) |>
  kable()
```

### Replacement with Alternate Values

```{r}
earthquakes |>
  mutate(event_type = if_else(type == 'earthquake', type, 'Other')) |>
  count(event_type) |>
  kable()
```

```{r}
extremes <-
  earthquakes |>
  summarize(p95 = quantile(mag, probs = 0.95, na.rm = TRUE),
            p05 = quantile(mag, probs = 0.05, na.rm = TRUE))

extremes |> kable()
```

Note that this SQL from the book^[I simplify the SQL from the book assuming that the table already includes a `CROSS JOIN` with the SQL equivalent of `extremes` and I use shorter variable names for the extremes.]

```{sql}
#| eval: false
CASE 
  WHEN mag > p95 THEN p95
  WHEN mag < p05 THEN p05
  ELSE mag
END AS mag_winsorized
```

can be replaced with a single line:

```{sql}
#| eval: false
LEAST(GREATEST(mag, p05), p95) AS mag_winsorized
```

The R equivalents of `LEAST` and `GREATEST` are `pmin` and `pmax`, respectively.
And `dbplyr` will translate `pmin` and `pmax` for us, so we can get winsorized data as follows.

```{r}
#| warning: false
earthquakes_wins <-
  earthquakes |>
  mutate(mag = if_else(mag %in% c(-9.99, -9), NA, mag)) |>
  filter(!is.na(mag)) |>
  cross_join(extremes) |>
  mutate(mag_winsorized = pmin(pmax(mag, p05), p95)) |>
  select(time, place, mag, mag_winsorized) 

earthquakes_wins |>
  arrange(desc(mag)) |>
  collect(n = 3) |>
  kable()

earthquakes_wins |>
  filter(mag == mag_winsorized) |>
  collect(n = 3) |>
  kable()

earthquakes_wins |>
  arrange(mag) |>
  collect(n = 3) |>
  kable()
```

### Rescaling

In the book, it says `WHERE depth >= 0.05`, but I need to use `WHERE depth > 0.05` to match the results there.

```{r}
quake_depths <-
  earthquakes |>
  filter(depth > 0.05) |>
  mutate(depth = round(depth, 1)) |>
  select(depth)

quake_depths |>
  mutate(log_depth = log(depth, base = 10)) |>
  count(depth, log_depth) |>
  arrange(depth) |>
  collect(n = 3) |>
  kable()

quake_depths |>
  ggplot(aes(x = depth)) +
  geom_histogram(binwidth = 0.1)
```

```{r}
quake_depths |>
  ggplot(aes(x = log(depth, base = 10))) +
  geom_histogram(binwidth = 0.1)
```

```{r}
#| include: FALSE
dbDisconnect(db, shutdown = TRUE)
```
