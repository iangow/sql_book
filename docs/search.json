[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SQL for Data Analysis using R",
    "section": "",
    "text": "Bachelor of Commerce/Bachelor of Laws at the University of New South Wales.↩︎\nI vaguely recall getting a lower grade than my friend for the group assignment, which was inexplicable since there was no indication who did what. I survived.↩︎\nI recall that partners had the higher-powered laptops—necessary for writing Lotus Notes and reading Powerpoint slides—while analysts like me had older laptops. It didn’t make sense to me at the time, and still doesn’t.↩︎"
  },
  {
    "objectID": "sql-intro.html",
    "href": "sql-intro.html",
    "title": "1  Introduction to SQL",
    "section": "",
    "text": "The focus of Tanimura (2021) is on preparing data sets for analysis by business practitioners, who might work in accounting, marketing, financial analysis, human resources, or product management. Data analysis blends computing, statistics, and background business knowledge. A successful data analyst will be able to write queries to get the right data for a particular question, but will also have skills in data visualization and statistical analysis."
  },
  {
    "objectID": "sql-intro.html#why-sql",
    "href": "sql-intro.html#why-sql",
    "title": "1  Introduction to SQL",
    "section": "1.2 Why SQL?",
    "text": "1.2 Why SQL?\nSQL is the lanaguage of databases. So one answer to the question “why use SQL?” invokes Sutton’s Law. An apocryphal story has it that, when asked why he robbed banks, famed bank robber Willie Sutton replied “because that’s where the money is.” We use SQL because databases are where the data are."
  },
  {
    "objectID": "sql-intro.html#sql-the-basics",
    "href": "sql-intro.html#sql-the-basics",
    "title": "1  Introduction to SQL",
    "section": "1.3 SQL: The basics",
    "text": "1.3 SQL: The basics\n\n1.3.1 Setting up your computer\nAssuming that you have the ability to install software and a WRDS account, setting up your computer so that you can run the code in this book is straightforward and takes just a few minutes. We list the required steps below and also provide a video demonstrating these steps here.\n\nDownload and install R. R is available for all major platforms (Windows, Linux, and MacOS) here.\nDownload and install RStudio. An open-source version of RStudio is available here.\nInstall required packages from CRAN. CRAN stands for “Comprehensive R Archive Network” and is the official repository for packages (also known as libraries) made available for R. In this book, we will make use of a number of R packages. These can be installed easily by running the following code in RStudio.1\n\n\ninstall.packages(c(\"DBI\", \"dbplyr\", \"dplyr\", \"duckdb\", \"flextable\",\n    \"janitor\", \"knitr\", \"readxl\", \"stringr\", \"tidyverse\"))\n\n\n1.3.2 Reading in data\nIf you copy and paste the following code into your R console and run it, you will load data into a database table retail_sales, which can be queried using SQL. You may find it easiest to download the template prepared for this chapter.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\n\ndb <- dbConnect(duckdb::duckdb())\n\nretail_sales <-\n  tbl(db, \"read_csv_auto('data/us_retail_sales.csv')\") |>\n  compute(name = \"retail_sales\")\n\n\n\n1.3.3 Table expressions\nA table expression contains a FROM clause and (optionally) WHERE, GROUP BY, and HAVING clauses. The WHERE, GROUP BY, and HAVING clauses specify a pipeline of successive transformations performed on the table specified in the FROM clause. These transformations produce a virtual table containing the rows that are passed to SELECT to compute the output rows of the query.\nPerhaps the most basic form of the SELECT statement is SELECT *, which asks for all columns from the table specified in the FROM clause.\n\nSELECT *\nFROM retail_sales\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nsales_month\nnaics_code\nkind_of_business\nreason_for_null\nsales\n\n\n\n\n1992-01-01\n441\nMotor vehicle and parts dealers\nNA\n29811\n\n\n1992-01-01\n4411\nAutomobile dealers\nNA\n25800\n\n\n1992-01-01\n4411, 4412\nAutomobile and other motor vehicle dealers\nNA\n26788\n\n\n1992-01-01\n44111\nNew car dealers\nNA\n24056\n\n\n1992-01-01\n44112\nUsed car dealers\nNA\n1744\n\n\n1992-01-01\n4413\nAutomotive parts, acc., and tire stores\nNA\n3023\n\n\n1992-01-01\n442\nFurniture and home furnishings stores\nNA\n3846\n\n\n1992-01-01\n442, 443\nFurniture, home furn, electronics, and appliance stores\nNA\n7503\n\n\n1992-01-01\n4421\nFurniture stores\nNA\n2392\n\n\n1992-01-01\n4422\nHome furnishings stores\nNA\n1454\n\n\n\n\n\nBut we can also specify columns that we are interested in by name.\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\nsales_month\nkind_of_business\nsales\n\n\n\n\n1992-01-01\nMotor vehicle and parts dealers\n29811\n\n\n1992-01-01\nAutomobile dealers\n25800\n\n\n1992-01-01\nAutomobile and other motor vehicle dealers\n26788\n\n\n1992-01-01\nNew car dealers\n24056\n\n\n1992-01-01\nUsed car dealers\n1744\n\n\n1992-01-01\nAutomotive parts, acc., and tire stores\n3023\n\n\n1992-01-01\nFurniture and home furnishings stores\n3846\n\n\n1992-01-01\nFurniture, home furn, electronics, and appliance stores\n7503\n\n\n1992-01-01\nFurniture stores\n2392\n\n\n1992-01-01\nHome furnishings stores\n1454\n\n\n\n\n\nThe queries above retrieve all records or rows from a table.2 Often we want to focus on rows meeting certain conditions and the WHERE clause allows us to do this. Here we retrieve all sales occurring in or after January 2002.3\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\nWHERE sales_month >= '2002-01-01'\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\nsales_month\nkind_of_business\nsales\n\n\n\n\n2002-01-01\nMotor vehicle and parts dealers\n60565\n\n\n2002-01-01\nAutomobile dealers\n52948\n\n\n2002-01-01\nAutomobile and other motor vehicle dealers\n55799\n\n\n2002-01-01\nNew car dealers\n48146\n\n\n2002-01-01\nUsed car dealers\n4802\n\n\n2002-01-01\nAutomotive parts, acc., and tire stores\n4766\n\n\n2002-01-01\nFurniture and home furnishings stores\n7149\n\n\n2002-01-01\nFurniture, home furn, electronics, and appliance stores\n14342\n\n\n2002-01-01\nFurniture stores\n4097\n\n\n2002-01-01\nHome furnishings stores\n3052\n\n\n\n\n\n\n\n1.3.4 Functions and operators\nIn the WHERE clause above we specified sales_month >= '2002-01-01'. We naturally think of this as being either TRUE or FALSE for a given observation, but there is also a third option that we will discuss in a moment and which allows us to handle cases where we do not know the value for sales_month`.\nSQL contains the usual range of comparison operators: <, >, <=, >=, =, and !=, where = means “equal” and both <> and != mean “not equal”.\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\nWHERE kind_of_business = 'Used car dealers'\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nkind_of_business\nsales\n\n\n\n\n1992-01-01\nUsed car dealers\n1744\n\n\n1992-02-01\nUsed car dealers\n1990\n\n\n1992-03-01\nUsed car dealers\n2177\n\n\n1992-04-01\nUsed car dealers\n2601\n\n\n1992-05-01\nUsed car dealers\n2171\n\n\n1992-06-01\nUsed car dealers\n2207\n\n\n1992-07-01\nUsed car dealers\n2251\n\n\n1992-08-01\nUsed car dealers\n2087\n\n\n1992-09-01\nUsed car dealers\n2016\n\n\n1992-10-01\nUsed car dealers\n2149\n\n\n\n\n\nApart from the comparison operators, SQL includes the usual mathematical operators, including +, -, *, and /.\nAll SQL implementations include a multitude of functions, including mathematical functions such as abs(), exp(), log(), ln(), and sqrt().\n\n\n1.3.5 Summarizing data\nAn important class\n\nSELECT kind_of_business, sum(sales) AS total_sales\nFROM retail_sales\nGROUP BY kind_of_business\nORDER BY total_sales DESC;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\nkind_of_business\ntotal_sales\n\n\n\n\nRetail and food services sales, total\n118053993\n\n\nRetail sales and food services excl gasoline stations\n107701613\n\n\nRetail sales, total\n105580364\n\n\nRetail sales and food services excl motor vehicle and parts\n93509935\n\n\nRetail sales and food services excl motor vehicle and parts and gasoline stations\n83157555\n\n\nRetail sales, total (excl. motor vehicle and parts dealers)\n81036306\n\n\nGAFO(1)\n29041144\n\n\nMotor vehicle and parts dealers\n24544058\n\n\nAutomobile and other motor vehicle dealers\n22462364\n\n\nAutomobile dealers\n20963805\n\n\n\n\n\n\nSELECT kind_of_business, sum(sales) AS total_sales\nFROM retail_sales\nGROUP BY 1\nORDER BY 2 DESC;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\nkind_of_business\ntotal_sales\n\n\n\n\nRetail and food services sales, total\n118053993\n\n\nRetail sales and food services excl gasoline stations\n107701613\n\n\nRetail sales, total\n105580364\n\n\nRetail sales and food services excl motor vehicle and parts\n93509935\n\n\nRetail sales and food services excl motor vehicle and parts and gasoline stations\n83157555\n\n\nRetail sales, total (excl. motor vehicle and parts dealers)\n81036306\n\n\nGAFO(1)\n29041144\n\n\nMotor vehicle and parts dealers\n24544058\n\n\nAutomobile and other motor vehicle dealers\n22462364\n\n\nAutomobile dealers\n20963805\n\n\n\n\n\n\nSELECT year(sales_month) AS year,\n  sum(sales) AS total_sales\nFROM retail_sales\nWHERE kind_of_business = 'Used car dealers'\nGROUP BY 1\nORDER BY 2 DESC;\n\n\nDisplaying records 1 - 10\n\n\nyear\ntotal_sales\n\n\n\n\n2020\n124040\n\n\n2019\n119067\n\n\n2018\n114390\n\n\n2017\n112174\n\n\n2016\n105905\n\n\n2015\n98330\n\n\n2014\n90424\n\n\n2013\n83916\n\n\n2012\n80228\n\n\n2007\n79696\n\n\n\n\n\n\nSELECT year(sales_month) AS year,\n  sum(sales) AS total_sales\nFROM retail_sales\nWHERE kind_of_business = 'Used car dealers'\nGROUP BY 1\nHAVING total_sales >= 100000\nORDER BY 2 DESC;\n\n\n5 records\n\n\nyear\ntotal_sales\n\n\n\n\n2020\n124040\n\n\n2019\n119067\n\n\n2018\n114390\n\n\n2017\n112174\n\n\n2016\n105905\n\n\n\n\n\n\nSELECT kind_of_business, reason_for_null,\n  count(*)\nFROM retail_sales\nWHERE sales IS NULL\nGROUP BY 1, 2\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\nkind_of_business\nreason_for_null\ncount_star()\n\n\n\n\nAll other home furnishings stores\nNot Available\n108\n\n\nElectronics stores\nNot Available\n60\n\n\nPaint and wallpaper stores\nNot Available\n108\n\n\nSupermarkets and other grocery (except convenience) stores\nNot Available\n108\n\n\nOther clothing stores\nNot Available\n108\n\n\nFloor covering stores\nSupressed\n46\n\n\nDrinking places\nSupressed\n34\n\n\nJewelry stores\nSupressed\n3\n\n\nFull service restaurants\nSupressed\n11\n\n\nHome furnishings stores\nSupressed\n3\n\n\n\n\n\n\n\n1.3.6 What have we not covered?\n\nJoins\nSubqueries and common table expressions (CTEs)\n\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "dbplyr-intro.html",
    "href": "dbplyr-intro.html",
    "title": "2  Using dplyr with databases",
    "section": "",
    "text": "Chapter 2 of Tanimura (2021) provides a good foundation discussion of issues related to preparing data for analysis. While the discussion is couched in terms of SQL, in reality the issues are not specific to SQL or databases. For this reason, I recommend that you read the chapter.\nIn this chapter, I instead show how dplyr, a core Tidyverse package, can be used with SQL databases."
  },
  {
    "objectID": "dbplyr-intro.html#introduction-to-dplyr",
    "href": "dbplyr-intro.html#introduction-to-dplyr",
    "title": "2  Using dplyr with databases",
    "section": "2.1 Introduction to dplyr",
    "text": "2.1 Introduction to dplyr\nOur focus on using R and dplyr to access SQL databases means that we only need to learn a subset of the functionality of R.\nWhile R offers a number of data structures—including vectors, lists, and matrices—we will focus primarily on data frames, which are R’s equivalents of tables in SQL databases. R also offers rich functionality for statistical testing and modelling, but we will make little use of this here.\n\nlibrary(DBI)\nlibrary(tidyverse)\n\n\n2.1.1 Introduction to SQL\n\ndb <- dbConnect(duckdb::duckdb())\n\nretail_sales <-\n  tbl(db, \"read_csv_auto('data/us_retail_sales.csv')\") |>\n  compute(name = \"retail_sales\")\n\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\n\n\nretail_sales |>\n  select(sales_month, kind_of_business, sales)\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n   sales_month kind_of_business                                        sales\n   <date>      <chr>                                                   <dbl>\n 1 1992-01-01  Motor vehicle and parts dealers                         29811\n 2 1992-01-01  Automobile dealers                                      25800\n 3 1992-01-01  Automobile and other motor vehicle dealers              26788\n 4 1992-01-01  New car dealers                                         24056\n 5 1992-01-01  Used car dealers                                         1744\n 6 1992-01-01  Automotive parts, acc., and tire stores                  3023\n 7 1992-01-01  Furniture and home furnishings stores                    3846\n 8 1992-01-01  Furniture, home furn, electronics, and appliance stores  7503\n 9 1992-01-01  Furniture stores                                         2392\n10 1992-01-01  Home furnishings stores                                  1454\n# ℹ more rows\n\n\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\nWHERE sales_month >= '2002-01-01'\n\n\nretail_sales |>\n  select(sales_month, kind_of_business, sales) |>\n  filter(sales_month >= '2002-01-01')\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n   sales_month kind_of_business                                        sales\n   <date>      <chr>                                                   <dbl>\n 1 2002-01-01  Motor vehicle and parts dealers                         60565\n 2 2002-01-01  Automobile dealers                                      52948\n 3 2002-01-01  Automobile and other motor vehicle dealers              55799\n 4 2002-01-01  New car dealers                                         48146\n 5 2002-01-01  Used car dealers                                         4802\n 6 2002-01-01  Automotive parts, acc., and tire stores                  4766\n 7 2002-01-01  Furniture and home furnishings stores                    7149\n 8 2002-01-01  Furniture, home furn, electronics, and appliance stores 14342\n 9 2002-01-01  Furniture stores                                         4097\n10 2002-01-01  Home furnishings stores                                  3052\n# ℹ more rows\n\n\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\nWHERE \n\n\nUse == instead of =.\nCan use \" or ' for quotes.\n\n\nretail_sales |>\n  select(sales_month, kind_of_business, sales) |>\n  filter(kind_of_business == 'Used car dealers')\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n   sales_month kind_of_business sales\n   <date>      <chr>            <dbl>\n 1 1992-01-01  Used car dealers  1744\n 2 1992-02-01  Used car dealers  1990\n 3 1992-03-01  Used car dealers  2177\n 4 1992-04-01  Used car dealers  2601\n 5 1992-05-01  Used car dealers  2171\n 6 1992-06-01  Used car dealers  2207\n 7 1992-07-01  Used car dealers  2251\n 8 1992-08-01  Used car dealers  2087\n 9 1992-09-01  Used car dealers  2016\n10 1992-10-01  Used car dealers  2149\n# ℹ more rows\n\n\n\nSELECT kind_of_business, sum(sales) AS total_sales\nFROM retail_sales\nGROUP BY kind_of_business\nORDER BY total_sales DESC;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\nkind_of_business\ntotal_sales\n\n\n\n\nRetail and food services sales, total\n118053993\n\n\nRetail sales and food services excl gasoline stations\n107701613\n\n\nRetail sales, total\n105580364\n\n\nRetail sales and food services excl motor vehicle and parts\n93509935\n\n\nRetail sales and food services excl motor vehicle and parts and gasoline stations\n83157555\n\n\nRetail sales, total (excl. motor vehicle and parts dealers)\n81036306\n\n\nGAFO(1)\n29041144\n\n\nMotor vehicle and parts dealers\n24544058\n\n\nAutomobile and other motor vehicle dealers\n22462364\n\n\nAutomobile dealers\n20963805\n\n\n\n\n\n\nuse na.rm = TRUE\nPut new variable name to the left of aggregate function\nUse arrange() instead of ORDER BY\nUse desc() instead of DESC\nNo need to separately select GROUP BY variables.\nNo exact equivalent to reference by order\n\n\nretail_sales |>\n  group_by(kind_of_business) |>\n  summarize(total_sales = sum(sales, na.rm = TRUE)) |>\n  arrange(desc(total_sales))\n\n# Source:     SQL [?? x 2]\n# Database:   DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n# Ordered by: desc(total_sales)\n   kind_of_business                                                  total_sales\n   <chr>                                                                   <dbl>\n 1 Retail and food services sales, total                               118053993\n 2 Retail sales and food services excl gasoline stations               107701613\n 3 Retail sales, total                                                 105580364\n 4 Retail sales and food services excl motor vehicle and parts          93509935\n 5 Retail sales and food services excl motor vehicle and parts and …    83157555\n 6 Retail sales, total (excl. motor vehicle and parts dealers)          81036306\n 7 GAFO(1)                                                              29041144\n 8 Motor vehicle and parts dealers                                      24544058\n 9 Automobile and other motor vehicle dealers                           22462364\n10 Automobile dealers                                                   20963805\n# ℹ more rows\n\n\n\nretail_sales |>\n  group_by(kind_of_business) |>\n  summarize(total_sales = sum(sales, na.rm = TRUE)) |>\n  arrange(desc(total_sales))\n\n# Source:     SQL [?? x 2]\n# Database:   DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n# Ordered by: desc(total_sales)\n   kind_of_business                                                  total_sales\n   <chr>                                                                   <dbl>\n 1 Retail and food services sales, total                               118053993\n 2 Retail sales and food services excl gasoline stations               107701613\n 3 Retail sales, total                                                 105580364\n 4 Retail sales and food services excl motor vehicle and parts          93509935\n 5 Retail sales and food services excl motor vehicle and parts and …    83157555\n 6 Retail sales, total (excl. motor vehicle and parts dealers)          81036306\n 7 GAFO(1)                                                              29041144\n 8 Motor vehicle and parts dealers                                      24544058\n 9 Automobile and other motor vehicle dealers                           22462364\n10 Automobile dealers                                                   20963805\n# ℹ more rows\n\n\n\nSELECT year(sales_month) AS year,\n  sum(sales) AS total_sales\nFROM retail_sales\nWHERE kind_of_business = 'Used car dealers'\nGROUP BY 1\nORDER BY 2 DESC;\n\n\nDisplaying records 1 - 10\n\n\nyear\ntotal_sales\n\n\n\n\n2020\n124040\n\n\n2019\n119067\n\n\n2018\n114390\n\n\n2017\n112174\n\n\n2016\n105905\n\n\n2015\n98330\n\n\n2014\n90424\n\n\n2013\n83916\n\n\n2012\n80228\n\n\n2007\n79696\n\n\n\n\n\n\nretail_sales |>\n  mutate(year = year(sales_month)) |>\n  filter(kind_of_business == 'Used car dealers') |>\n  group_by(year) |>\n  summarize(total_sales = sum(sales, na.rm = TRUE)) |>\n  arrange(desc(total_sales))\n\n# Source:     SQL [?? x 2]\n# Database:   DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n# Ordered by: desc(total_sales)\n    year total_sales\n   <dbl>       <dbl>\n 1  2020      124040\n 2  2019      119067\n 3  2018      114390\n 4  2017      112174\n 5  2016      105905\n 6  2015       98330\n 7  2014       90424\n 8  2013       83916\n 9  2012       80228\n10  2007       79696\n# ℹ more rows\n\n\n\nSELECT year(sales_month) AS year,\n  sum(sales) AS total_sales\nFROM retail_sales\nWHERE kind_of_business = 'Used car dealers'\nGROUP BY 1\nHAVING total_sales >= 100000\nORDER BY 2 DESC;\n\n\n5 records\n\n\nyear\ntotal_sales\n\n\n\n\n2020\n124040\n\n\n2019\n119067\n\n\n2018\n114390\n\n\n2017\n112174\n\n\n2016\n105905\n\n\n\n\n\n\nretail_sales |>\n  mutate(year = year(sales_month)) |>\n  filter(kind_of_business == 'Used car dealers') |>\n  group_by(year) |>\n  summarize(total_sales = sum(sales, na.rm = TRUE)) |>\n  filter(total_sales >= 100000) |>\n  arrange(desc(total_sales)) \n\n# Source:     SQL [5 x 2]\n# Database:   DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n# Ordered by: desc(total_sales)\n   year total_sales\n  <dbl>       <dbl>\n1  2020      124040\n2  2019      119067\n3  2018      114390\n4  2017      112174\n5  2016      105905\n\n\n\nSELECT kind_of_business, reason_for_null,\n  count(*) AS num_null\nFROM retail_sales\nWHERE sales IS NULL\nGROUP BY 1, 2\nORDER BY 3 DESC\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\nkind_of_business\nreason_for_null\nnum_null\n\n\n\n\nAll other home furnishings stores\nNot Available\n108\n\n\nPaint and wallpaper stores\nNot Available\n108\n\n\nSupermarkets and other grocery (except convenience) stores\nNot Available\n108\n\n\nOther clothing stores\nNot Available\n108\n\n\nElectronics stores\nNot Available\n60\n\n\nFloor covering stores\nSupressed\n46\n\n\nDrinking places\nSupressed\n34\n\n\nFull service restaurants\nSupressed\n11\n\n\nJewelry stores\nSupressed\n3\n\n\nHome furnishings stores\nSupressed\n3\n\n\n\n\n\n\nis.na() instead of IS NULL\n\n\nretail_sales |>\n  filter(is.na(sales)) |>\n  group_by(kind_of_business, reason_for_null) |>\n  summarize(num_null = n(), .groups = \"drop\") %>%\n  arrange(desc(num_null))\n\n# Source:     SQL [?? x 3]\n# Database:   DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n# Ordered by: desc(num_null)\n   kind_of_business                                     reason_for_null num_null\n   <chr>                                                <chr>              <dbl>\n 1 All other home furnishings stores                    Not Available        108\n 2 Paint and wallpaper stores                           Not Available        108\n 3 Supermarkets and other grocery (except convenience)… Not Available        108\n 4 Other clothing stores                                Not Available        108\n 5 Electronics stores                                   Not Available         60\n 6 Floor covering stores                                Supressed             46\n 7 Drinking places                                      Supressed             34\n 8 Full service restaurants                             Supressed             11\n 9 Jewelry stores                                       Supressed              3\n10 Home furnishings stores                              Supressed              3\n# ℹ more rows\n\n\n\n\n2.1.2 Missing data\nThe SQL in the book generally uses the form x::date rather than the more standard SQL CAST(x AS DATE). In dbplyr, we would use as.Date(x) and dbplyr would translate as CAST(x AS DATE). The following code and output demonstrates how dbplyr translated from dplyr to SQL.\nThe table stored in dates_processed below is equivalent to that created and stored in the database as date_dim in the code supplied with book. This date_dim table is only used in #sec-time-series of the book and we will not even use it there (for reasons to be explained).\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(knitr)\n\n\ndb <- dbConnect(duckdb::duckdb(), bigint = \"integer\")"
  },
  {
    "objectID": "dbplyr-intro.html#preparing-shaping-data",
    "href": "dbplyr-intro.html#preparing-shaping-data",
    "title": "2  Using dplyr with databases",
    "section": "2.2 Preparing: Shaping Data",
    "text": "2.2 Preparing: Shaping Data\n\n2.2.1 For Which Output: BI, Visualization, Statistics, ML\n\n\n2.2.2 Pivoting with CASE Statements\n\n\n2.2.3 Unpivoting with UNION Statements\nA user of dplyr has access to the functions pivot_wider and pivot_longer, which make it much easier to “pivot” and “unpivot” tables than using CASE statements, which could become long and tedious.\nTo illustrate the dplyr way of doing things, I will create ctry_pops to match the data discussed in Chapter 2. First, I create the data set using the tribble() function from dplyr.\n\nctry_pops <-\n  tribble(\n  ~country, ~year_1980,  ~year_1990, ~year_2000, ~year_2010,\n  \"Canada\", 24593, 27791, 31100, 34207,\n  \"Mexico\", 68347, 84634, 99775, 114061,\n  \"United States\", 227225, 249623, 282162, 309326\n)\n\nSecond, I pivot the local data frame using pivot_longer.\n\nctry_pops_long <-\n  ctry_pops |>\n  pivot_longer(cols = -country, \n               names_to = \"year\",\n               names_prefix = \"year_\",\n               values_ptypes = integer(),\n               values_to = \"population\") \nctry_pops_long |>\n  kable()\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\nCanada\n1980\n24593\n\n\nCanada\n1990\n27791\n\n\nCanada\n2000\n31100\n\n\nCanada\n2010\n34207\n\n\nMexico\n1980\n68347\n\n\nMexico\n1990\n84634\n\n\nMexico\n2000\n99775\n\n\nMexico\n2010\n114061\n\n\nUnited States\n1980\n227225\n\n\nUnited States\n1990\n249623\n\n\nUnited States\n2000\n282162\n\n\nUnited States\n2010\n309326\n\n\n\n\n\nNext, I copy the data to PostgreSQL, so that it’s a (temporary) table inside the database.1\n\nctry_pops_db <- copy_to(db, ctry_pops)\nctry_pops_db\n\n# Source:   table<ctry_pops> [3 x 5]\n# Database: DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n  country       year_1980 year_1990 year_2000 year_2010\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 Canada            24593     27791     31100     34207\n2 Mexico            68347     84634     99775    114061\n3 United States    227225    249623    282162    309326\n\n\n\nctry_pops_db_long <-\n  ctry_pops_db |>\n  pivot_longer(cols = -country, \n               names_to = \"year\",\n               names_prefix = \"year_\",\n               values_to = \"population\") \n\nFrom the output below, we can see that dbplyr has taken care of the tedious business of constructing several statements for us.\n\nctry_pops_db_long |>\n  show_query()\n\n<SQL>\n(\n  (\n    (\n      SELECT country, '1980' AS \"year\", year_1980 AS population\n      FROM ctry_pops\n    )\n    UNION ALL\n    (\n      SELECT country, '1990' AS \"year\", year_1990 AS population\n      FROM ctry_pops\n    )\n  )\n  UNION ALL\n  (\n    SELECT country, '2000' AS \"year\", year_2000 AS population\n    FROM ctry_pops\n  )\n)\nUNION ALL\n(\n  SELECT country, '2010' AS \"year\", year_2010 AS population\n  FROM ctry_pops\n)\n\n\nAnd from the following, we can see that the result is the same as it was when using dplyr on a local data frame.\n\nctry_pops_db_long |>\n  kable()\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\nCanada\n1980\n24593\n\n\nMexico\n1980\n68347\n\n\nUnited States\n1980\n227225\n\n\nCanada\n1990\n27791\n\n\nMexico\n1990\n84634\n\n\nUnited States\n1990\n249623\n\n\nCanada\n2000\n31100\n\n\nMexico\n2000\n99775\n\n\nUnited States\n2000\n282162\n\n\nCanada\n2010\n34207\n\n\nMexico\n2010\n114061\n\n\nUnited States\n2010\n309326\n\n\n\n\n\nAnd we can reverse the pivot_longer() using pivot_wider().\n\nctry_pops_db_long |>\n  compute() |>\n  pivot_wider(names_from = year, \n              values_from = population, \n              names_prefix = \"year_\") |>\n  kable()\n\n\n\n\ncountry\nyear_1980\nyear_1990\nyear_2000\nyear_2010\n\n\n\n\nCanada\n24593\n27791\n31100\n34207\n\n\nMexico\n68347\n84634\n99775\n114061\n\n\nUnited States\n227225\n249623\n282162\n309326\n\n\n\n\n\n\n\n2.2.4 pivot and unpivot Functions\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "time-series.html",
    "href": "time-series.html",
    "title": "3  Time Series Analysis",
    "section": "",
    "text": "Chapter 3 of Tanimura (2021) is really two chapters. The first part of the chapter discusses some finer details of dates, date-times, and time stamps. This material is important, but a little technical and bewildering without some concrete use cases. This complexity is compounded by the use of DuckDB here and PostgreSQL in Tanimura (2021), as some details work a little differently from one backend to the other. Finally, while there are some interesting issues in using dbplyr with timestamp data, there probably need data and settings to fully comprehend. As such, I have put the code related to this part of Chapter 3 of Tanimura (2021) in Section 1, part of the appendix.\nThe second part of the chapter is where Tanimura (2021) really starts to take off. This is where it starts playing around with real data."
  },
  {
    "objectID": "time-series.html#sec-retail-data",
    "href": "time-series.html#sec-retail-data",
    "title": "3  Time Series Analysis",
    "section": "3.1 The Retail Sales Data Set",
    "text": "3.1 The Retail Sales Data Set\nWe read the data\n\ndb <- dbConnect(duckdb::duckdb())\n\nretail_sales <-\n  tbl(db, \"read_csv_auto('data/us_retail_sales.csv')\") |>\n  compute(name = \"retail_sales\")\n\nNow that we have the data in R, a few questions arise.\nFirst, why would we move it to a database? Second, how can we move it to a database? Related to the previous question will be: which database to we want to move it to?\nTaking these questions in turn, the first one is a good question. With retail_sales as a local data frame, we can run almost all the analyses below with only the slightest modifications. The modifications needed are to replace every instance of window_order() with arrange().1 The “almost all” relates to the moving average, which relies on window_frame() from dbplyr, which has no exact equivalent in dplyr.\nSo the first point is that “almost all” implies an advantage for using dbplyr. On occasion, the SQL engine provided by PostgreSQL will allow us to do some data manipulations more easily than we can in R. But of course, there are many cases when the opposite is true. That said, why not have both? Store data in a database and collect() as necessary to do things that R is better at?\nSecond, performance can be better using SQL. Compiling a version of this chapter using dplyr with a local data frame for the remainder took just under 14 seconds. Using a PostgreSQL backend, it took 8.5 seconds. Adding back in the queries with window_order() that I removed so that I could compile with dplyr and using DuckDB as the backend, the document took 8.3 seconds to compile (this beat out PostgreSQL doing the same in 9.6 seconds). While these differences are not practically very significant, they could be with more demanding tasks. Also, a database will not always beat R or Python for performance, but often will and having the option to use a database backend is a good thing.\nThird, having the data in a database allows you to interrogate the data using SQL. If you are more familiar with SQL, or just know how to do a particular task in SQL, this can be beneficial.2\nFourth, I think there is merit in separating the tasks of acquiring and cleaning data from the task of analysing those data. Many data analysts have a work flow that entails ingesting and cleaning data for each analysis task. My experience is that it is often better to do the ingesting and cleaning once and then reuse the cleaned data in subsequent analyses. A common pattern involves reusing the same data in many analyses and it can be helpful to divide the tasks in a way that using an SQL database encourages. Also the skills in ingesting and cleaning data can be different from those for analysing those data, so sometimes it makes sense for one person to do one task, push the data to a database, and then have someone else do some or all of the analysis.\nRegarding the second question, there are a few options. But for this chapter we will use duckdb\nTo install DuckDB, all we have to do is install.packages(\"duckdb\").\nThen we can create a connection as follows.\nHere we use the default of an in-memory database. At the end of this chapter, we discuss how we could store the data in a file (say, legislators.duckdb) if we want persistent storage."
  },
  {
    "objectID": "time-series.html#trending-the-data",
    "href": "time-series.html#trending-the-data",
    "title": "3  Time Series Analysis",
    "section": "3.2 Trending the Data",
    "text": "3.2 Trending the Data\n\n3.2.1 Simple Trends\n\nSELECT sales_month, sales\nFROM retail_sales\nWHERE kind_of_business = 'Retail and food services sales, total'\nORDER BY 1\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nsales\n\n\n\n\n1992-01-01\n146376\n\n\n1992-02-01\n147079\n\n\n1992-03-01\n159336\n\n\n1992-04-01\n163669\n\n\n1992-05-01\n170068\n\n\n1992-06-01\n168663\n\n\n1992-07-01\n169890\n\n\n1992-08-01\n170364\n\n\n1992-09-01\n164617\n\n\n1992-10-01\n173655\n\n\n\n\n\n\nretail_sales |>\n    filter(kind_of_business == 'Retail and food services sales, total') |>\n  select(sales_month, sales) |>\n  ggplot(aes(x = sales_month, y = sales)) +\n  geom_line()\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year,\n    sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business = 'Retail and food services sales, total'\nGROUP BY 1\n;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nsales\n\n\n\n\n1992\n2014102\n\n\n1993\n2153095\n\n\n1994\n2330235\n\n\n1995\n2450628\n\n\n1996\n2603794\n\n\n1997\n2726131\n\n\n1998\n2852956\n\n\n1999\n3086990\n\n\n2000\n3287537\n\n\n2001\n3378906\n\n\n\n\n\n\nretail_sales |>\n  filter(kind_of_business == 'Retail and food services sales, total') |>\n  mutate(sales_year = year(sales_month)) |>\n  group_by(sales_year) |>\n  summarize(sales = sum(sales, na.rm = TRUE)) |>\n  ggplot(aes(x = sales_year, y = sales)) +\n  geom_line()\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year, \n  kind_of_business, sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business IN \n          ('Book stores',\n           'Sporting goods stores',\n           'Hobby, toy, and game stores')\nGROUP BY 1,2\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nkind_of_business\nsales\n\n\n\n\n1992\nBook stores\n8327\n\n\n1992\nSporting goods stores\n15583\n\n\n1992\nHobby, toy, and game stores\n11251\n\n\n1993\nBook stores\n9108\n\n\n1993\nSporting goods stores\n16791\n\n\n1993\nHobby, toy, and game stores\n11651\n\n\n1994\nBook stores\n10107\n\n\n1994\nSporting goods stores\n18825\n\n\n1994\nHobby, toy, and game stores\n12850\n\n\n1995\nBook stores\n11196\n\n\n\n\n\n\n\n3.2.2 Comparing Components\n\nretail_sales |>\n  filter(kind_of_business %in% \n           c('Book stores',\n             'Sporting goods stores',\n             'Hobby, toy, and game stores')) |>\n  mutate(sales_year = year(sales_month)) |>\n  group_by(sales_year, kind_of_business) |>\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") |>\n  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\nWHERE kind_of_business IN ('Men''s clothing stores','Women''s clothing stores')\nORDER BY 1,2;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nkind_of_business\nsales\n\n\n\n\n1992-01-01\nMen’s clothing stores\n701\n\n\n1992-01-01\nWomen’s clothing stores\n1873\n\n\n1992-02-01\nMen’s clothing stores\n658\n\n\n1992-02-01\nWomen’s clothing stores\n1991\n\n\n1992-03-01\nMen’s clothing stores\n731\n\n\n1992-03-01\nWomen’s clothing stores\n2403\n\n\n1992-04-01\nMen’s clothing stores\n816\n\n\n1992-04-01\nWomen’s clothing stores\n2665\n\n\n1992-05-01\nMen’s clothing stores\n856\n\n\n1992-05-01\nWomen’s clothing stores\n2752\n\n\n\n\n\n\nretail_sales |>\n  filter(kind_of_business %in% c(\"Men's clothing stores\",\n                                 \"Women's clothing stores\")) |>\n  select(sales_month, kind_of_business, sales) |>\n  ggplot(aes(x = sales_month, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year,\n  kind_of_business, sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business IN \n        ('Men''s clothing stores',\n        'Women''s clothing stores')\nGROUP BY 1, 2\nORDER BY 1, 2;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nkind_of_business\nsales\n\n\n\n\n1992\nMen’s clothing stores\n10179\n\n\n1992\nWomen’s clothing stores\n31815\n\n\n1993\nMen’s clothing stores\n9962\n\n\n1993\nWomen’s clothing stores\n32350\n\n\n1994\nMen’s clothing stores\n10032\n\n\n1994\nWomen’s clothing stores\n30585\n\n\n1995\nMen’s clothing stores\n9315\n\n\n1995\nWomen’s clothing stores\n28696\n\n\n1996\nMen’s clothing stores\n9546\n\n\n1996\nWomen’s clothing stores\n28238\n\n\n\n\n\n\nretail_sales |>\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) |>\n  mutate(sales_year = year(sales_month)) |>\n  group_by(sales_year, kind_of_business) |>\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") |>\n  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT date_part('year', sales_month) AS sales_year,\n  sum(CASE WHEN kind_of_business = 'Women''s clothing stores' \n          then sales \n          END) AS womens_sales,\n  sum(CASE WHEN kind_of_business = 'Men''s clothing stores' \n          then sales \n          END) AS mens_sales\nFROM retail_sales\nWHERE kind_of_business IN \n   ('Men''s clothing stores',\n    'Women''s clothing stores')\nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nwomens_sales\nmens_sales\n\n\n\n\n1992\n31815\n10179\n\n\n1993\n32350\n9962\n\n\n1994\n30585\n10032\n\n\n1995\n28696\n9315\n\n\n1996\n28238\n9546\n\n\n1997\n27822\n10069\n\n\n1998\n28332\n10196\n\n\n1999\n29549\n9667\n\n\n2000\n31447\n9507\n\n\n2001\n31453\n8625\n\n\n\n\n\n\npivoted_sales <-\n  retail_sales |>\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) |>\n  mutate(kind_of_business = if_else(kind_of_business == \"Women's clothing stores\",\n                                    \"womens\", \"mens\"),\n         sales_year = year(sales_month)) |>\n  group_by(sales_year, kind_of_business) |>\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") |>\n  pivot_wider(id_cols = \"sales_year\",\n              names_from = \"kind_of_business\",\n              names_glue = \"{kind_of_business}_{.value}\",\n              values_from = \"sales\")  \n\npivoted_sales |>\n  show_query()\n\n<SQL>\nSELECT\n  sales_year,\n  MAX(CASE WHEN (kind_of_business = 'mens') THEN sales END) AS mens_sales,\n  MAX(CASE WHEN (kind_of_business = 'womens') THEN sales END) AS womens_sales\nFROM (\n  SELECT sales_year, kind_of_business, SUM(sales) AS sales\n  FROM (\n    SELECT\n      sales_month,\n      naics_code,\n      CASE WHEN (kind_of_business = 'Women''s clothing stores') THEN 'womens' WHEN NOT (kind_of_business = 'Women''s clothing stores') THEN 'mens' END AS kind_of_business,\n      reason_for_null,\n      sales,\n      EXTRACT(year FROM sales_month) AS sales_year\n    FROM retail_sales\n    WHERE (kind_of_business IN ('Men''s clothing stores', 'Women''s clothing stores'))\n  ) q01\n  GROUP BY sales_year, kind_of_business\n) q02\nGROUP BY sales_year\n\npivoted_sales |>\n  arrange(sales_year) |>\n  collect(n = 10) |>\n  kable()\n\n\n\n\nsales_year\nmens_sales\nwomens_sales\n\n\n\n\n1992\n10179\n31815\n\n\n1993\n9962\n32350\n\n\n1994\n10032\n30585\n\n\n1995\n9315\n28696\n\n\n1996\n9546\n28238\n\n\n1997\n10069\n27822\n\n\n1998\n10196\n28332\n\n\n1999\n9667\n29549\n\n\n2000\n9507\n31447\n\n\n2001\n8625\n31453\n\n\n\n\n\n\npivoted_sales |>\n  filter(sales_year <= 2019) |>\n  group_by(sales_year) |>\n  mutate(womens_minus_mens = womens_sales - mens_sales,\n         mens_minus_womens = mens_sales - womens_sales) |>\n  select(sales_year, womens_minus_mens, mens_minus_womens) |>\n  ggplot(aes(y = womens_minus_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\npivoted_sales |>\n  filter(sales_year <= 2019) |>\n  group_by(sales_year) |>\n  mutate(womens_times_of_mens = womens_sales / mens_sales) |>\n  ggplot(aes(y = womens_times_of_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\npivoted_sales |>\n  filter(sales_year <= 2019) |>\n  group_by(sales_year) |>\n  mutate(womens_pct_of_mens = (womens_sales / mens_sales - 1) * 100) |>\n  ggplot(aes(y = womens_pct_of_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\n\n3.2.3 Percent of Total Calculations\n\nretail_sales |>\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) |>\n  group_by(sales_month) |>\n  mutate(total_sales = sum(sales, na.rm = TRUE))  |>\n  ungroup() |>\n  mutate(pct_total_sales = sales * 100 / total_sales) |>\n  select(sales_month, kind_of_business, pct_total_sales) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\nsales_month\nkind_of_business\npct_total_sales\n\n\n\n\n1992-06-01\nMen’s clothing stores\n26.02991\n\n\n1992-06-01\nWomen’s clothing stores\n73.97009\n\n\n1992-08-01\nMen’s clothing stores\n22.62667\n\n\n\n\n\n\nretail_sales |>\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) |>\n  group_by(sales_month) |>\n  mutate(total_sales = sum(sales, na.rm = TRUE)) |>\n  ungroup() |>\n  mutate(pct_total_sales = sales * 100 / total_sales) |>\n  show_query()\n\n<SQL>\nSELECT *, (sales * 100.0) / total_sales AS pct_total_sales\nFROM (\n  SELECT *, SUM(sales) OVER (PARTITION BY sales_month) AS total_sales\n  FROM retail_sales\n  WHERE (kind_of_business IN ('Men''s clothing stores', 'Women''s clothing stores'))\n) q01\n\n\n\nretail_sales |>\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) |>\n  group_by(sales_month) |>\n  mutate(total_sales = sum(sales, na.rm = TRUE)) |>\n  ungroup() |>\n  mutate(pct_total_sales = sales * 100 / total_sales) |>\n  ggplot(aes(y = pct_total_sales, x = sales_month, color = kind_of_business)) +\n  geom_line()\n\n\n\n\n\n\n3.2.4 Indexing to See Percent Change over Time\n\nretail_sales |>\n  filter(kind_of_business == \"Women's clothing stores\") |>\n  mutate(sales_year = year(sales_month)) |>\n  group_by(sales_year) |>\n  summarize(sales = sum(sales, na.rm = TRUE)) |>\n  ungroup() |>\n  window_order(sales_year) |>\n  mutate(index_sales = first(sales),\n         pct_from_index = (sales/index_sales - 1) * 100) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\nsales_year\nsales\nindex_sales\npct_from_index\n\n\n\n\n1992\n31815\n31815\n0\n\n\n1993\n32350\n31815\n0\n\n\n1994\n30585\n31815\n-100\n\n\n\n\n\n\nretail_sales |>\n  filter(kind_of_business %in% c(\"Women's clothing stores\",\n                                 \"Men's clothing stores\"),\n         sales_month <= '2019-12-31') |>\n  mutate(sales_year = year(sales_month)) |>\n  group_by(kind_of_business, sales_year) |>\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") |>\n  group_by(kind_of_business) |>\n  window_order(sales_year) |>\n  mutate(index_sales = first(sales),\n         pct_from_index = (sales/index_sales - 1) * 100) |>\n  ungroup() |>\n  ggplot(aes(y = pct_from_index, x = sales_year, color = kind_of_business)) +\n  geom_line()"
  },
  {
    "objectID": "time-series.html#rolling-time-windows",
    "href": "time-series.html#rolling-time-windows",
    "title": "3  Time Series Analysis",
    "section": "3.3 Rolling Time Windows",
    "text": "3.3 Rolling Time Windows\n\n3.3.1 Calculating Rolling Time Windows\n\nmvg_avg <-\n  retail_sales |>\n  filter(kind_of_business == \"Women's clothing stores\") |>\n  window_order(sales_month) |>\n  window_frame(-11, 0) |>\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n()) |>\n  filter(sales_month >= '1993-01-01') \n\nmvg_avg |>\n  select(sales_month, moving_avg, records_count) |>\n  collect(n = 3) |>\n  kable(digits = 2)\n\n\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n1993-01-01\n2672.08\n12\n\n\n1993-02-01\n2673.25\n12\n\n\n1993-03-01\n2676.50\n12\n\n\n\n\n\n\nmvg_avg |>\n  ggplot(aes(x = sales_month)) +\n  geom_line(aes(y = sales, colour = \"Sales\")) +\n  geom_line(aes(y = moving_avg, colour = \"Moving average\")) \n\n\n\n\n\nSELECT \n  sales_month,\n  avg(sales) over w AS moving_avg,\n  count(sales) over w AS records_count\nFROM retail_sales\nWHERE kind_of_business = 'Women''s clothing stores'\nWINDOW w AS (order by sales_month \n             rows between 11 preceding and current row)\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n1992-01-01\n1873.000\n1\n\n\n1992-02-01\n1932.000\n2\n\n\n1992-03-01\n2089.000\n3\n\n\n1992-04-01\n2233.000\n4\n\n\n1992-05-01\n2336.800\n5\n\n\n1992-06-01\n2351.333\n6\n\n\n1992-07-01\n2354.429\n7\n\n\n1992-08-01\n2392.250\n8\n\n\n1992-09-01\n2410.889\n9\n\n\n1992-10-01\n2445.300\n10\n\n\n\n\n\n\nretail_sales |>\n  filter(kind_of_business == \"Women's clothing stores\") |>\n  window_order(sales_month) |>\n  window_frame(-11, 0) |>\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n()) |>\n  select(sales_month, moving_avg, records_count) |>\n  collect(n = 10) |>\n  kable()\n\n\ndate_dim <-\n  tibble(date = seq(as.Date('1993-01-01'), \n                    as.Date('2020-12-01'), \n                    by = \"1 month\")) |>\n  copy_to(db, df = _, overwrite = TRUE, name = \"date_dim\")\n\n\nWITH jan_jul AS (\n  SELECT sales_month, sales\n  FROM retail_sales \n  WHERE kind_of_business = 'Women''s clothing stores'\n     AND date_part('month', sales_month) IN (1, 7))\n     \nSELECT a.date, b.sales_month, b.sales\nFROM date_dim a\nINNER JOIN jan_jul b \nON b.sales_month BETWEEN a.date - interval '11 months' AND a.date\nWHERE a.date BETWEEN '1993-01-01' AND '2020-12-01';\n\n\nDisplaying records 1 - 10\n\n\ndate\nsales_month\nsales\n\n\n\n\n1993-01-01\n1992-07-01\n2373\n\n\n1993-02-01\n1992-07-01\n2373\n\n\n1993-03-01\n1992-07-01\n2373\n\n\n1993-04-01\n1992-07-01\n2373\n\n\n1993-05-01\n1992-07-01\n2373\n\n\n1993-06-01\n1992-07-01\n2373\n\n\n1993-01-01\n1993-01-01\n2123\n\n\n1993-02-01\n1993-01-01\n2123\n\n\n1993-03-01\n1993-01-01\n2123\n\n\n1993-04-01\n1993-01-01\n2123\n\n\n\n\n\n\njan_jul <-\n  retail_sales |>\n  filter(kind_of_business == \"Women's clothing stores\",\n         month(sales_month) %in% c(1, 7)) |>\n  select(sales_month, sales)\n\ndate_dim |>\n  mutate(date_start = date - months(11)) |>\n  inner_join(jan_jul, \n             join_by(between(y$sales_month, x$date_start, x$date))) |>\n  select(date, sales_month, sales) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\ndate\nsales_month\nsales\n\n\n\n\n1993-01-01\n1992-07-01\n2373\n\n\n1993-02-01\n1992-07-01\n2373\n\n\n1993-03-01\n1992-07-01\n2373\n\n\n\n\n\n\ndate_dim |>\n  mutate(date_start = date - months(11)) |>\n  inner_join(jan_jul, \n             join_by(between(y$sales_month, x$date_start, x$date))) |>\n  group_by(date) |>\n  summarize(moving_avg = mean(sales, na.rm = TRUE),\n            records = n()) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\ndate\nmoving_avg\nrecords\n\n\n\n\n1993-01-01\n2248\n2\n\n\n1993-02-01\n2248\n2\n\n\n1993-03-01\n2248\n2\n\n\n\n\n\n\nWITH sales_months AS (\n  SELECT distinct sales_month\n  FROM retail_sales\n  WHERE sales_month between '1993-01-01' and '2020-12-01')\n\nSELECT a.sales_month, avg(b.sales) as moving_avg\nFROM sales_months a\nJOIN retail_sales b \non b.sales_month between \n    a.sales_month - interval '11 months' and a.sales_month\n  and b.kind_of_business = 'Women''s clothing stores' \nGROUP BY 1\nORDER BY 1\nLIMIT 3;\n\n\n3 records\n\n\nsales_month\nmoving_avg\n\n\n\n\n1993-01-01\n2672.083\n\n\n1993-02-01\n2673.250\n\n\n1993-03-01\n2676.500\n\n\n\n\n\n\nsales_months <-\n  retail_sales |>\n  filter(between(sales_month, \n                 as.Date('1993-01-01'), \n                 as.Date('2020-12-01'))) |>\n  distinct(sales_month)\n\nsales_months |>\n  mutate(month_start = sales_month - months(11)) |>\n  inner_join(retail_sales, \n             join_by(between(y$sales_month, x$month_start, x$sales_month)),\n                     suffix = c(\"\", \"_y\")) |>\n  filter(kind_of_business == \"Women's clothing stores\") |>\n  group_by(sales_month) |>\n  summarize(moving_avg = mean(sales, na.rm = TRUE)) |>\n  arrange(sales_month) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\nsales_month\nmoving_avg\n\n\n\n\n1993-01-01\n2672.083\n\n\n1993-02-01\n2673.250\n\n\n1993-03-01\n2676.500\n\n\n\n\n\n\n\n3.3.2 Calculating Cumulative Values\n\nSELECT sales_month, sales,\n  sum(sales) OVER w AS sales_ytd\nFROM retail_sales\nWHERE kind_of_business = 'Women''s clothing stores'\nWINDOW w AS (PARTITION BY date_part('year', sales_month) \n             ORDER BY sales_month)\nLIMIT 3;\n\n\n3 records\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n2017-01-01\n2454\n2454\n\n\n2017-02-01\n2763\n5217\n\n\n2017-03-01\n3485\n8702\n\n\n\n\n\n\nytd_sales <-\n  retail_sales |>\n  filter(kind_of_business == \"Women's clothing stores\") |>\n  mutate(year = year(sales_month)) |>\n  group_by(year) |>\n  window_order(sales_month) |>\n  mutate(sales_ytd = cumsum(sales)) |>\n  ungroup() |>\n  select(sales_month, sales, sales_ytd) \n\nytd_sales |>\n  filter(month(sales_month) %in% c(1:3, 12)) |>\n  collect(n = 6) |>\n  kable()\n\n\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n2017-01-01\n2454\n2454\n\n\n2019-01-01\n2511\n2511\n\n\n2017-02-01\n2763\n5217\n\n\n2019-02-01\n2680\n5191\n\n\n2017-03-01\n3485\n8702\n\n\n2019-03-01\n3585\n8776\n\n\n\n\n\n\nfactor <- 40/4.5\n\nytd_sales |>\n  filter(year(sales_month) %in% 2016:2020) |>\n  ggplot(aes(x = sales_month, y = sales_ytd)) +\n  geom_bar(stat = \"identity\") +\n  geom_line(aes(y = sales * factor, colour = I(\"blue\"))) +\n  scale_y_continuous(\n    \"Sales YTD\", \n    sec.axis = sec_axis(~ . / factor, name = \"Monthly Sales\")\n  )\n\n\n\n\n\nSELECT a.sales_month, a.sales,\n  sum(b.sales) AS sales_ytd\nFROM retail_sales a\nINNER JOIN retail_sales b ON \n date_part('year',a.sales_month) = date_part('year',b.sales_month)\n AND b.sales_month <= a.sales_month\n AND b.kind_of_business = 'Women''s clothing stores'\nWHERE a.kind_of_business = 'Women''s clothing stores'\nGROUP BY 1,2;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n1992-12-01\n4416\n31815\n\n\n1993-12-01\n4170\n32350\n\n\n1992-11-01\n2946\n27399\n\n\n1993-11-01\n2923\n28180\n\n\n1992-10-01\n2755\n24453\n\n\n1993-10-01\n2713\n25257\n\n\n1992-09-01\n2560\n21698\n\n\n1993-09-01\n2622\n22544\n\n\n1992-08-01\n2657\n19138\n\n\n1993-08-01\n2626\n19922\n\n\n\n\n\n\nretail_sales_yr <-\n  retail_sales |>\n  mutate(year = year(sales_month))\n  \nretail_sales_yr |>\n  filter(kind_of_business == \"Women's clothing stores\") |>\n  inner_join(retail_sales_yr, \n             join_by(year, kind_of_business,\n                     sales_month >= sales_month),\n             suffix = c(\"\", \"_y\")) |>\n  group_by(sales_month, sales) |>\n  summarize(sales_ytd = sum(sales_y, na.rm = TRUE),\n            .groups = \"drop\") |>\n  filter(month(sales_month) %in% c(1:3, 12)) |>\n  collect(n = 6) |>\n  kable()\n\n\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n1992-02-01\n1991\n3864\n\n\n1993-02-01\n2005\n4128\n\n\n1994-02-01\n1970\n3756\n\n\n1992-03-01\n2403\n6267\n\n\n1993-03-01\n2442\n6570\n\n\n1994-03-01\n2560\n6316"
  },
  {
    "objectID": "time-series.html#analyzing-with-seasonality",
    "href": "time-series.html#analyzing-with-seasonality",
    "title": "3  Time Series Analysis",
    "section": "3.4 Analyzing with Seasonality",
    "text": "3.4 Analyzing with Seasonality\n\nretail_sales |>\n  filter(kind_of_business %in% c(\"Jewelry stores\",\n                                 \"Book stores\",\n                                 \"Grocery stores\")) |>\n  ggplot(aes(x = sales_month, y = sales)) +\n  geom_line() +\n  facet_wrap(vars(kind_of_business), nrow = 3, scales = \"free\")\n\n\n\n\n\n3.4.1 Period-over-Period Comparisons: YoY and MoM\n\nSELECT kind_of_business, sales_month, sales,\n  lag(sales_month) OVER w AS prev_month,\n  lag(sales) OVER w AS prev_month_sales\nFROM retail_sales\nWHERE kind_of_business = 'Book stores'\nWINDOW w AS (PARTITION BY kind_of_business ORDER BY sales_month)\n\n\nDisplaying records 1 - 10\n\n\nkind_of_business\nsales_month\nsales\nprev_month\nprev_month_sales\n\n\n\n\nBook stores\n1992-01-01\n790\nNA\nNA\n\n\nBook stores\n1992-02-01\n539\n1992-01-01\n790\n\n\nBook stores\n1992-03-01\n535\n1992-02-01\n539\n\n\nBook stores\n1992-04-01\n523\n1992-03-01\n535\n\n\nBook stores\n1992-05-01\n552\n1992-04-01\n523\n\n\nBook stores\n1992-06-01\n589\n1992-05-01\n552\n\n\nBook stores\n1992-07-01\n592\n1992-06-01\n589\n\n\nBook stores\n1992-08-01\n894\n1992-07-01\n592\n\n\nBook stores\n1992-09-01\n861\n1992-08-01\n894\n\n\nBook stores\n1992-10-01\n645\n1992-09-01\n861\n\n\n\n\n\n\nbooks_w_lag <-\n  retail_sales |>\n  filter(kind_of_business == 'Book stores') |>\n  group_by(kind_of_business) |>\n  window_order(sales_month) |>\n  mutate(prev_month = lag(sales_month),\n         prev_month_sales = lag(sales)) |>\n  select(kind_of_business,\n         sales_month, sales,\n         prev_month, prev_month_sales)\n\nbooks_w_lag |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\nkind_of_business\nsales_month\nsales\nprev_month\nprev_month_sales\n\n\n\n\nBook stores\n1992-01-01\n790\nNA\nNA\n\n\nBook stores\n1992-02-01\n539\n1992-01-01\n790\n\n\nBook stores\n1992-03-01\n535\n1992-02-01\n539\n\n\n\n\n\n\nbooks_monthly <-\n  books_w_lag |>\n  mutate(pct_growth = (sales / prev_month_sales - 1) * 100) |>\n  select(-prev_month, -prev_month_sales)\n\n\nbooks_yearly <- \n  retail_sales |>\n  filter(kind_of_business == 'Book stores') |>\n  mutate(sales_year = year(sales_month)) |>\n  group_by(sales_year) |>\n  summarize(yearly_sales = sum(sales, na.rm = TRUE),\n            .groups = \"drop\") |>\n  window_order(sales_year) |>\n  mutate(prev_year_sales = lag(yearly_sales),\n         pct_growth = (yearly_sales/prev_year_sales - 1) * 100)\n\nbooks_yearly |>\n  collect(n = 3) |>\n  kable(digits = 2)\n\n\n\n\nsales_year\nyearly_sales\nprev_year_sales\npct_growth\n\n\n\n\n1992\n8327\nNA\nNA\n\n\n1993\n9108\n8327\n0\n\n\n1994\n10107\n9108\n0\n\n\n\n\n\n\nbooks_monthly |>\n  filter(!is.na(pct_growth)) |>\n  ggplot(aes(x = sales_month, y = pct_growth)) +\n  geom_line()\n\n\n\n\n\n\n3.4.2 Period-over-Period Comparisons: Same Month Versus Last Year\n\nbooks_lagged_year_month <-\n  retail_sales |>\n  filter(kind_of_business == 'Book stores') |>\n  mutate(month = month(sales_month)) |>\n  group_by(month) |>\n  window_order(sales_month) |>\n  mutate(prev_year_month = lag(sales_month),\n         prev_year_sales = lag(sales)) |>\n  ungroup() |>\n  select(sales_month, sales, prev_year_month, prev_year_sales)\n\nbooks_lagged_year_month |>\n  filter(month(sales_month) <= 2, \n         year(sales_month) <= 1994) |>\n  arrange(sales_month) |>\n  collect(n = 6) |>\n  kable()\n\n\n\n\nsales_month\nsales\nprev_year_month\nprev_year_sales\n\n\n\n\n1992-01-01\n790\nNA\nNA\n\n\n1992-02-01\n539\nNA\nNA\n\n\n1993-01-01\n998\n1992-01-01\n790\n\n\n1993-02-01\n568\n1992-02-01\n539\n\n\n1994-01-01\n1053\n1993-01-01\n998\n\n\n1994-02-01\n635\n1993-02-01\n568\n\n\n\n\n\n\nbooks_lagged_year_month |>\n  mutate(dollar_diff = sales - prev_year_sales,\n         pct_diff = dollar_diff/prev_year_sales * 100) |>\n  select(-prev_year_month, -prev_year_sales) |>\n  filter(month(sales_month) == 1) |>\n  collect(n = 3) |>\n  kable(digits = 2)\n\n\n\n\nsales_month\nsales\ndollar_diff\npct_diff\n\n\n\n\n1992-01-01\n790\nNA\nNA\n\n\n1993-01-01\n998\n208\n0\n\n\n1994-01-01\n1053\n55\n0\n\n\n\n\n\n\nbooks_lagged_year_month |>\n  filter(!is.na(prev_year_sales)) |>\n  mutate(dollar_diff = sales - prev_year_sales,\n         pct_diff = dollar_diff/prev_year_sales * 100) |>\n  select(sales_month, sales, dollar_diff, pct_diff) |>\n  pivot_longer(-sales_month) |>\n  collect() |>\n  mutate(name = fct_inorder(name)) |>\n  ggplot(aes(x = sales_month, y = value)) +\n  geom_line() +\n  facet_wrap(. ~ name, nrow = 3, scales = \"free\")\n\n\n\n\nWith PostgreSQL, we would use to_char(sales_month,'Month'); with DuckDB, the equivalent is monthname(sales_month). To use an approach that works with either backend, we draw on arguments to the lubridate function month().\n\nsales_92_94 <-\n  retail_sales |>\n  filter(kind_of_business == 'Book stores',\n         year(sales_month) %in% 1992:1994) |>\n  select(sales_month, sales) |>\n  mutate(month_number = month(sales_month),\n         month_name = month(sales_month, \n                            label = TRUE, abbr = FALSE),\n         year = as.integer(year(sales_month)))\n\nsales_92_94 |>\n  pivot_wider(id_cols = c(month_number, month_name),\n              names_from = year,\n              names_prefix = \"sales_\",\n              values_from = sales) |>\n  kable()\n\n\n\n\nmonth_number\nmonth_name\nsales_1992\nsales_1993\nsales_1994\n\n\n\n\n1\nJanuary\n790\n998\n1053\n\n\n2\nFebruary\n539\n568\n635\n\n\n3\nMarch\n535\n602\n634\n\n\n4\nApril\n523\n583\n610\n\n\n5\nMay\n552\n612\n684\n\n\n6\nJune\n589\n618\n724\n\n\n7\nJuly\n592\n607\n678\n\n\n8\nAugust\n894\n983\n1154\n\n\n9\nSeptember\n861\n903\n1022\n\n\n10\nOctober\n645\n669\n732\n\n\n11\nNovember\n642\n692\n772\n\n\n12\nDecember\n1165\n1273\n1409\n\n\n\n\n\n\nsales_92_94 |>\n  collect() |>\n  mutate(year = factor(year),\n         month_name = month(month_number, label = TRUE)) |>\n  ggplot(aes(x = month_name, y = sales, \n             group = year, colour = year)) +\n  geom_line()\n\n\n\n\n\n\n3.4.3 Comparing to Multiple Prior Periods\n\nprev_three <-\n  retail_sales |>\n  filter(kind_of_business == 'Book stores') |>\n  mutate(month = month(sales_month)) |>\n  group_by(month) |>\n  window_order(sales_month) |>\n  mutate(prev_sales_1 = lag(sales, 1),\n         prev_sales_2 = lag(sales, 2),\n         prev_sales_3 = lag(sales, 3)) |>\n  ungroup()\n\nprev_three |>\n  filter(month == 1) |>\n  select(sales_month, sales, starts_with(\"prev_sales\")) |>\n  collect(n = 5) |>\n  kable()\n\n\n\n\nsales_month\nsales\nprev_sales_1\nprev_sales_2\nprev_sales_3\n\n\n\n\n1992-01-01\n790\nNA\nNA\nNA\n\n\n1993-01-01\n998\n790\nNA\nNA\n\n\n1994-01-01\n1053\n998\n790\nNA\n\n\n1995-01-01\n1308\n1053\n998\n790\n\n\n1996-01-01\n1373\n1308\n1053\n998\n\n\n\n\n\n\nprev_three |>\n  mutate(avg_prev_three = (prev_sales_1 + \n                         prev_sales_2 + \n                         prev_sales_3)/3,\n         pct_of_prev_3 = 100 * sales/avg_prev_three) |>\n  select(sales_month, sales, pct_of_prev_3) |>\n  filter(month(sales_month) == 1,\n         year(sales_month) %in% c(1995:1997, 2017:2019)) |>\n  collect(n = 10) |>\n  kable(digits = 2)\n\n\n\n\nsales_month\nsales\npct_of_prev_3\n\n\n\n\n1995-01-01\n1308\n138.12\n\n\n1996-01-01\n1373\n122.63\n\n\n1997-01-01\n1558\n125.17\n\n\n2017-01-01\n1386\n94.63\n\n\n2018-01-01\n1217\n84.95\n\n\n2019-01-01\n1004\n74.74\n\n\n\n\n\n\nprev_three_win <-\n  retail_sales |>\n  filter(kind_of_business == 'Book stores') |>\n  mutate(month = month(sales_month)) |>\n  group_by(month) |>\n  window_order(sales_month) |>\n  window_frame(-3, -1) |>\n  mutate(avg_prev_three = mean(sales, na.rm = TRUE)) |>\n  ungroup() |>\n  mutate(pct_of_prev_3 = 100 * sales/avg_prev_three)\n\n\nprev_three_win |>\n  select(sales_month, sales, pct_of_prev_3) |>\n  filter(month(sales_month) == 1,\n         year(sales_month) %in% c(1995:1997, 2017:2019)) |>\n  collect(n = 10) |>\n  kable(digits = 2)\n\n\n\n\nsales_month\nsales\npct_of_prev_3\n\n\n\n\n1995-01-01\n1308\n138.12\n\n\n1996-01-01\n1373\n122.63\n\n\n1997-01-01\n1558\n125.17\n\n\n2017-01-01\n1386\n94.63\n\n\n2018-01-01\n1217\n84.95\n\n\n2019-01-01\n1004\n74.74"
  },
  {
    "objectID": "time-series.html#persistent-storage",
    "href": "time-series.html#persistent-storage",
    "title": "3  Time Series Analysis",
    "section": "3.5 Persistent storage",
    "text": "3.5 Persistent storage\nHaving created a database connection, we can write the local data frame to the database using (say) copy_to().\nWe could specify temporary = FALSE if we wanted the data to be there permanently.3\n\n3.5.1 Using PostgreSQL\n\n\n3.5.2 Read-only databases\nIn some cases, you will have access to a database, but no write privileges for that database. In such a case, copy_inline() can be useful.4 Note that it seems you cannot interrogate a table created using copy_inline() using SQL, though it will behave in most respects just like a table created using copy_to() when using dbplyr. It is useful to note that copy_inline() is probably not a good solution if your data are hundreds of thousands of rows or more because the table is effectively turned into literal SQL.\n\nretail_sales_alt <- copy_inline(db, retail_sales_local)\n\n\n\n3.5.3 Closing the database connection\n\ndbDisconnect(db, shutdown=TRUE)\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "cohorts.html",
    "href": "cohorts.html",
    "title": "4  Cohorts",
    "section": "",
    "text": "Chapter 4 examines the fascinating topic of cohorts, where a cohort is a group of observations (often people) who acquire a shared characteristic at (approximately) the same time. For example, children entering kindergarten in New Zealand in 2017 or the Harvard MBA Class of 2002.\nWhile cohort analysis has some attractive features, I guess that data do not often come in a format that facilitates such analysis. Instead, as is the case with the legislators data set studied in Chapter 4, the data analyst needs to rearrange the data to support cohort analysis.\nI found Chapter 4 a little confusing on a first pass through it.1 The chapter launches into some SQL code intended to create cohorts, but it’s a little unclear why we’re doing what we’re doing, and we quickly see that our cohort analysis does not make sense (e.g., we have more of our original cohort in period 5 than we had in period 4) and we must have done something wrong. I think I see the idea Cathy is going for here: one needs to think carefully about how to arrange the data to avoid subtle mistakes. The challenge I see is that it’s not obvious that everyone would make the same mistake and one is too deep in the weeds of the code to really see the forest for the trees.\nSo before I launch into the code, I will spend a little time thinking about things conceptually. I will start with a different example from that used in Chapter 4, but one that I think brings out some of the issues.\nFor some reason I associate cohort analysis with life expectancy. The people who are born today form a cohort and one might ask: How long do we expect members of this cohort to live? One often hears life expectancy statistics quoted as something like: “In Australia, a boy born in 2018–2020 can expect to live to the age of 81.2 years and a girl would be expected to live to 85.3 years compared to 51.1 for boys and 54.8 years for girls born in in 1891–1900.”2\nThe people who construct the life expectancies for the children must be veritable polymaths. They need to anticipate future developments in medical care and technology. Skin cancer is a significant cause of death in Australia, due to a mismatch between the median complexion and the intensity of the sun. But the analysts calculating life expectancy need to think about how medical technology is likely to affect rates of death from carcinoma in the future. I can imagine whole-body scanners a bit like the scanners in US airports that detect skin cancers before they become problematic. These analysts also need to understand how road safety will evolve. Will children today all be in driverless vehicles in fifty years time and will accidents then be a rarity? And what about war? The data analyst needs to be able to forecast the possibility of World War III breaking out and shortening life spans. Who are these people?\nOf course it seems unlikely these über-analysts exist. Rather they surely do something more prosaic. Here is my guess as to how life expectancies are constructed.3 I guess that the data analyst gathers data on cohorts notionally formed at some point in the past and then looks at survival rates for that cohort over some period, then aggregates those data into a life expectancy.\nFor example, the data analyst might gather data on people who turned 21 in 2018 and then data on whether those people surived to their 22nd birthday. The proportion of such people who make their 22nd birthday could be interpreted as a survival probability \\(p_{21}\\). Repeat that for each age-based cohort to get probabilities \\(\\left\\{p_i: i = 0, 1, \\dots, 119, 120 \\right\\}\\). Now to find the median life expectancy, we could calculate something like this:4\n\\[ \\left\\{j: \\arg \\min_{i} \\left(\\prod_{0}^{i} p_i\\right) \\leq \\frac{1}{2} \\right\\} \\] So we have a (fairly) well-defined procedure here. There are obviously some details to be worked out. For example, do we focus on one year (2018 in this case)? Or collect data over multiple years? Does it make sense to form cohorts by years? Or would grouping into larger cohorts (e.g., 20–25) make more sense? Do we identify people by birthdays? Or just use some kind of census date? (People who are 21 on 1 July might have just turned 21, or might be about to turn 22.)\nBut what exactly have we calculated? In a sense it’s a nonsensical number. Why would survival rates for 88-year-olds in 2018 be relevant for the life expectancy of newborns today, who will face a very different world when they turn 88 in 2111. First, perhaps the analysts really don’t calculate it in this way (though I’m doubtful they are polymaths). Second, even though it’s a “meaningless” number, it probably still has attractive properties, such as the ability to represent in a one or two numbers a lot about the quality of life in Australia.\nA final note is that it is not clear to me where the “51.1 for boys and 54.8 years for girls born in in 1891–1900” values come from. Are these the equivalent life expectancies calculated using data available around 1900? Or are these the observed lifespans of people born in 1891–1900? If the latter, how accurate were the former as estimates of these values?\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(ggplot2)\nlibrary(duckdb)"
  },
  {
    "objectID": "cohorts.html#the-legislators-data-set",
    "href": "cohorts.html#the-legislators-data-set",
    "title": "4  Cohorts",
    "section": "4.2 The Legislators Data Set",
    "text": "4.2 The Legislators Data Set\n\n4.2.1 Reading the data\nLet’s move onto the legislators data set.\n\ndb <- dbConnect(duckdb::duckdb())\n\nlegislators <-\n  tbl(db, \"read_csv_auto('data/legislators.csv')\") |>\n  compute(name = \"legislators\")\n\nlegislators_terms <-\n    tbl(db, \"read_csv_auto('data/legislators_terms.csv')\") |>\n  compute(name = \"legislators_terms\")\n\n\n\n4.2.2 Exploring the data\nThere are two tables in the legislators data set. The legislators data looks to have id_bioguide as a primary key.\n\nWITH\n\nid_rows AS (\n  SELECT id_bioguide, count(*) AS rows_per_id\n  FROM legislators\n  GROUP BY 1)\n\nSELECT rows_per_id, count(*) AS num_ids\nFROM id_rows\nGROUP BY 1;\n\n\n1 records\n\n\nrows_per_id\nnum_ids\n\n\n\n\n1\n12518\n\n\n\n\n\n\nSELECT COUNT(*) AS num_missing_ids\nFROM legislators\nWHERE id_bioguide IS NULL\n\n\n1 records\n\n\nnum_missing_ids\n\n\n\n\n0\n\n\n\n\n\n\nWITH id_rows AS (\n  SELECT id_bioguide, term_start,\n    count(*) AS rows_per_id\n  FROM legislators_terms\n  GROUP BY 1, 2)\n\nSELECT rows_per_id, count(*) AS num_ids\nFROM id_rows\nGROUP BY 1;\n\n\n1 records\n\n\nrows_per_id\nnum_ids\n\n\n\n\n1\n44063\n\n\n\n\n\n\nSELECT COUNT(*) AS num_missing_ids\nFROM legislators_terms\nWHERE id_bioguide IS NULL OR term_start IS NULL\n\n\n1 records\n\n\nnum_missing_ids\n\n\n\n\n0\n\n\n\n\n\n\nWITH id_rows AS (\n  SELECT id_bioguide, term_number,\n    count(*) AS rows_per_id\n  FROM legislators_terms\n  GROUP BY 1, 2)\n\nSELECT rows_per_id, count(*) AS num_ids\nFROM id_rows\nGROUP BY 1;\n\n\n1 records\n\n\nrows_per_id\nnum_ids\n\n\n\n\n1\n44063\n\n\n\n\n\n\nSELECT COUNT(*) AS num_missing_ids\nFROM legislators_terms\nWHERE term_end IS NULL\n\n\n1 records\n\n\nnum_missing_ids\n\n\n\n\n0\n\n\n\n\n\n\nSELECT max(term_start) AS max_term_start,\n  max(term_end) AS max_term_end\nFROM legislators_terms\n\n\n1 records\n\n\nmax_term_start\nmax_term_end\n\n\n\n\n2020-05-19\n2025-01-03\n\n\n\n\n\n\nlegislators_terms |>\n  filter(term_end > '2020-05-19') |>\n  count(term_end, term_type)\n\n\n\n\nterm_end\nterm_type\nn\n\n\n\n\n2021-01-03\nrep\n437\n\n\n2021-01-03\nsen\n34\n\n\n2023-01-03\nsen\n32\n\n\n\n\n\nClearly we have term_end dates that are anticipated based on"
  },
  {
    "objectID": "cohorts.html#retention",
    "href": "cohorts.html#retention",
    "title": "4  Cohorts",
    "section": "4.3 Retention",
    "text": "4.3 Retention\n\n4.3.1 SQL for a Basic Retention Curve\n\nSELECT id_bioguide, min(term_start) AS first_term\nFROM legislators_terms \nGROUP BY 1\nLIMIT 3;\n\n\n3 records\n\n\nid_bioguide\nfirst_term\n\n\n\n\nF000062\n1992-11-10\n\n\nT000464\n2007-01-04\n\n\nA000360\n2003-01-07\n\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1) \n\nSELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n  COUNT(DISTINCT a.id_bioguide) AS cohort_retained\nFROM first_terms a\nINNER JOIN legislators_terms b\nUSING (id_bioguide)\nGROUP BY 1\nORDER BY 1\nLIMIT 4;\n\n\n4 records\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n3600\n\n\n2\n3619\n\n\n3\n1831\n\n\n\n\n\n\nfirst_terms <- \n  legislators_terms |>\n  group_by(id_bioguide) |>\n  summarize(first_term = min(term_start, na.rm = TRUE),\n            .groups = \"drop\")\n\ncohorts <-\n  legislators_terms |>\n  inner_join(first_terms, by = \"id_bioguide\") |>\n  mutate(period = year(age(term_start, first_term))) |>\n  group_by(period) |>\n  summarize(cohort_retained = sql(\"count(distinct id_bioguide)\")) \n\ncohorts |>\n  arrange(period)\n\n\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n3600\n\n\n2\n3619\n\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts AS (\n  SELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n    COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b \n  USING (id_bioguide)\n  GROUP BY 1)\n  \nSELECT period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / \n    first_value(cohort_retained) OVER w AS prop_retained\nFROM cohorts\nWINDOW w AS (ORDER BY period)\nLIMIT 3;\n\n\n3 records\n\n\nperiod\ncohort_size\ncohort_retained\nprop_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n3600\n0.2875859\n\n\n2\n12518\n3619\n0.2891037\n\n\n\n\n\n\nretained_data <-\n  cohorts |>\n  window_order(period) |>\n  mutate(cohort_size = first(cohort_retained)) |>\n  mutate(pct_retained = cohort_retained * 1.0/cohort_size) |>\n  select(period, cohort_size, cohort_retained, pct_retained) \n\nretained_data\n\n\n\n\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n3600\n0.2875859\n\n\n2\n12518\n3619\n0.2891037\n\n\n\n\n\n\nretained_data |>\n  ggplot(aes(x = period, y = pct_retained)) +\n  geom_line()\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts AS (\n  SELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n      COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  GROUP BY 1),\n  \nretained_data AS (\n  SELECT period,\n    first_value(cohort_retained) OVER w AS cohort_size,\n    cohort_retained,\n    cohort_retained * 1.0 / first_value(cohort_retained) OVER w AS pct_retained\n  FROM cohorts\n  WINDOW w AS (ORDER BY period))\n\nSELECT cohort_size,\n  max(CASE WHEN period = 0 THEN pct_retained END) AS yr0,\n  max(CASE WHEN period = 1 THEN pct_retained END) AS yr1,\n  max(CASE WHEN period = 2 THEN pct_retained END) AS yr2,\n  max(CASE WHEN period = 3 THEN pct_retained END) AS yr3,\n  max(CASE WHEN period = 4 THEN pct_retained END) AS yr4\nFROM retained_data\nGROUP BY 1;\n\n\n1 records\n\n\ncohort_size\nyr0\nyr1\nyr2\nyr3\nyr4\n\n\n\n\n12518\n1\n0.2875859\n0.2891037\n0.1462694\n0.2564307\n\n\n\n\n\n\nretained_data |>\n  select(period, pct_retained) |>\n  filter(period <= 4) |>\n  collect() |>\n  arrange(period) |>\n  pivot_wider(names_from = period, \n              names_prefix = \"yr\",\n              values_from = pct_retained)\n\n\n\n\nyr0\nyr1\nyr2\nyr3\nyr4\n\n\n\n\n1\n0.2875859\n0.2891037\n0.1462694\n0.2564307\n\n\n\n\n\n\n\n4.3.2 Adjusting Time Series to Increase Retention Accuracy\nUse copy_inline() here if using a read-only database.\n\nyear_ends <-\n  tibble(date = seq(as.Date('1770-12-31'), \n                    as.Date('2030-12-31'), \n                    by = \"1 year\")) |>\n  copy_to(db, df = _, overwrite = TRUE, name = \"year_ends\")\n\n\nWITH\n\nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT a.id_bioguide, a.first_term, b.term_start, b.term_end,\n  c.date,\n  date_part('year', age(c.date, a.first_term)) AS period\nFROM first_terms a\nJOIN legislators_terms b USING (id_bioguide)\nLEFT JOIN year_ends c\nON c.date BETWEEN b.term_start and b.term_end\nORDER BY id_bioguide\nLIMIT 3;\n\n\n3 records\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\ndate\nperiod\n\n\n\n\nA000001\n1951-01-03\n1951-01-03\n1953-01-03\n1951-12-31\n0\n\n\nA000001\n1951-01-03\n1951-01-03\n1953-01-03\n1952-12-31\n1\n\n\nA000002\n1947-01-03\n1971-01-21\n1973-01-03\n1972-12-31\n25\n\n\n\n\n\n\ncohorts <-\n  first_terms |>\n  inner_join(legislators_terms, by = join_by(id_bioguide)) |>\n  left_join(year_ends, \n            by = join_by(between(y$date, x$term_start, x$term_end))) |>\n  mutate(period = date_part('year', age(date, first_term))) |>\n  select(id_bioguide, first_term, term_start, term_end, date, period) \n\ncohorts\n\n\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\ndate\nperiod\n\n\n\n\nT000165\n2020-05-19\n2020-05-19\n2021-01-03\n2020-12-31\n0\n\n\nG000061\n2020-05-19\n2020-05-19\n2021-01-03\n2020-12-31\n0\n\n\nM000687\n1987-01-06\n2020-05-05\n2021-01-03\n2020-12-31\n33\n\n\n\n\n\n\nWITH\n\nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts_retained AS (        \n    SELECT coalesce(date_part('year', age(c.date, a.first_term)), 0) AS period,\n      COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n    FROM first_terms a\n    JOIN legislators_terms b\n    USING (id_bioguide)\n    LEFT JOIN year_ends c ON c.date BETWEEN b.term_start AND b.term_end\n    GROUP BY 1)\n    \nSELECT period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0/first_value(cohort_retained) OVER w AS pct_retained\nFROM cohorts_retained\nWINDOW w AS (ORDER BY period);\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n12328\n0.9848219\n\n\n2\n12518\n8166\n0.6523406\n\n\n3\n12518\n8069\n0.6445918\n\n\n4\n12518\n5862\n0.4682857\n\n\n5\n12518\n5795\n0.4629334\n\n\n6\n12518\n4361\n0.3483783\n\n\n7\n12518\n4339\n0.3466209\n\n\n8\n12518\n3521\n0.2812750\n\n\n9\n12518\n3485\n0.2783991\n\n\n\n\n\n\ncohorts_retained <-\n  cohorts |>\n  mutate(period = coalesce(date_part('year', age(date, first_term)), 0)) |>\n  select(period, id_bioguide) |>\n  distinct() |>\n  group_by(period) |>\n  summarize(cohort_retained = n()) \n\npct_retained <-\n  cohorts_retained |>\n  window_order(period) |>\n  mutate(cohort_size = first(cohort_retained),\n         cohort_retained = as.double(cohort_retained),\n         pct_retained = cohort_retained/cohort_size) \n\npct_retained |>\n  arrange(period)\n\n\n\n\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12328\n12518\n0.9848219\n\n\n2\n8166\n12518\n0.6523406\n\n\n\n\n\n\npct_retained |>\n  ggplot(aes(x = period, y = pct_retained)) + \n  geom_line()\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT id_bioguide, a.first_term, b.term_start,\n  CASE WHEN b.term_type = 'rep' THEN b.term_start + interval '2 years'\n       WHEN b.term_type = 'sen' THEN b.term_start + interval '6 years'\n  END AS term_end\nFROM first_terms a\nJOIN legislators_terms b USING (id_bioguide)\nLIMIT 3;\n\n\n3 records\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\n\n\n\n\nF000062\n1992-11-10\n2019-01-03\n2025-01-03\n\n\nT000464\n2007-01-04\n2019-01-03\n2025-01-03\n\n\nA000360\n2003-01-07\n2015-01-06\n2021-01-06\n\n\n\n\n\n\nfirst_terms |>\n  inner_join(legislators_terms, by = join_by(id_bioguide)) |>\n  mutate(term_end = \n           case_when(term_type == 'rep' ~ term_start + years(2),\n                     term_type == 'sen' ~ term_start + years(6))) |>\n  select(id_bioguide, first_term, term_start, term_end)\n\n\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\n\n\n\n\nF000062\n1992-11-10\n2019-01-03\n2025-01-03\n\n\nT000464\n2007-01-04\n2019-01-03\n2025-01-03\n\n\nA000360\n2003-01-07\n2015-01-06\n2021-01-06\n\n\n\n\n\nFor now, I have omitted the query after the paragraph starting “A second option …”.\n\n\n4.3.3 Cohorts Derived from the Time Series Itself\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT date_part('year', a.first_term) AS first_year,\n  COALESCE(date_part('year', age(c.date, a.first_term)), 0) AS period,\n  COUNT(DISTINCT a.id_bioguide) AS cohort_retained\nFROM first_terms a\nINNER JOIN legislators_terms b \nUSING (id_bioguide)\nLEFT JOIN year_ends c \nON c.date BETWEEN b.term_start AND b.term_end \nGROUP BY 1, 2\nORDER BY 1, 2\nLIMIT 3;\n\n\n3 records\n\n\nfirst_year\nperiod\ncohort_retained\n\n\n\n\n1789\n0\n89\n\n\n1789\n1\n89\n\n\n1789\n2\n57\n\n\n\n\n\n\nyr_cohort_retaineds <-\n  first_terms |>\n  inner_join(legislators_terms, by = \"id_bioguide\") |>\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) |>\n  mutate(first_year = year(first_term),\n         period = coalesce(year(age(date, first_term)), 0)) |>\n  group_by(first_year, period) |>\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\") \n\nyr_cohort_retaineds |>\n  arrange(first_year, period)\n\n\n\n\nfirst_year\nperiod\ncohort_retained\n\n\n\n\n1789\n0\n89\n\n\n1789\n1\n89\n\n\n1789\n2\n57\n\n\n\n\n\n\nyr_cohort_retaineds |>\n  group_by(first_year) |>\n  window_order(period) |>\n  mutate(cohort_size = first(cohort_retained),\n         pct_retained = 1.0 * cohort_retained/cohort_size) |>\n  select(first_year, period, cohort_size, cohort_retained, pct_retained) |>\n  arrange(first_year, period)\n\n\n\n\nfirst_year\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n1789\n0\n89\n89\n1.0000000\n\n\n1789\n1\n89\n89\n1.0000000\n\n\n1789\n2\n89\n57\n0.6404494\n\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n                \nfirst_centuries AS (\n  SELECT \n    date_part('century', a.first_term) AS first_century,\n    coalesce(date_part('year', age(c.date, a.first_term)), 0) AS period,\n    COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n  FROM first_terms AS a\n  INNER JOIN legislators_terms b \n  USING (id_bioguide)\n  LEFT JOIN year_ends c \n  ON c.date BETWEEN b.term_start AND b.term_end \n  GROUP BY 1, 2)\n\nSELECT first_century, period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / first_value(cohort_retained) OVER w AS prop_retained\nFROM first_centuries\nWINDOW w AS (PARTITION BY first_century ORDER BY period)\nORDER BY 1, 2\nLIMIT 3;\n\n\n3 records\n\n\nfirst_century\nperiod\ncohort_size\ncohort_retained\nprop_retained\n\n\n\n\n18\n0\n368\n368\n1.0000000\n\n\n18\n1\n368\n360\n0.9782609\n\n\n18\n2\n368\n242\n0.6576087\n\n\n\n\n\n\ncen_cohort_retaineds <-\n  first_terms |>\n  inner_join(legislators_terms, by = \"id_bioguide\") |>\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) |>\n  mutate(first_century = century(first_term),\n         period = coalesce(year(age(date, first_term)), 0)) |>\n  group_by(first_century, period) |>\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\")\n\n\ncen_pct_retaineds <-\n  cen_cohort_retaineds |>\n  group_by(first_century) |>\n  window_order(period) |>\n  mutate(cohort_size = first(cohort_retained),\n         pct_retained = 1.0 * cohort_retained/cohort_size) |>\n  ungroup() |>\n  select(first_century, period, cohort_size, cohort_retained, pct_retained) \n\ncen_pct_retaineds |>\n  arrange(first_century, period)\n\n\n\n\nfirst_century\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n18\n0\n368\n368\n1.0000000\n\n\n18\n1\n368\n360\n0.9782609\n\n\n18\n2\n368\n242\n0.6576087\n\n\n\n\n\n\ncen_pct_retaineds |>\n  collect() |>\n  mutate(first_century = factor(first_century)) |>\n  ggplot(aes(x = period, y = pct_retained,\n             color = first_century,\n             linetype = first_century,\n             group = first_century)) + \n  geom_line()\n\n\n\n\n\nSELECT DISTINCT id_bioguide,\n  min(term_start) OVER w AS first_term,\n  first_value(state) OVER w AS first_state\nFROM legislators_terms \nWINDOW w AS (PARTITION BY id_bioguide ORDER BY term_start)\nORDER BY id_bioguide\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\nfirst_state\n\n\n\n\nA000001\n1951-01-03\nND\n\n\nA000002\n1947-01-03\nVA\n\n\nA000003\n1817-12-01\nGA\n\n\nA000004\n1843-12-04\nMA\n\n\nA000005\n1887-12-05\nTX\n\n\nA000006\n1868-01-01\nNC\n\n\nA000007\n1875-12-06\nMA\n\n\nA000008\n1857-12-07\nME\n\n\nA000009\n1973-01-03\nSD\n\n\nA000010\n1954-01-01\nNE\n\n\n\n\n\n\nfirst_states <-\n  first_terms <- \n  legislators_terms |>\n  group_by(id_bioguide) |>\n  window_order(term_start) |>\n  mutate(first_term = min(term_start, na.rm = TRUE),\n         first_state = first(state)) |>\n  ungroup() |>\n  select(id_bioguide, first_term, first_state) |>\n  distinct()\n\n\nWITH first_states AS (\n  SELECT DISTINCT id_bioguide,\n    min(term_start) OVER w AS first_term,\n    first_value(state) OVER w AS first_state\n  FROM legislators_terms \n  WINDOW w AS (PARTITION BY id_bioguide ORDER BY term_start)),\n\nstate_first_periods AS (\n  SELECT \n    first_state,\n    coalesce(date_part('year', age(c.date, a.first_term)), 0) AS period,\n    COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n  FROM first_states AS a\n  INNER JOIN legislators_terms b \n  USING (id_bioguide)\n  LEFT JOIN year_ends c \n  ON c.date BETWEEN b.term_start AND b.term_end \n  GROUP BY 1, 2)\n\nSELECT first_state, period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / first_value(cohort_retained) OVER w AS prop_retained\nFROM state_first_periods\nWINDOW w AS (PARTITION BY first_state ORDER BY period)\nORDER BY 1, 2\nLIMIT 3;\n\n\n3 records\n\n\nfirst_state\nperiod\ncohort_size\ncohort_retained\nprop_retained\n\n\n\n\nAK\n0\n19\n19\n1.0000000\n\n\nAK\n1\n19\n19\n1.0000000\n\n\nAK\n2\n19\n15\n0.7894737\n\n\n\n\n\n\nstate_first_periods <-\n  first_states |>\n  inner_join(legislators_terms, by = \"id_bioguide\") |>\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) |>\n  mutate(period = coalesce(year(age(date, first_term)), 0)) |>\n  group_by(first_state, period) |>\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\")\n\n\nstate_pct_retaineds <- \n  state_first_periods |>\n  select(first_state, period, cohort_retained) |>\n  group_by(first_state) |>\n  window_order(period) |>\n  mutate(cohort_size = first(cohort_retained),\n         prop_retained = cohort_retained * 1.0 / cohort_size) |>\n  ungroup()\n\nstate_pct_retaineds |>\n  arrange(first_state, period) \n\n\n\n\nfirst_state\nperiod\ncohort_retained\ncohort_size\nprop_retained\n\n\n\n\nAK\n0\n19\n19\n1.0000000\n\n\nAK\n1\n19\n19\n1.0000000\n\n\nAK\n2\n15\n19\n0.7894737\n\n\n\n\n\n\ntop_5_states <- c(\"NY\", \"PA\", \"OH\", \"IL\", \"MA\")\n\nstate_pct_retaineds |>\n  filter(first_state %in% top_5_states) |>\n  ggplot(aes(x = period, \n             y = prop_retained,\n             color = first_state,\n             linetype = first_state,\n             group = first_state)) + \n  geom_line()\n\n\n\n\n\n\n4.3.4 Defining the Cohort from a Separate Table\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT d.gender,\n  coalesce(date_part('year',age(c.date,a.first_term)),0) AS period,\n  count(distinct a.id_bioguide) AS cohort_retained\nFROM first_terms a\nJOIN legislators_terms b ON a.id_bioguide = b.id_bioguide \nLEFT JOIN year_ends c ON c.date BETWEEN b.term_start AND b.term_end \nINNER JOIN legislators d ON a.id_bioguide = d.id_bioguide\nGROUP BY 1,2\nORDER BY 2,1\nLIMIT 4;\n\n\n4 records\n\n\ngender\nperiod\ncohort_retained\n\n\n\n\nF\n0\n366\n\n\nM\n0\n12152\n\n\nF\n1\n349\n\n\nM\n1\n11979\n\n\n\n\n\n\ncohorts <-\n  first_terms |>\n  inner_join(legislators_terms, by = \"id_bioguide\") |>\n  left_join(year_ends, join_by(between(y$date, x$term_start, x$term_end))) |>\n  inner_join(legislators, by = \"id_bioguide\") |>\n  mutate(period = coalesce(year(age(date, first_term)), 0)) |>\n  group_by(gender, period) |>\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\")\n\ncohorts |>\n  arrange(period, gender) |>\n  collect(n = 4) \n\n\n\n\ngender\nperiod\ncohort_retained\n\n\n\n\nF\n0\n366\n\n\nM\n0\n12152\n\n\nF\n1\n349\n\n\nM\n1\n11979\n\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n  \ncohorts AS (\n  SELECT d.gender,\n    coalesce(date_part('year',age(c.date,a.first_term)),0) as period,\n    count(distinct a.id_bioguide) as cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  LEFT JOIN year_ends c on c.date between b.term_start and b.term_end \n  JOIN legislators d on a.id_bioguide = d.id_bioguide\n  GROUP BY 1, 2)\n  \nSELECT gender, period,\n  cohort_retained,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained * 1.0 / \n  first_value(cohort_retained) OVER w AS pct_retained\nFROM cohorts aa\nWINDOW w AS (partition by gender order by period)\nORDER BY 2, 1\nLIMIT 4;\n\n\n4 records\n\n\ngender\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\nF\n0\n366\n366\n1.0000000\n\n\nM\n0\n12152\n12152\n1.0000000\n\n\nF\n1\n349\n366\n0.9535519\n\n\nM\n1\n11979\n12152\n0.9857637\n\n\n\n\n\n\ncohorts |>\n  group_by(gender) |>\n  window_order(period) |>\n  mutate(cohort_size = first(cohort_retained)) |>\n  ungroup() |>\n  mutate(pct_retained = 1.0 * cohort_retained / cohort_size) |>\n    arrange(period, gender) |>\n  collect(n = 4)\n\n\n\n\ngender\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\nF\n0\n366\n366\n1.0000000\n\n\nM\n0\n12152\n12152\n1.0000000\n\n\nF\n1\n349\n366\n0.9535519\n\n\nM\n1\n11979\n12152\n0.9857637\n\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n  \ncohorts AS (\n  SELECT d.gender,\n    coalesce(date_part('year',age(c.date, a.first_term)),0) as period,\n    count(distinct a.id_bioguide) as cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  LEFT JOIN year_ends c on c.date between b.term_start and b.term_end \n  JOIN legislators d on a.id_bioguide = d.id_bioguide\n  WHERE first_term BETWEEN '1917-01-01' AND '1999-12-31'\n  GROUP BY 1, 2)\n  \nSELECT gender, period,\n  cohort_retained,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained * 1.0 / \n  first_value(cohort_retained) OVER w AS pct_retained\nFROM cohorts aa\nWINDOW w AS (PARTITION BY gender ORDER BY period)\nORDER BY 2, 1\nLIMIT 4;\n\n\n4 records\n\n\ngender\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\nF\n0\n200\n200\n1.0000000\n\n\nM\n0\n3833\n3833\n1.0000000\n\n\nF\n1\n187\n200\n0.9350000\n\n\nM\n1\n3769\n3833\n0.9833029\n\n\n\n\n\n\ncohorts <-\n  first_terms |>\n  filter(between(first_term, '1917-01-01', '1999-12-31')) |>\n  inner_join(legislators_terms, by = \"id_bioguide\") |>\n  left_join(year_ends, join_by(between(y$date, x$term_start, x$term_end))) |>\n  inner_join(legislators, by = \"id_bioguide\") |>\n  mutate(period = coalesce(year(age(date, first_term)), 0)) |>\n  group_by(gender, period) |>\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\") \n\ncohorts |>\n  group_by(gender) |>\n  window_order(period) |>\n  mutate(cohort_size = first(cohort_retained)) |>\n  ungroup() |>\n  mutate(pct_retained = 1.0 * cohort_retained / cohort_size) |>\n  arrange(period, gender) |>\n  collect(n = 4)\n\n\n\n\ngender\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\nF\n0\n200\n200\n1.0000000\n\n\nM\n0\n3833\n3833\n1.0000000\n\n\nF\n1\n187\n200\n0.9350000\n\n\nM\n1\n3769\n3833\n0.9833029\n\n\n\n\n\n\n\n4.3.5 Dealing with Sparse Cohorts\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS term_start,\n  FROM legislators_terms\n  GROUP BY 1),\n\nfirst_states AS (\n  SELECT id_bioguide, gender,\n    term_start AS first_term, \n    state AS first_state\n  FROM first_terms\n  INNER JOIN legislators_terms\n  USING (id_bioguide, term_start)\n  INNER JOIN legislators\n  USING (id_bioguide)\n  WHERE term_start between '1917-01-01' and '1999-12-31'),\n  \ncohorts AS (\n  SELECT first_state, gender,\n    coalesce(date_part('year', age(date, first_term)), 0) AS period,\n    COUNT(DISTINCT id_bioguide) AS cohort_retained\n  FROM first_states\n  INNER JOIN legislators_terms\n  USING (id_bioguide)\n  LEFT JOIN year_ends\n  ON date BETWEEN term_start AND term_end\n  GROUP BY 1, 2, 3),\n\nperiods AS (\n  SELECT generate_series as period \n   FROM generate_series(0, 20, 1)),\n\ncohort_sizes AS (\n  SELECT gender, first_state,\n    COUNT(DISTINCT id_bioguide) AS cohort_size\n  FROM first_states\n  GROUP BY 1, 2),\n  \npct_retaineds AS (\n  SELECT gender, first_state, period, \n    cohort_size,\n    coalesce(cohort_retained, 0) AS cohort_retained,\n    coalesce(cohort_retained, 0) * 1.0 / cohort_size AS pct_retained\n  FROM cohort_sizes\n  CROSS JOIN periods\n  LEFT JOIN cohorts\n  USING (gender, first_state, period))\n  \nSELECT gender, first_state, cohort_size,\n  max(case when period = 0 then pct_retained end) as yr0,\n  max(case when period = 2 then pct_retained end) as yr2,\n  max(case when period = 4 then pct_retained end) as yr4,\n  max(case when period = 6 then pct_retained end) as yr6,\n  max(case when period = 8 then pct_retained end) as yr8,\n  max(case when period = 10 then pct_retained end) as yr10\nFROM pct_retaineds\nWHERE first_state IN ('AL', 'AR', 'CA')\nGROUP BY 1, 2, 3\nORDER BY 1, 2\nLIMIT 3;\n\n\n3 records\n\n\ngender\nfirst_state\ncohort_size\nyr0\nyr2\nyr4\nyr6\nyr8\nyr10\n\n\n\n\nF\nAL\n3\n1\n0.00\n0.0\n0.00\n0.00\n0.00\n\n\nF\nAR\n5\n1\n0.80\n0.2\n0.40\n0.40\n0.40\n\n\nF\nCA\n25\n1\n0.92\n0.8\n0.64\n0.68\n0.68\n\n\n\n\n\n\nfirst_terms <- \n  legislators_terms |>\n  group_by(id_bioguide) |>\n  summarize(term_start = min(term_start, na.rm = TRUE),\n            .groups = \"drop\")\n\nfirst_states <-\n  first_terms |>\n  inner_join(legislators_terms, join_by(id_bioguide, term_start)) |>\n  inner_join(legislators, by = \"id_bioguide\") |>\n  rename(first_term = term_start,\n         first_state = state) |>\n  filter(between(first_term, '1917-01-01', '1999-12-31')) |>\n  select(id_bioguide, gender, first_term, first_state)\n\n\ncohorts <-\n  first_states |> \n  inner_join(legislators_terms, by = \"id_bioguide\") |>\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) |>\n  mutate(period = coalesce(year(age(date, first_term)), 0)) |>\n  group_by(first_state, gender, period) |>\n  summarize(cohort_retained = n_distinct(id_bioguide), \n            .groups = \"drop\")\n\nFrom the query below, it seems we’re having an issue whereby legislators are “returning from the dead” in a sense.\n\ncohorts |> \n  filter(first_state == \"AR\", gender == \"F\") |> \n  select(period, cohort_retained) |>\n  arrange(period) |>\n  collect(n = 8)\n\n\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n5\n\n\n1\n5\n\n\n2\n4\n\n\n3\n4\n\n\n4\n1\n\n\n5\n1\n\n\n6\n2\n\n\n7\n2\n\n\n\n\n\nDoes this mean something is wrong with our query? Let’s check.\nOf course, unlike cohorts of living people, dropping out of the cohort of legislators does not prevent you from returning later on. We can pull out a portion of the query creating cohorts and take a closer look. We first get the id_bioguide values for the female representatives from Arkansas (AR) who are around in period == 4. We then look at some data from legislators and legislators_terms for these two representatives.\n\nweird_id_bioguides <-\n  first_states |> \n  filter(first_state == \"AR\", gender == \"F\") |>\n  inner_join(legislators_terms, by = \"id_bioguide\") |>\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) |>\n  mutate(period = coalesce(year(age(date, first_term)), 0)) |>\n  filter(period >= 4) |>\n  select(id_bioguide) |>\n  distinct() |>\n  pull()\n\nlegislators |>\n  filter(id_bioguide %in% weird_id_bioguides) |>\n  inner_join(legislators_terms, by = \"id_bioguide\") |>\n  select(id_bioguide, full_name, term_type, term_start, term_end) |>\n  arrange(id_bioguide, term_start) |>\n  collect()\n\n\n\n\n\n\n\n\n\n\n\nid_bioguide\nfull_name\nterm_type\nterm_start\nterm_end\n\n\n\n\nC000138\nHattie Wyatt Caraway\nsen\n1931-12-07\n1933-03-03\n\n\nC000138\nHattie Wyatt Caraway\nsen\n1933-03-09\n1939-01-03\n\n\nC000138\nHattie Wyatt Caraway\nsen\n1939-01-03\n1945-01-03\n\n\nL000035\nBlanche Lambert Lincoln\nrep\n1993-01-05\n1995-01-03\n\n\nL000035\nBlanche Lambert Lincoln\nrep\n1995-01-04\n1997-01-03\n\n\nL000035\nBlanche Lambert Lincoln\nsen\n1999-01-06\n2005-01-03\n\n\nL000035\nBlanche Lambert Lincoln\nsen\n2005-01-04\n2011-01-03\n\n\n\n\n\nFrom the above, it seems that Blanche Lambert Lincoln would have dropped out of the cohort in periods 4 and 5, then returned in 6 and 7.\nTo aid this kind of “debugging” of our queries, we could easily have retained some underlying data in cohorts. For example, by added cohort_ids = array_agg(id_bioguide) to the query, we can easily interrogate cohorts for the underlying legislator IDs.\n\ncohorts <-\n  first_states |> \n  inner_join(legislators_terms, by = \"id_bioguide\") |>\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) |>\n  mutate(period = coalesce(year(age(date, first_term)), 0)) |>\n  group_by(first_state, gender, period) |>\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            cohort_ids = array_agg(id_bioguide),\n            .groups = \"drop\")\n\n\ncohorts |>\n  filter(first_state == \"AR\", gender == \"F\", \n         between(period, 3, 8)) |>\n  mutate(cohort_ids = as.character(cohort_ids)) |>\n  select(period, cohort_retained, cohort_ids) |>\n  arrange(period) |>\n  collect()\n\n\n\n\nperiod\ncohort_retained\ncohort_ids\n\n\n\n\n3\n4\n[L000035, C000138, W000634, O000061]\n\n\n4\n1\n[C000138]\n\n\n5\n1\n[C000138]\n\n\n6\n2\n[L000035, C000138]\n\n\n7\n2\n[L000035, C000138]\n\n\n8\n2\n[L000035, C000138]\n\n\n\n\n\n\nperiods <- \n  tibble(period = 1:20) |>\n  copy_to(db, df = _)\n\n\ncohort_sizes <-\n  first_states |>\n  group_by(gender, first_state) |>\n  summarize(cohort_size = n_distinct(id_bioguide),\n            .groups = \"drop\")\n\n\npct_retaineds <-\n  cohort_sizes |>\n  cross_join(periods) |>\n  left_join(cohorts, \n            join_by(gender, first_state, period)) |>\n  mutate(cohort_retained = coalesce(cohort_retained, 0),\n         pct_retained = cohort_retained * 1.0/ cohort_size) |>\n  select(gender, first_state, period, cohort_size,\n         cohort_retained, pct_retained)\n\n\npct_retaineds |>\n  filter(period %in% c(2, 4, 6, 8, 10),\n         first_state %in% c('AL', 'AR', 'CA')) |>\n  select(gender, first_state, cohort_size, period, pct_retained) |>\n  pivot_wider(names_from = \"period\",\n              names_prefix = \"yr\",\n              values_from = \"pct_retained\") |>\n  arrange(gender, first_state)\n\n\n\n\ngender\nfirst_state\ncohort_size\nyr2\nyr4\nyr6\nyr8\nyr10\n\n\n\n\nF\nAL\n3\n0.00\n0.0\n0.00\n0.00\n0.00\n\n\nF\nAR\n5\n0.80\n0.2\n0.40\n0.40\n0.40\n\n\nF\nCA\n25\n0.92\n0.8\n0.64\n0.68\n0.68\n\n\n\n\n\n\npct_retaineds |>\n  filter(first_state %in% c('AL', 'AR', 'CA')) |>\n  ggplot(aes(x = period, y = pct_retained, \n             colour = gender, group = gender)) +\n  geom_line() +\n  facet_wrap(vars(first_state), ncol = 1)\n\n\n\n\n\n\n4.3.6 Defining Cohorts from Dates other than the First Date\n\nWITH first_terms AS (\n  SELECT DISTINCT id_bioguide, term_type,\n    '2000-01-01'::date AS first_term,\n    min(term_start) AS min_start\n  FROM legislators_terms \n  WHERE term_start <= '2000-12-31' and term_end >= '2000-01-01'\n  GROUP BY 1, 2, 3),\n  \ncohort_dates AS (\n  SELECT id_bioguide, date\n  FROM legislators_terms\n  LEFT JOIN year_ends ON date BETWEEN term_start AND term_end),\n  \ncohorts AS (\n  SELECT term_type,\n    coalesce(date_part('year', age(date, first_term)), 0) AS period,\n    COUNT(DISTINCT id_bioguide) AS cohort_retained\n  FROM first_terms\n  JOIN cohort_dates\n  USING (id_bioguide)\n  GROUP BY 1, 2)\n  \nSELECT term_type, period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / \n    first_value(cohort_retained) OVER w AS pct_retained\nFROM cohorts\nWHERE period >= 0\nWINDOW w AS (PARTITION BY term_type ORDER BY period)\nORDER BY 2, 1\nLIMIT 4;\n\n\n4 records\n\n\nterm_type\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\nrep\n0\n440\n440\n1.0000000\n\n\nsen\n0\n101\n101\n1.0000000\n\n\nrep\n1\n440\n392\n0.8909091\n\n\nsen\n1\n101\n89\n0.8811881\n\n\n\n\n\n\nmin_year <- 2000L\n\nfirst_terms <-\n  legislators_terms |>\n  filter(year(term_start) <= min_year, \n         year(term_end) >= min_year) |>\n  mutate(first_term = as.Date(paste0(min_year, '-01-01'))) |>\n  group_by(id_bioguide, term_type, first_term) |>\n  summarize(min_start = min(term_start, na.rm = TRUE),\n            .groups = \"drop\")\n\n\ncohort_dates <-\n  legislators_terms |>\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) |>\n  filter(year(date) >= min_year) |>\n  select(id_bioguide, date)\n\n\ncohorts <-\n  first_terms |>\n  inner_join(cohort_dates, by = \"id_bioguide\") |>\n  mutate(period = coalesce(year(age(date, first_term)), 0)) |>\n  group_by(term_type, period) |>\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\")\n\n\npct_retained_2000 <-\n  cohorts |> \n  group_by(term_type) |>\n  window_order(period) |>\n  mutate(cohort_size = first(cohort_retained),\n         pct_retained = cohort_retained * 1.0 / cohort_size) |>\n  select(term_type, period, cohort_size, cohort_retained,\n         pct_retained)\n\npct_retained_2000 |>\n  arrange(period, term_type) |>\n  collect(n = 4)\n\n# A tibble: 4 × 5\n# Groups:   term_type [2]\n  term_type period cohort_size cohort_retained pct_retained\n  <chr>      <dbl>       <dbl>           <dbl>        <dbl>\n1 rep            0         439             439        1    \n2 sen            0         101             101        1    \n3 rep            1         439             392        0.893\n4 sen            1         101              89        0.881\n\n\n\npct_retained_2000 |>\n  filter(period <= 20) |>\n  mutate(pct_retained = pct_retained * 100) |>\n  ggplot(aes(x = period, y = pct_retained,\n             colour = term_type, group = term_type)) +\n  geom_line()"
  },
  {
    "objectID": "cohorts.html#related-cohort-analyses",
    "href": "cohorts.html#related-cohort-analyses",
    "title": "4  Cohorts",
    "section": "4.4 Related Cohort Analyses",
    "text": "4.4 Related Cohort Analyses\n\n4.4.1 Survivorship\n\nWITH first_centuries AS (\n   SELECT id_bioguide,\n    date_part('century', min(term_start)) AS first_century,\n    min(term_start) AS first_term,\n    max(term_start) AS last_term,\n    date_part('year', age(max(term_start), min(term_start))) AS tenure\n  FROM legislators_terms\n  GROUP BY 1)\n  \nSELECT first_century,\n  count(distinct id_bioguide) as cohort_size,\n  count(distinct case when tenure >= 10 then id_bioguide end) as survived_10,\n  count(distinct case when tenure >= 10 then id_bioguide end) * 1.0 \n / count(distinct id_bioguide) as pct_survived_10\nFROM first_centuries\nGROUP BY 1\nORDER BY 1;\n\n\n4 records\n\n\nfirst_century\ncohort_size\nsurvived_10\npct_survived_10\n\n\n\n\n18\n368\n83\n0.2255435\n\n\n19\n6299\n892\n0.1416098\n\n\n20\n5091\n1853\n0.3639756\n\n\n21\n760\n119\n0.1565789\n\n\n\n\n\n\nWITH first_centuries AS (\n  SELECT id_bioguide,\n    date_part('century', min(term_start)) AS first_century,\n    count(term_start) AS total_terms\n  FROM legislators_terms\n  GROUP BY 1)\n  \nSELECT first_century,\n  COUNT(DISTINCT id_bioguide) AS cohort_size,\n  COUNT(DISTINCT CASE WHEN total_terms >= 5 THEN id_bioguide END) AS survived_5,\n  COUNT(DISTINCT CASE WHEN total_terms >= 5 THEN id_bioguide END) * 1.0\n    / count(distinct id_bioguide) AS pct_survived_5_terms\nFROM first_centuries\nGROUP BY 1\nORDER BY 1;\n\n\n4 records\n\n\nfirst_century\ncohort_size\nsurvived_5\npct_survived_5_terms\n\n\n\n\n18\n368\n63\n0.1711957\n\n\n19\n6299\n711\n0.1128751\n\n\n20\n5091\n2153\n0.4229032\n\n\n21\n760\n205\n0.2697368\n\n\n\n\n\n\nWITH first_centuries AS (\n  SELECT id_bioguide,\n    date_part('century',min(term_start)) AS first_century,\n    count(term_start) AS total_terms\n  FROM legislators_terms\n  GROUP BY 1),\n\nterms AS (\n  SELECT generate_series as terms \n  FROM generate_series(1, 20, 1))\n  \nSELECT first_century,\n  terms,\n  count(distinct id_bioguide) as cohort,\n  count(distinct case when total_terms >= terms then id_bioguide end) as cohort_survived,\n  count(distinct case when total_terms >= terms then id_bioguide end) * 1.0 \n / count(distinct id_bioguide) as pct_survived\nFROM first_centuries\nCROSS JOIN terms\nGROUP BY 1, 2\nORDER BY first_century, terms\nLIMIT 3;\n\n\n3 records\n\n\nfirst_century\nterms\ncohort\ncohort_survived\npct_survived\n\n\n\n\n18\n1\n368\n368\n1.0000000\n\n\n18\n2\n368\n249\n0.6766304\n\n\n18\n3\n368\n153\n0.4157609\n\n\n\n\n\n\nfirst_centuries <-\n  legislators_terms |>\n  group_by(id_bioguide) |>\n  summarize(first_century = century(min(term_start, na.rm = TRUE)),\n            total_terms = n())\n\n\nterms <- \n  tibble(terms = 1:20) |>\n  copy_to(db, df = _, name = \"terms\")\n\n\nfirst_centuries |>\n  cross_join(terms) |>\n  mutate(survived_id = if_else(total_terms >= terms, id_bioguide, NA)) |>\n  group_by(first_century, terms) |>\n  summarize(cohort = n_distinct(id_bioguide),\n            cohort_survived = n_distinct(survived_id), \n            .groups = \"drop\") |>\n  mutate(pct_survived = 1.0 * cohort_survived / cohort) |>\n  arrange(first_century, terms)\n\n\n\n\nfirst_century\nterms\ncohort\ncohort_survived\npct_survived\n\n\n\n\n18\n1\n368\n368\n1.0000000\n\n\n18\n2\n368\n249\n0.6766304\n\n\n18\n3\n368\n153\n0.4157609\n\n\n\n\n\n\n\n4.4.2 Returnship, or Repeat Purchase Behaviour\n\nWITH rep_first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  GROUP BY 1)\n      \nSELECT date_part('century', first_term)::int AS cohort_century,\n  count(id_bioguide) AS reps\nFROM rep_first_terms\nGROUP BY 1\nORDER BY 1\n\n\n4 records\n\n\ncohort_century\nreps\n\n\n\n\n18\n299\n\n\n19\n5773\n\n\n20\n4481\n\n\n21\n683\n\n\n\n\n\n\nWITH rep_first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  GROUP BY 1)\n\nSELECT date_part('century', first_term) AS cohort_century,\n  count(id_bioguide) as reps\nFROM rep_first_terms\nGROUP BY 1\nORDER BY 1;\n\n\n4 records\n\n\ncohort_century\nreps\n\n\n\n\n18\n299\n\n\n19\n5773\n\n\n20\n4481\n\n\n21\n683\n\n\n\n\n\n\nWITH rep_first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  GROUP BY 1),\n\nnum_reps AS (\n  SELECT date_part('century', first_term) as cohort_century,\n    count(id_bioguide) as reps\n  FROM rep_first_terms\n  GROUP BY 1),\n  \nsens AS (\n  SELECT date_part('century',b.first_term) as cohort_century,\n    count(distinct b.id_bioguide) as rep_and_sen\n  FROM rep_first_terms b\n  JOIN legislators_terms c on b.id_bioguide = c.id_bioguide\n  and c.term_type = 'sen' and c.term_start > b.first_term\n  GROUP BY 1)\n  \nSELECT aa.cohort_century,\n  bb.rep_and_sen * 1.0 / aa.reps as pct_rep_and_sen\nFROM num_reps aa\nLEFT JOIN sens bb \non aa.cohort_century = bb.cohort_century;\n\n\n4 records\n\n\ncohort_century\npct_rep_and_sen\n\n\n\n\n20\n0.0566838\n\n\n21\n0.0366032\n\n\n18\n0.1906355\n\n\n19\n0.0569894\n\n\n\n\n\n\nWITH \n\nrep_first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  GROUP BY 1),\n  \nreps AS (\n  SELECT date_part('century',a.first_term) as cohort_century,\n    count(id_bioguide) as reps\n  FROM rep_first_terms a\n  WHERE first_term <= '2009-12-31'\n  GROUP BY 1),\n  \nsens AS (\n  SELECT date_part('century',b.first_term) as cohort_century,\n    count(distinct b.id_bioguide) as rep_and_sen\n  FROM rep_first_terms b\n  JOIN legislators_terms c \n  on b.id_bioguide = c.id_bioguide\n    and c.term_type = 'sen' and c.term_start > b.first_term\n    WHERE age(c.term_start, b.first_term) <= interval '10 years'\n  GROUP BY 1)\n  \nSELECT aa.cohort_century\n,bb.rep_and_sen * 1.0 / aa.reps as pct_rep_and_sen\nFROM reps aa\nLEFT JOIN sens bb \nUSING (cohort_century);\n\n\n4 records\n\n\ncohort_century\npct_rep_and_sen\n\n\n\n\n20\n0.0348137\n\n\n21\n0.0763636\n\n\n18\n0.0969900\n\n\n19\n0.0244240\n\n\n\n\n\n\nWITH \n\nrep_first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  GROUP BY 1),\n  \nreps AS (\n  SELECT date_part('century', first_term) as cohort_century,\n    count(id_bioguide) as reps\n  FROM rep_first_terms\n  WHERE first_term <= '2009-12-31'\n  GROUP BY 1),\n  \nsen_terms AS (\n  SELECT id_bioguide, term_type, term_start\n  FROM legislators_terms\n  WHERE term_type = 'sen'),\n  \ngaps AS (\n  SELECT id_bioguide, first_term, \n    age(term_start, first_term) AS gap\n  FROM rep_first_terms \n  JOIN sen_terms \n  USING (id_bioguide)\n  WHERE term_start > first_term),\n  \ngap_indicators AS (\n  SELECT date_part('century', first_term) as cohort_century,\n    count(distinct case when gap <= interval '5 years' then id_bioguide end) as rep_and_sen_5_yrs,\n    count(distinct case when gap <= interval '10 years' then id_bioguide end) as rep_and_sen_10_yrs,\n    count(distinct case when gap <= interval '15 years' then id_bioguide end) as rep_and_sen_15_yrs\n  FROM gaps\n  GROUP BY 1)\n\nSELECT cohort_century::int as cohort_century,\n  round(rep_and_sen_5_yrs * 1.0 / reps, 4) as pct_5_yrs,\n  round(rep_and_sen_10_yrs * 1.0 / reps, 4) as pct_10_yrs,\n  round(rep_and_sen_15_yrs * 1.0 / reps, 4) as pct_15_yrs\nFROM reps\nLEFT JOIN gap_indicators\nUSING (cohort_century)\nORDER BY cohort_century\n\n\n4 records\n\n\ncohort_century\npct_5_yrs\npct_10_yrs\npct_15_yrs\n\n\n\n\n18\n0.0502\n0.0970\n0.1438\n\n\n19\n0.0088\n0.0244\n0.0409\n\n\n20\n0.0100\n0.0348\n0.0478\n\n\n21\n0.0400\n0.0764\n0.0873\n\n\n\n\n\n\n\n4.4.3 Cumulative Calculations\n\nWITH types AS (\n  SELECT distinct id_bioguide,\n    first_value(term_type) over w as first_type,\n    min(term_start) over w as first_term,\n    (min(term_start) over w) + interval '10 years' as first_plus_10\n  FROM legislators_terms\n  WINDOW w AS (partition by id_bioguide order by term_start))\n  \nSELECT date_part('century',a.first_term)::int as century,\n  first_type,\n  count(distinct a.id_bioguide) as cohort,\n  count(b.term_start) as terms\nFROM types a\nLEFT JOIN legislators_terms b \non a.id_bioguide = b.id_bioguide and \n  b.term_start between a.first_term and a.first_plus_10\nGROUP BY 1, 2;\n\n\n8 records\n\n\ncentury\nfirst_type\ncohort\nterms\n\n\n\n\n20\nrep\n4473\n16203\n\n\n19\nrep\n5744\n12165\n\n\n20\nsen\n618\n1008\n\n\n19\nsen\n555\n795\n\n\n18\nsen\n71\n101\n\n\n21\nsen\n77\n118\n\n\n21\nrep\n683\n2203\n\n\n18\nrep\n297\n760\n\n\n\n\n\n\nWITH a AS (\n  SELECT DISTINCT id_bioguide,\n    first_value(term_type) OVER w AS first_type,\n    min(term_start) OVER w AS first_term\n  FROM legislators_terms\n  WINDOW w AS (PARTITION BY id_bioguide ORDER BY term_start)),\n  \ncohort_terms AS (\n  SELECT date_part('century',a.first_term)::int AS century,\n    first_type,\n    COUNT(distinct a.id_bioguide) AS cohort,\n    COUNT(b.term_start) as terms,\n    COUNT(b.term_start) * 1.0 / COUNT(DISTINCT a.id_bioguide) AS cohort_term\n  FROM a\n  LEFT JOIN legislators_terms b \n  on a.id_bioguide = b.id_bioguide AND \n    b.term_start BETWEEN a.first_term AND a.first_term + interval '10 years'\n  GROUP BY 1, 2)\n  \nSELECT century,\n  max(case when first_type = 'rep' then cohort end) as rep_cohort,\n  max(case when first_type = 'rep' then cohort_term end) as avg_rep_terms,\n  max(case when first_type = 'sen' then cohort end) as sen_cohort,\n  max(case when first_type = 'sen' then cohort_term end) as avg_sen_terms\nFROM cohort_terms\nGROUP BY 1;\n\n\n4 records\n\n\ncentury\nrep_cohort\navg_rep_terms\nsen_cohort\navg_sen_terms\n\n\n\n\n20\n4473\n3.622401\n618\n1.631068\n\n\n19\n5744\n2.117862\n555\n1.432432\n\n\n21\n683\n3.225476\n77\n1.532468\n\n\n18\n297\n2.558923\n71\n1.422535"
  },
  {
    "objectID": "cohorts.html#cross-section-analysis-through-a-cohort-lens",
    "href": "cohorts.html#cross-section-analysis-through-a-cohort-lens",
    "title": "4  Cohorts",
    "section": "4.5 Cross-Section Analysis, Through a Cohort Lens",
    "text": "4.5 Cross-Section Analysis, Through a Cohort Lens\n\nSELECT b.date, count(distinct a.id_bioguide) as legislators\nFROM legislators_terms a\nJOIN year_ends b on b.date between a.term_start and a.term_end\nand b.date <= '2020-12-31'\nGROUP BY 1\n;\n\n\nDisplaying records 1 - 10\n\n\ndate\nlegislators\n\n\n\n\n1855-12-31\n306\n\n\n1856-12-31\n306\n\n\n1853-12-31\n315\n\n\n1854-12-31\n316\n\n\n1851-12-31\n303\n\n\n1852-12-31\n310\n\n\n1849-12-31\n310\n\n\n1850-12-31\n313\n\n\n1848-12-31\n306\n\n\n1847-12-31\n301\n\n\n\n\n\n\nSELECT b.date\n,date_part('century',first_term)::int as century\n,count(distinct a.id_bioguide) as legislators\nFROM legislators_terms a\nJOIN year_ends b on b.date between a.term_start and a.term_end\nand b.date <= '2020-12-31'\nJOIN\n(\n        SELECT id_bioguide, min(term_start) as first_term\n        FROM legislators_terms\n        GROUP BY 1\n) c on a.id_bioguide = c.id_bioguide        \nGROUP BY 1,2\n;\n\n\nDisplaying records 1 - 10\n\n\ndate\ncentury\nlegislators\n\n\n\n\n1919-12-31\n20\n516\n\n\n1965-12-31\n20\n546\n\n\n1995-12-31\n20\n539\n\n\n2005-12-31\n20\n388\n\n\n1975-12-31\n20\n547\n\n\n1811-12-31\n19\n153\n\n\n1893-12-31\n19\n470\n\n\n1909-12-31\n20\n344\n\n\n1937-12-31\n20\n547\n\n\n2010-12-31\n21\n262\n\n\n\n\n\n\nSELECT date\n,century\n,legislators\n,sum(legislators) over (partition by date) as cohort\n,legislators * 100.0 / sum(legislators) over (partition by date) as pct_century\nFROM\n(\n        SELECT b.date\n        ,date_part('century',first_term)::int as century\n        ,count(distinct a.id_bioguide) as legislators\n        FROM legislators_terms a\n        JOIN year_ends b on b.date between a.term_start and a.term_end\nand b.date <= '2020-01-01'\n        JOIN\n        (\n                SELECT id_bioguide, min(term_start) as first_term\n                FROM legislators_terms\n                GROUP BY 1\n        ) c on a.id_bioguide = c.id_bioguide        \n        GROUP BY 1,2\n) a\nORDER BY 1,2\n;\n\n\nDisplaying records 1 - 10\n\n\ndate\ncentury\nlegislators\ncohort\npct_century\n\n\n\n\n1789-12-31\n18\n89\n89\n100\n\n\n1790-12-31\n18\n95\n95\n100\n\n\n1791-12-31\n18\n99\n99\n100\n\n\n1792-12-31\n18\n101\n101\n100\n\n\n1793-12-31\n18\n141\n141\n100\n\n\n1794-12-31\n18\n140\n140\n100\n\n\n1795-12-31\n18\n145\n145\n100\n\n\n1796-12-31\n18\n150\n150\n100\n\n\n1797-12-31\n18\n152\n152\n100\n\n\n1798-12-31\n18\n155\n155\n100\n\n\n\n\n\n\nWITH \n\nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) as first_term\n                FROM legislators_terms\n                GROUP BY 1),\n                \naa AS (\n  SELECT b.date,\n    date_part('century',first_term)::int as century,\n    count(distinct a.id_bioguide) as legislators\n  FROM legislators_terms a\n  JOIN year_ends b \n  ON b.date between a.term_start and a.term_end\n    and b.date <= '2020-01-01'\n  JOIN first_terms c \n  USING (id_bioguide)\n  GROUP BY 1,2) \n                \nSELECT date,\n  coalesce(sum(case when century = 18 then legislators end) * 100.0 / sum(legislators),0) as pct_18,\n  coalesce(sum(case when century = 19 then legislators end) * 100.0 / sum(legislators),0) as pct_19,\n  coalesce(sum(case when century = 20 then legislators end) * 100.0 / sum(legislators),0) as pct_20,\n  coalesce(sum(case when century = 21 then legislators end) * 100.0 / sum(legislators),0) as pct_21\nFROM aa\nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\ndate\npct_18\npct_19\npct_20\npct_21\n\n\n\n\n1789-12-31\n100\n0\n0\n0\n\n\n1790-12-31\n100\n0\n0\n0\n\n\n1791-12-31\n100\n0\n0\n0\n\n\n1792-12-31\n100\n0\n0\n0\n\n\n1793-12-31\n100\n0\n0\n0\n\n\n1794-12-31\n100\n0\n0\n0\n\n\n1795-12-31\n100\n0\n0\n0\n\n\n1796-12-31\n100\n0\n0\n0\n\n\n1797-12-31\n100\n0\n0\n0\n\n\n1798-12-31\n100\n0\n0\n0\n\n\n\n\n\n\nSELECT id_bioguide, date,\n  count(date) over (partition by id_bioguide order by date rows between unbounded preceding and current row) as cume_years\nFROM (\n  SELECT distinct id_bioguide, date\n  FROM legislators_terms\n  JOIN year_ends \n  ON date between term_start and term_end\n    and date <= '2020-01-01');\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\ndate\ncume_years\n\n\n\n\nA000009\n1973-12-31\n1\n\n\nA000009\n1974-12-31\n2\n\n\nA000009\n1975-12-31\n3\n\n\nA000009\n1976-12-31\n4\n\n\nA000009\n1977-12-31\n5\n\n\nA000009\n1978-12-31\n6\n\n\nA000009\n1979-12-31\n7\n\n\nA000009\n1980-12-31\n8\n\n\nA000009\n1981-12-31\n9\n\n\nA000009\n1982-12-31\n10\n\n\n\n\n\n\nSELECT date, cume_years,\n  count(distinct id_bioguide) as legislators\nFROM\n(\n    SELECT id_bioguide, date\n    ,count(date) over (partition by id_bioguide order by date rows between unbounded preceding and current row) as cume_years\n    FROM\n    (\n        SELECT distinct a.id_bioguide, b.date\n        FROM legislators_terms a\n        JOIN year_ends b \n        on b.date between a.term_start and a.term_end and b.date <= '2020-01-01'\n        GROUP BY 1,2\n    ) aa\n) aaa\nGROUP BY 1,2\n;\n\n\nDisplaying records 1 - 10\n\n\ndate\ncume_years\nlegislators\n\n\n\n\n1995-12-31\n17\n30\n\n\n1999-12-31\n5\n68\n\n\n2018-12-31\n6\n71\n\n\n1988-12-31\n2\n54\n\n\n1978-12-31\n2\n79\n\n\n1903-12-31\n7\n75\n\n\n2018-12-31\n2\n61\n\n\n1948-12-31\n3\n10\n\n\n1868-12-31\n2\n119\n\n\n1999-12-31\n9\n30\n\n\n\n\n\n\nWITH aa AS (\n  SELECT DISTINCT a.id_bioguide, b.date\n  FROM legislators_terms a\n  JOIN year_ends b \n    on b.date BETWEEN a.term_start AND a.term_end AND b.date <= '2020-01-01'\n  GROUP BY 1,2),\n  \naaa AS (\n  SELECT id_bioguide, date,\n    count(date) OVER (PARTITION BY id_bioguide ORDER BY date) AS cume_years\n  FROM aa),\n  \naaaa AS (\n  SELECT date, cume_years,\n    COUNT(DISTINCT id_bioguide) as legislators\n  FROM aaa\n  GROUP BY 1,2)\nSELECT date, count(*) as tenures\nFROM aaaa\nGROUP BY 1;\n\n\nDisplaying records 1 - 10\n\n\ndate\ntenures\n\n\n\n\n1973-12-31\n35\n\n\n1974-12-31\n36\n\n\n1975-12-31\n36\n\n\n1976-12-31\n36\n\n\n1977-12-31\n34\n\n\n1978-12-31\n34\n\n\n1979-12-31\n31\n\n\n1980-12-31\n32\n\n\n1981-12-31\n31\n\n\n1982-12-31\n32\n\n\n\n\n\n\nWITH term_dates AS (\n  SELECT distinct a.id_bioguide, b.date\n  FROM legislators_terms a\n  JOIN year_ends b \n  on b.date between a.term_start and a.term_end and b.date <= '2020-01-01'),\n\ncum_term_dates AS (\n  SELECT id_bioguide, date,\n    count(date) over (partition by id_bioguide order by date rows between unbounded preceding and current row) as cume_years\n  FROM term_dates),\n  \ncum_term_bands AS (\n  SELECT date,\n    case when cume_years <= 4 then '1 to 4'\n         when cume_years <= 10 then '5 to 10'\n        when cume_years <= 20 then '11 to 20'\n         else '21+' end as tenure,\n    count(distinct id_bioguide) as legislators\n  FROM cum_term_dates\n  GROUP BY 1,2)\n  \nSELECT date, tenure,\n  legislators * 100.0 / sum(legislators) over w as pct_legislators \nFROM cum_term_bands\nWINDOW w AS (partition by date)\nORDER BY date DESC;\n\n\nDisplaying records 1 - 10\n\n\ndate\ntenure\npct_legislators\n\n\n\n\n2019-12-31\n1 to 4\n29.98138\n\n\n2019-12-31\n5 to 10\n32.02980\n\n\n2019-12-31\n21+\n17.87710\n\n\n2019-12-31\n11 to 20\n20.11173\n\n\n2018-12-31\n11 to 20\n21.33581\n\n\n2018-12-31\n5 to 10\n33.76623\n\n\n2018-12-31\n1 to 4\n25.60297\n\n\n2018-12-31\n21+\n19.29499\n\n\n2017-12-31\n11 to 20\n21.37546\n\n\n2017-12-31\n5 to 10\n34.75836\n\n\n\n\n\n\nterm_dates <-\n  legislators_terms |>\n  inner_join(year_ends |> filter(date <= '2020-01-01'),\n             join_by(between(y$date, x$term_start, x$term_end))) |>\n  distinct(id_bioguide, date) \n\n\ncum_term_dates <-\n  term_dates |>\n  group_by(id_bioguide) |>\n  window_order(date) |>\n  window_frame(-Inf, 0) |>\n  mutate(cume_years = n()) |>\n  ungroup() |>\n  select(id_bioguide, date, cume_years) \n\n\ncum_term_bands <-\n  cum_term_dates |> \n  mutate(tenure = case_when(cume_years <= 4 ~ '1 to 4',\n                            cume_years <= 10 ~ '5 to 10',\n                            cume_years <= 20 ~ '11 to 20',\n                            TRUE ~ '21+')) |>\n  group_by(date, tenure) |>\n  summarize(legislators = n_distinct(id_bioguide),\n            .groups = \"drop\") \n\n\ntotal_legs <-\n  cum_term_bands |>\n  group_by(date) |>\n  summarize(num_legs = sum(legislators, na.rm = TRUE),\n            .groups = \"drop\")\n\n\ncum_term_bands |>\n  inner_join(total_legs, by = \"date\") |>\n  mutate(pct_legislators = legislators * 100.0 / num_legs) |>\n  select(date, tenure, pct_legislators) |>\n  arrange(desc(date)) |>\n  collect(n = 8)\n\n\n\n\ndate\ntenure\npct_legislators\n\n\n\n\n2019-12-31\n5 to 10\n32.02980\n\n\n2019-12-31\n1 to 4\n29.98138\n\n\n2019-12-31\n11 to 20\n20.11173\n\n\n2019-12-31\n21+\n17.87710\n\n\n2018-12-31\n11 to 20\n21.33581\n\n\n2018-12-31\n5 to 10\n33.76623\n\n\n2018-12-31\n21+\n19.29499\n\n\n2018-12-31\n1 to 4\n25.60297\n\n\n\n\n\n\ndbDisconnect(db, shutdown = TRUE)"
  },
  {
    "objectID": "text.html",
    "href": "text.html",
    "title": "5  Text Analysis",
    "section": "",
    "text": "I would actually reframe this question as “why store text data in a database?” and offer different reasons from those offered in Tanimura (2021). To structure my answer I will use a representative textual analysis problem (really set of problems) I’ve managed in the past.\nPublic companies routinely communicate with investors or their representatives through conference calls. Most public companies hold conference calls when they announce their earnings results for a quarter or year. The typical earnings conference call starts with a presentation of results by management, typically the CEO or CEO, followed by the “Q&A” portion of the call during which call participants can ask questions of management Apart from representatives of the company, the typical participant in a conference call is an equity analyst. Equity analysts typically cover a relatively small numbers of companies, typically in a single industry, and provide insights and investment recommendations and related to their covered companies and industries.\nAnalyst recommendations usually come from valuation analyses that draw on projections of future financial performance. An analyst’s valuation model is usually constructed using a spreadsheet and to some extent an analyst’s questions on a conference call will seek information that can be used for model inputs.\nTranscripts of conference calls are collected by a number of data providers, who presumably supply them to various users, including investors and academic researchers. I have used transcripts of conference calls in my own research. The data provider in my case provided a continuous stream of transcripts in XML files. Each call is contained in its own XML file with a file name that indicates the unique identifier of the call. Some elements of the call are contained in structured XML, but the bulk of the data in a call are in a single unstructured XML field.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\n\n\ndb <- dbConnect(duckdb::duckdb())\nufo <-\n  tbl(db, \"read_csv_auto('data/ufo*.csv', HEADER=TRUE)\") |>\n  mutate(id = row_number()) |> \n  compute(name = \"ufo\")\n\n\nufo |>\n  mutate(length = length(sighting_report)) |>\n  ggplot(aes(x = length)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\ninitcap <- function(x) {\n  if_else(nchar(x) > 0,\n          paste0(toupper(substr(x, 1, 1)),\n                 tolower(substr(x, 2, nchar(x)))), x)\n}\n\n\nufo %>%\n  mutate(has_occurred = grepl('Occurred :', sighting_report),\n         has_reported = grepl('Reported:', sighting_report),\n         has_entered = grepl('\\\\(.*Entered as :', sighting_report),\n         has_posted = grepl('Posted:', sighting_report),\n         has_location = grepl('Location:', sighting_report),\n         has_shape = grepl('Shape:', sighting_report),\n         has_duration = grepl('Duration:', sighting_report)) %>%\n  # show_query()\n  count(has_occurred, has_reported, has_entered, has_posted,\n        has_location, has_shape, has_duration)\n\n# Source:   SQL [2 x 8]\n# Database: DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n  has_occurred has_reported has_entered has_posted has_location has_shape\n  <lgl>        <lgl>        <lgl>       <lgl>      <lgl>        <lgl>    \n1 TRUE         TRUE         TRUE        TRUE       TRUE         TRUE     \n2 TRUE         TRUE         FALSE       TRUE       TRUE         TRUE     \n# ℹ 2 more variables: has_duration <lgl>, n <dbl>\n\n\n\nufo %>% select(sighting_report)\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n   sighting_report                                                              \n   <chr>                                                                        \n 1 Occurred : (Entered as : --)Reported: 12/27/2013 2:31:11 PM 14:31Posted: 1/1…\n 2 Occurred : (Entered as : 2010)Reported: 3/7/2014 10:36:37 PM 22:36Posted: 3/…\n 3 Occurred : (Entered as : unknown)Reported: 3/21/2006 12:11:10 PM 12:11Posted…\n 4 Occurred : 00:00 (Entered as : 0:00)Reported: 6/15/2013 11:52:48 AM 11:52Pos…\n 5 Occurred : 00:00 (Entered as : 1947-2008 0:00)Reported: 2/3/2008 3:04:26 AM …\n 6 Occurred : 00:00 (Entered as : 20)Reported: 9/17/2012 12:14:05 PM 12:14Poste…\n 7 Occurred : 00:00 (Entered as : 2018 0:00)Reported: 12/18/2019 10:24:17 PM 22…\n 8 Occurred : 00:00 (Entered as : 5/20/20014 0:00)Reported: 4/7/2014 1:39:31 AM…\n 9 Occurred : 00:04 (Entered as : 00:04)Reported: 12/28/2019 3:52:42 AM 03:52Po…\n10 Occurred : 00:34 (Entered as : 09/91/19 00:34)Reported: 9/1/2019 12:46:41 AM…\n# ℹ more rows\n\n\n\nregex <- paste0(\"Occurred :\\\\s*(.*)\\\\s*\",\n                \"Reported:\\\\s*(.* [AP]M).*?\\\\s*\",\n                \"Posted:\\\\s*(.*)\\\\s*\",\n                \"Location:\\\\s*(.*)\\\\s*\",\n                \"Shape:\\\\s*(.*)\\\\s*\",\n                \"Duration:\\\\s*(.*)\\\\s*\")\n\nregex2 <- paste0(\"^(.*?)\",\n                 \"(?:\\\\s*\\\\(Entered as :\\\\s*(.*)\\\\))?\\\\s*$\")\n\nufo_extracted <- \n  ufo |>\n  mutate(occurred_plus = regexp_extract(sighting_report, regex, 1L),\n         reported = regexp_extract(sighting_report, regex, 2L),\n         posted = regexp_extract(sighting_report, regex, 3L),\n         location = regexp_extract(sighting_report, regex, 4L),\n         shape = regexp_extract(sighting_report, regex, 5L),\n         duration = regexp_extract(sighting_report, regex, 6L)) |>\n  select(id, occurred_plus:duration) |>\n  mutate(occurred_raw = regexp_extract(occurred_plus, regex2, 1L),\n         entered = regexp_extract(occurred_plus, regex2, 1L)) |>\n  select(-occurred_plus) |>\n  mutate(location_clean = regexp_replace(location, \n                                         \"(outside of|close to)\", \"near\")) |>\n  mutate(reported = \n           case_when(reported == '' ~ NA,\n                     nchar(reported) < 8 ~ NA,\n                     regexp_matches(reported, \"[AP]M\") ~\n                       strptime(reported, \"%m/%d/%Y %I:%M:%S %p\"),\n                     TRUE ~ strptime(reported, \"%m/%d/%Y %H:%M:%S\")),\n         occurred = \n           case_when(occurred_raw == '' ~ NA,\n                     nchar(occurred_raw) < 8 ~ NA,\n                     regexp_matches(occurred_raw, \"^[0-9]+/[0-9]+/[0-9]{4}$\") ~\n                       strptime(occurred_raw, \"%m/%d/%Y\"),\n                     regexp_matches(occurred_raw, \"[AP]M\") ~\n                       strptime(occurred_raw, \"%m/%d/%Y %I:%M:%S %p\"),\n                     regexp_matches(occurred_raw, \"[0-9]{2}:[0-9]{2}:[0-9]{2}\") ~\n                       strptime(occurred_raw, \"%m/%d/%Y %H:%M:%S\"),\n                     TRUE ~ strptime(occurred_raw, \"%m/%d/%Y %H:%M\")),\n         posted = if_else(posted == '', NA, \n                          as.Date(strptime(posted, \"%m/%d/%Y\")))) |>\n  collect() |>\n  mutate(shape = initcap(shape)) \n\n\nufo_extracted |>\n  ggplot(aes(y = fct_rev(fct_infreq(shape)))) +\n  geom_bar() +\n  ylab(\"Shape\")\n\n\n\n\n\nufo_extracted |>\n  filter(!is.na(occurred_raw)) |>\n  count(occurred_raw) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10) |>\n  mutate(occurred_raw = fct_rev(fct_inorder(as.character(occurred_raw)))) |>\n  ggplot(aes(y = occurred_raw, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nufo_extracted |>\n  count(duration) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10) |>\n  mutate(duration = fct_rev(fct_inorder(duration))) |>\n  ggplot(aes(y = duration, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nufo_extracted |>\n  count(location) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10) |>\n  mutate(location = fct_rev(fct_inorder(location))) |>\n  ggplot(aes(y = location, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "anomalies.html",
    "href": "anomalies.html",
    "title": "6  Anomaly Detection",
    "section": "",
    "text": "library(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\nThe ntile() offered by dbplyr works a little differently from other window functions. Rather than preceding the mutate with a window_order(), we need to specify an order_by argument.1\nBelow Cathy uses INNER JOIN with ON 1 = 1.\nInstead, I use CROSS JOIN (this is cross_join in dplyr).\nThe output in the book differs from one gets from running the code, so I add !(mag %in% c(-9, -9.99)) to get closer to the book’s output.\nNote that in constructing mag_stats, I follow the book in using avg(mag) and stddev_pop(mag). In practice, I would probably lean more to using R-compatible mean(mag, na.rm = TRUE) and sd(mag, na.rm = TRUE), respectively. This makes little differ in practice—the only difference is that sd is translated into stddev_samp instead of stddev_pop, which is barely different in this case—but I believe it is helpful to be consistent where possible. Often I find myself moving the data processing from PosrtgreSQL to R or vice versa and this is much easier if the dbplyr code is consistent with the dplyr equivalent."
  },
  {
    "objectID": "anomalies.html#graphing-to-find-anomalies-visually",
    "href": "anomalies.html#graphing-to-find-anomalies-visually",
    "title": "6  Anomaly Detection",
    "section": "6.1 Graphing to find anomalies visually",
    "text": "6.1 Graphing to find anomalies visually\n\nearthquakes |>\n  filter(!is.na(mag)) |>\n  ggplot(aes(x = mag)) +\n  geom_histogram(breaks = seq(-10, 10, 0.1))\n\n\n\n\n\nearthquakes |>\n  filter(!is.na(mag),\n         between(mag, 7.2, 9.5)) |>\n  ggplot(aes(x = mag)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\nearthquakes |>\n  filter(!is.na(mag),\n         between(mag, 7.2, 9.5)) |>\n  ggplot(aes(x = mag)) +\n  geom_bar() +\n  scale_x_binned(breaks = seq(7.2, 9.5, 0.1))\n\n\n\n\n\nearthquakes |>\n  filter(!is.na(mag), !is.na(depth)) |>\n  distinct(mag, depth) |>\n  ggplot(aes(x = mag, y = depth)) +\n  geom_point(size = 0.1, colour = \"blue\")\n\n\n\n\n\nearthquakes |>\n  filter(!is.na(mag), !is.na(depth)) |>\n  filter(between(mag, 4, 7), depth <= 50) |>\n  ggplot(aes(x = mag, y = depth)) +\n  geom_count(color = \"blue\")\n\n\n\n\n\njapan_quakes <-\n  earthquakes |>\n  filter(!is.na(mag), !is.na(depth)) |>\n  filter(grepl(\"Japan\", place)) \n\njapan_quakes |>\n  ggplot(aes(y = mag)) +\n  geom_boxplot(width = 0.5)\n\n\n\n\n\njapan_quakes |>\n  summarize(p25 = quantile(mag, probs = 0.25, na.rm = TRUE),\n            p50 = quantile(mag, probs = 0.50, na.rm = TRUE),\n            p75 = quantile(mag, probs = 0.75, na.rm = TRUE)) |>\n  mutate(iqr = (p75 - p25) * 1.5,\n         lower_whisker = p25 - (p75 - p25) * 1.5,\n         upper_whisker = p75 + (p75 - p25) * 1.5) |>\n  kable()\n\n\n\n\np25\np50\np75\niqr\nlower_whisker\nupper_whisker\n\n\n\n\n4.3\n4.5\n4.7\n0.6\n3.7\n5.3\n\n\n\n\n\n\njapan_quakes |>\n  select(mag, time) |>\n  collect() |>\n  mutate(year = as.factor(year(time))) |>\n  ggplot(aes(y = mag, x = year, group = year)) +\n  geom_boxplot()"
  },
  {
    "objectID": "anomalies.html#forms-of-anomalies",
    "href": "anomalies.html#forms-of-anomalies",
    "title": "6  Anomaly Detection",
    "section": "6.2 Forms of Anomalies",
    "text": "6.2 Forms of Anomalies\n\n6.2.1 Anomalous Values\n\nearthquakes |>\n  filter(mag >= 1.08) |>\n  group_by(mag) |>\n  summarize(count = n()) |>\n  arrange(mag) |>\n  collect(n = 5) |>\n  kable()\n\n\n\n\nmag\ncount\n\n\n\n\n1.08\n3863\n\n\n1.08\n1\n\n\n1.09\n3712\n\n\n1.10\n39728\n\n\n1.11\n3674\n\n\n\n\n\n\nearthquakes |>\n  filter(depth > 600) |>\n  group_by(net) |>\n  summarize(count = n()) |>\n  arrange(net) |>\n  collect(n = 5) |>\n  kable()\n\n\n\n\nnet\ncount\n\n\n\n\nus\n1215\n\n\n\n\n\n\nearthquakes |>\n  filter(depth > 600) |>\n  group_by(place) |>\n  summarize(count = n()) |>\n  arrange(place) |>\n  collect(n = 5) |>\n  kable()\n\n\n\n\nplace\ncount\n\n\n\n\n100km NW of Ndoi Island, Fiji\n1\n\n\n100km SSW of Ndoi Island, Fiji\n1\n\n\n100km SW of Ndoi Island, Fiji\n1\n\n\n101km ENE of Suva, Fiji\n1\n\n\n101km NNE of Ndoi Island, Fiji\n1\n\n\n\n\n\n\nearthquakes |>\n  filter(depth > 600) |>\n  mutate(place_name = case_when(grepl(' of ', place) ~\n                                  split_part(place, ' of ', 2L),\n                                TRUE ~ place)) |>\n  group_by(place_name) |>\n  summarize(count = n()) |>\n  arrange(desc(count)) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\nplace_name\ncount\n\n\n\n\nNdoi Island, Fiji\n487\n\n\nFiji region\n186\n\n\nLambasa, Fiji\n140\n\n\n\n\n\n\nearthquakes |>\n  summarize(distinct_types = n_distinct(type),\n            distinct_lower = n_distinct(lower(type))) |>\n  kable()\n\n\n\n\ndistinct_types\ndistinct_lower\n\n\n\n\n25\n24\n\n\n\n\n\n\n\n6.2.2 Anomalous Counts or Frequencies\nWhy use date_trunc('year',time)::date as earthquake_year?\n\nSELECT EXTRACT(year FROM time) AS earthquake_year, \n  COUNT(*) AS earthquakes\nFROM earthquakes\nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nearthquake_year\nearthquakes\n\n\n\n\n2010\n122322\n\n\n2011\n107397\n\n\n2012\n105693\n\n\n2013\n114368\n\n\n2014\n135247\n\n\n2015\n122914\n\n\n2016\n122420\n\n\n2017\n130622\n\n\n2018\n179304\n\n\n2019\n171116\n\n\n\n\n\n\nearthquakes |>\n  mutate(earthquake_year = as.character(year(time))) |>\n  group_by(earthquake_year) |>\n  summarize(earthquakes = n()) |>\n  ggplot(aes(x = earthquake_year, y = earthquakes)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n2\n\nearthquakes |>\n  mutate(earthquake_year = as.character(year(time))) |>\n  select(earthquake_year) |>\n  ggplot(aes(x = earthquake_year)) +\n  geom_bar()\n\n\nearthquakes |>\n  mutate(earthquake_month = floor_date(time, \"month\")) |>\n  group_by(earthquake_month) |>\n  summarize(earthquakes = n(), .groups = \"drop\") |>\n  ggplot(aes(x = earthquake_month, y = earthquakes)) +\n  geom_line()\n\n\n\n\nFrom the book, “it turns out that the increase in earthquakes starting in 2017 can be at least partially explained by the status field. The status indicates whether the event has been reviewed by a human (‘reviewed’) or was directly posted by a system without review (‘automatic’).” This can be seen in the following plot.3\n\nearthquakes |>\n  mutate(earthquake_month = floor_date(time, \"month\")) |>\n  group_by(earthquake_month, status) |>\n  summarize(earthquakes = n(), .groups = \"drop\") |>\n  ggplot(aes(x = earthquake_month, y = earthquakes, color = status)) +\n  geom_line()\n\n\n\n\n\nearthquakes |>\n  filter(mag >= 6) |>\n  group_by(place) |>\n  summarize(earthquakes = n(), .groups = \"drop\") |>\n  arrange(desc(earthquakes)) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\nplace\nearthquakes\n\n\n\n\nnear the east coast of Honshu, Japan\n52\n\n\noff the east coast of Honshu, Japan\n34\n\n\nVanuatu\n28\n\n\n\n\n\nFor the next query, it seems easy enough to just put the result in a plot.\n\nearthquakes |>\n  filter(mag >= 6) |>\n  mutate(place = if_else(grepl(' of ', place),\n                         split_part(place, ' of ', 2L), \n                         place)) |>\n  count(place, name = \"earthquakes\") |>\n  arrange(desc(earthquakes)) |>\n  collect(n = 10) |>\n  ggplot(aes(y = fct_inorder(place), \n             x = earthquakes)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n6.2.3 Anomalies from the Absence of Data"
  },
  {
    "objectID": "anomalies.html#handling-anomalies",
    "href": "anomalies.html#handling-anomalies",
    "title": "6  Anomaly Detection",
    "section": "6.3 Handling Anomalies",
    "text": "6.3 Handling Anomalies\n\n6.3.1 Investigation\n\n\n6.3.2 Removal\n\nearthquakes |>\n  filter(!mag %in% c(-9,-9.99)) |>\n  select(time, mag, type) |>\n  collect(n = 10) |>\n  kable()\n\n\n\n\ntime\nmag\ntype\n\n\n\n\n2016-09-28 05:33:17\n0.73\nearthquake\n\n\n2016-09-28 05:35:37\n0.58\nearthquake\n\n\n2016-09-28 05:41:37\n2.16\nearthquake\n\n\n2016-09-28 05:47:16\n1.50\nearthquake\n\n\n2016-09-28 05:47:59\n1.44\nearthquake\n\n\n2016-09-28 05:52:54\n2.20\nearthquake\n\n\n2016-09-28 05:56:43\n2.23\nearthquake\n\n\n2016-09-28 05:59:17\n2.09\nearthquake\n\n\n2016-09-28 06:09:07\n0.89\nearthquake\n\n\n2016-09-28 06:11:14\n1.66\nearthquake\n\n\n\n\n\n\nearthquakes |>\n  summarize(avg_mag = avg(mag),\n            avg_mag_adjusted = avg(if_else(mag > -9, mag, NA))) |>\n  kable()\n\n\n\n\navg_mag\navg_mag_adjusted\n\n\n\n\n1.625101\n1.627323\n\n\n\n\n\n\nearthquakes |>\n  filter(place == 'Yellowstone National Park, Wyoming') |>\n  summarize(avg_mag = avg(mag),\n            avg_mag_adjusted = avg(if_else(mag > -9, mag, NA))) |>\n  kable()\n\n\n\n\navg_mag\navg_mag_adjusted\n\n\n\n\n0.4063935\n0.9233279\n\n\n\n\n\n\n\n6.3.3 Replacement with Alternate Values\n\nearthquakes |>\n  mutate(event_type = if_else(type == 'earthquake', type, 'Other')) |>\n  count(event_type) |>\n  kable()\n\n\n\n\nevent_type\nn\n\n\n\n\nearthquake\n1461750\n\n\nOther\n34176\n\n\n\n\n\n\nextremes <-\n  earthquakes |>\n  summarize(p95 = quantile(mag, probs = 0.95, na.rm = TRUE),\n            p05 = quantile(mag, probs = 0.05, na.rm = TRUE))\n\nextremes |> kable()\n\n\n\n\np95\np05\n\n\n\n\n4.5\n0.12\n\n\n\n\n\nNote that this SQL from the book4\n\nCASE \n  WHEN mag > p95 THEN p95\n  WHEN mag < p05 THEN p05\n  ELSE mag\nEND AS mag_winsorized\n\ncan be replaced with a single line:\n\nLEAST(GREATEST(mag, p05), p95) AS mag_winsorized\n\nThe R equivalents of LEAST and GREATEST are pmin and pmax, respectively. And dbplyr will translate pmin and pmax for us, so we can get winsorized data as follows.\n\nearthquakes_wins <-\n  earthquakes |>\n  mutate(mag = if_else(mag %in% c(-9.99, -9), NA, mag)) |>\n  filter(!is.na(mag)) |>\n  cross_join(extremes) |>\n  mutate(mag_winsorized = pmin(pmax(mag, p05), p95)) |>\n  select(time, place, mag, mag_winsorized) \n\nearthquakes_wins |>\n  arrange(desc(mag)) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\n\n\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2011-03-11 05:46:24\n2011 Great Tohoku Earthquake, Japan\n9.1\n4.5\n\n\n2010-02-27 06:34:11\noffshore Bio-Bio, Chile\n8.8\n4.5\n\n\n2012-04-11 08:38:36\noff the west coast of northern Sumatra\n8.6\n4.5\n\n\n\n\nearthquakes_wins |>\n  filter(mag == mag_winsorized) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\n\n\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2015-12-04 15:23:52\n159km NNE of Cape Yakataga, Alaska\n1.2\n1.2\n\n\n2015-12-04 15:26:03\n87km WSW of Atka, Alaska\n2.0\n2.0\n\n\n2015-12-04 15:32:17\n112km N of Larsen Bay, Alaska\n2.7\n2.7\n\n\n\n\nearthquakes_wins |>\n  arrange(mag) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\n\n\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2016-06-20 22:16:25\n20 km SSW of Pa‘auilo, Hawaii\n-5.0\n0.12\n\n\n2012-06-11 01:59:01\nNevada\n-2.6\n0.12\n\n\n2012-06-27 01:14:37\nNevada\n-2.6\n0.12\n\n\n\n\n\n\n\n6.3.4 Rescaling\nIn the book, it says WHERE depth >= 0.05, but I need to use WHERE depth > 0.05 to match the results there.\n\nquake_depths <-\n  earthquakes |>\n  filter(depth > 0.05) |>\n  mutate(depth = round(depth, 1)) |>\n  select(depth)\n\nquake_depths |>\n  mutate(log_depth = log(depth, base = 10)) |>\n  count(depth, log_depth) |>\n  arrange(depth) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\ndepth\nlog_depth\nn\n\n\n\n\n0.1\n-1.0000000\n6994\n\n\n0.2\n-0.6989700\n6876\n\n\n0.3\n-0.5228787\n7269\n\n\n\n\nquake_depths |>\n  ggplot(aes(x = depth)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\nquake_depths |>\n  ggplot(aes(x = log(depth, base = 10))) +\n  geom_histogram(binwidth = 0.1)"
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "7  Experiment Analysis",
    "section": "",
    "text": "I would reframe this discussion. We might be using a database because the results of an experiment are stored there.\nAn alternative approach might have the results of an experiment are extracted and exported to a CSV or Excel file and attached to an email and sent to you for analysis.1 But extracted from where? If the data are in a database, it would be better to cut out the middleman and just get the data directly.\nThe limitations of SQL essentially vanish when we access the data using dbplyr. R can do any statistical calculation we can think of, so we have no need of an online statistical calculator (though such calculators can help us frame our analyses and check our results).\nCathy does say that “many databases allow developers to extend SQL functionality with user-defined functions (UDFs) … but they are beyond the scope of this book.” For PostgreSQL there are PL/Python and PL/R, which allow creation of functions in Python and R, respectively. When I first started to use PostgreSQL, these extensions seemed pretty exciting and, because I was maintaining my own databases, I could use them. But over time, I found maintaining UDFs to be more effort than could be justified and I no longer use them. Instead, if I need to do analysis in Python or R, I will extract data from the database, do the analysis, then write data back to the database. While this likely partly reflects the kinds of analysis I do, I think that UDFs are likely to be off limits to most users because of the additional complexity of turning code into UDFs and because many users would lack sufficient database privileges to create these.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\nlibrary(flextable)\nlibrary(janitor)"
  },
  {
    "objectID": "experiments.html#the-data-set",
    "href": "experiments.html#the-data-set",
    "title": "7  Experiment Analysis",
    "section": "7.2 The Data Set",
    "text": "7.2 The Data Set\nAs discussed in Tanimura (2021), there are four tables used in this chapter. We can get these data from the GitHub site associated with Tanimura (2021).2\n\ndb <- dbConnect(duckdb::duckdb())\n\ngame_users <-\n  tbl(db, \"read_csv_auto('data/game_users.csv')\") |>\n  compute(name = \"game_users\")\ngame_actions <- \n  tbl(db, \"read_csv_auto('data/game_actions.csv')\") |>\n  compute(name = \"game_actions\")\ngame_purchases <- tbl(db, \"read_csv_auto('data/game_purchases.csv')\")\nexp_assignment <- tbl(db, \"read_csv_auto('data/exp_assignment.csv')\")"
  },
  {
    "objectID": "experiments.html#types-of-experiments",
    "href": "experiments.html#types-of-experiments",
    "title": "7  Experiment Analysis",
    "section": "7.3 Types of Experiments",
    "text": "7.3 Types of Experiments\nI would reword the first paragraph here to the following for clarity (edits in italics):\n\nThere is a wide range of experiments, If you can change something that a user, customer, constituent, or other entity experiences, you can in theory test the effect of that change on some outcome.\n\n\n7.3.1 Experiments with Binary Outcomes: The Chi-Squared Test\nTo better match the approach of the book, I essentially create the contingency table in the database. An alternative approach would have been to collect() after summarize() and then do the statistical analysis in R. In fact, many of the functions in R are better set-up for this approach. However, this means bring more data into R and doing more calculation in R. If the experiment is very large or you would rather the database server do more of the work, then the approach below may be preferred.\n\ncont_tbl <-\n  exp_assignment |>\n  left_join(game_actions, by = \"user_id\") |>\n  group_by(variant, user_id) |>\n  summarize(completed = \n              coalesce(any(action == \"onboarding complete\", na.rm = TRUE),\n                       FALSE),\n            .groups = \"drop\") |>\n  count(variant, completed) |>\n  mutate(completed = if_else(completed, \"Yes\", \"No\")) |>\n  pivot_wider(names_from = completed, values_from = n) |>\n  collect()\n\nUsing the packages janitor and flextable, I mimic the nicely formatted output shown in the book:\n\ncont_tbl |>\n  adorn_totals(c(\"row\", \"col\")) |>\n  mutate(`% Complete` = prettyNum(Yes/Total * 100, digits = 4)) |>\n  flextable() |>\n  add_header_row(values=c(\"\", \"Completed onboarding\", \"\"),\n                 colwidths = c(1, 2, 2))\n\n\nCompleted onboardingvariantYesNoTotal% Completevariant 138,28011,99550,27576.14control36,26813,62949,89772.69Total74,54825,624100,17274.42\n\n\nNow I can do the Chi-squared test. I need to turn variant into the row names of the contingency table so that we want a simple \\(2 \\times 2\\) numeric table as the input to our statistical test and I use column_to_rownames() to this end. I then pipe the result into the base R function chisq.test(). I specified correct = FALSE so that my result matched what I got from the online calculator I found. I then display the Chi-squared statistic and the \\(p\\)-value.\n\nres <- \n  cont_tbl |>\n  column_to_rownames(var = \"variant\") |>\n  chisq.test(correct = FALSE)\n\nres$statistic\n\nX-squared \n 157.0758 \n\nres$p.value\n\n[1] 4.926953e-36\n\n\n\n\n7.3.2 Experiments with Continuous Outcomes: The t-Test\n\namounts <- \n  exp_assignment |>\n  filter(exp_name == 'Onboarding') |>\n  left_join(game_purchases, by = \"user_id\") |>\n  group_by(variant, user_id) |>\n  summarize(amount = sum(coalesce(amount, 0), na.rm = TRUE),\n            .groups = \"drop\")\n\nt_test_stats <-\n  amounts |>\n  group_by(variant) |>\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) |>\n  collect()\n\nt_test_stats |>\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n50275\n3.688\n19.22\n\n\ncontrol\n49897\n3.781\n18.94\n\n\n\n\n\nWe can make a small function that we can pass a data frame to as df. The calculations assume that df contains two rows (one for each group) and columns named mean, sd, and n for the mean, standard deviation, and number of observations, respectively, in each group.\n\nt_test <- function(df) {\n  mean_diff = abs(df$mean[1] - df$mean[2])\n  se_diff <- sqrt(sum(df$sd^2 / df$n))\n  t_stat <- mean_diff / se_diff\n  p <- pt(t_stat, df = sum(df$n))\n  p_val <- 2 * min(p, 1 - p)\n  return(list(\"statistic\" = t_stat, \"p-value\" = p_val))\n}\n\nt_test(t_test_stats)\n\n$statistic\n[1] 0.7765458\n\n$`p-value`\n[1] 0.4374286\n\n\nThese values line up with those obtained from the online calculator I found.\nAn alternative approach would be to collect() the underlying data and do the \\(t\\)-test in R.\n\nt_test_data <-\n  amounts |>\n  select(variant, amount) |>\n  collect()\n\nt.test(formula = amount ~ variant, data = t_test_data)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by variant\nt = 0.77655, df = 100165, p-value = 0.4374\nalternative hypothesis: true difference in means between group control and group variant 1 is not equal to 0\n95 percent confidence interval:\n -0.1426893  0.3299478\nsample estimates:\n  mean in group control mean in group variant 1 \n               3.781218                3.687589 \n\n\n\nt_test_stats_2 <-\n  amounts |>\n  inner_join(game_actions, by = \"user_id\") |>\n  filter(action == \"onboarding complete\") |>\n  group_by(variant) |>\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) |>\n  collect()\n\nt_test_stats_2 |>\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n38280\n4.843\n21.899\n\n\ncontrol\n36268\n5.202\n22.049\n\n\n\n\n\n\nt_test(t_test_stats_2)\n\n$statistic\n[1] 2.229649\n\n$`p-value`\n[1] 0.02577371"
  },
  {
    "objectID": "experiments.html#challenges-with-experiments-and-options-for-rescuing-flawed-experiments",
    "href": "experiments.html#challenges-with-experiments-and-options-for-rescuing-flawed-experiments",
    "title": "7  Experiment Analysis",
    "section": "7.4 Challenges with Experiments and Options for Rescuing Flawed Experiments",
    "text": "7.4 Challenges with Experiments and Options for Rescuing Flawed Experiments\n\n7.4.1 Variant Assignment\n\n\n7.4.2 Outliers\n\nexp_assignment |>\n  left_join(game_purchases, by = \"user_id\", keep = TRUE,\n            suffix = c(\"\", \".y\")) |>\n  inner_join(game_actions, by = \"user_id\") |>\n  filter(action == \"onboarding complete\",\n         exp_name == 'Onboarding') |>\n  group_by(variant) |>\n  summarize(total_cohorted = n_distinct(user_id),\n            purchasers = n_distinct(user_id.y),\n            .groups = \"drop\") |>\n  mutate(pct_purchased = purchasers * 100.0 / total_cohorted) |>\n  kable(digits = 2)\n\n\n\n\nvariant\ntotal_cohorted\npurchasers\npct_purchased\n\n\n\n\ncontrol\n36268\n4988\n13.75\n\n\nvariant 1\n38280\n4981\n13.01\n\n\n\n\n\n\n\n7.4.3 Time Boxing\n\namounts_boxed <- \n  exp_assignment |>\n  filter(exp_name == 'Onboarding') |>\n  mutate(exp_end = exp_date + days(7)) |>\n  left_join(game_purchases, \n            by = join_by(user_id, exp_end >= purch_date)) |>\n  group_by(variant, user_id) |>\n  summarize(amount = sum(coalesce(amount, 0), na.rm = TRUE),\n            .groups = \"drop\")\n\n\nt_test_stats_boxed <-\n  amounts_boxed |>\n  group_by(variant) |>\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) |>\n  collect()\n\nt_test_stats_boxed |>\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n50275\n1.352\n5.613\n\n\ncontrol\n49897\n1.369\n5.766\n\n\n\n\n\n\nt_test(t_test_stats_boxed)\n\n$statistic\n[1] 0.4920715\n\n$`p-value`\n[1] 0.6226699\n\n\n\n\n7.4.4 Pre-Post Analysis\nThe original SQL query is something like this, but we make some tweaks to get this to work more naturally using dbplyr.\n\nSELECT \n  CASE WHEN a.created BETWEEN '2020-01-13' AND '2020-01-26' THEN 'pre'\n     WHEN a.created BETWEEN '2020-01-27' AND '2020-02-09' THEN 'post'\n     END AS variant,\n  count(distinct a.user_id) AS cohorted,\n  count(distinct b.user_id) AS opted_in,\n  count(distinct b.user_id) * 1.0 / count(DISTINCT a.user_id) AS pct_optin,\n  count(distinct a.created) AS days\nFROM game_users a\nLEFT JOIN game_actions b ON a.user_id = b.user_id \n  AND b.action = 'email_optin'\nWHERE a.created BETWEEN '2020-01-13' AND '2020-02-09'\nGROUP BY 1\n;\n\nThe first tweak is to translate the b.action = 'email_optin' part of the join into a filter, which we embed in the left_join() portion of our code (mirroring how it works in the SQL). The second tweak is to move the pct_optin out of the grouped aggregation portion of the query, as in the SQL it is referring to what we will later have as opted_in and cohorted. (In fact, I don’t bother with `pct_optin at all, as it’s easy to calculate when we use it in making a table.)\nNote that COUNT(DISTINCT col) becomes n_distinct(col). We use keep = TRUE and suffix = c(\"\", \".y\") to store what is b.user_id in the SQL as user_id.y.\nWe calculate not_opted_in so that our table is better prepared for the chisq.test() we will pass it to later on.\n\nopt_in_df <-\n  game_users |>\n  filter(between(created, \n                 as.Date('2020-01-13'), \n                 as.Date('2020-02-09'))) |>\n  left_join(game_actions |> \n              filter(action == 'email_optin'), by = \"user_id\",\n            keep = TRUE,\n            suffix = c(\"\", \".y\")) |>\n  mutate(variant = if_else(created <= '2020-01-26', 'pre', 'post')) |>\n  group_by(variant) |>\n  summarize(cohorted = n_distinct(user_id),\n            opted_in = n_distinct(user_id.y),\n            days = n_distinct(created),\n            .groups = \"drop\") |>\n  mutate(not_opted_in = cohorted - opted_in) |>\n  collect()\n\n\nopt_in_df |>\n  select(variant, days, opted_in, not_opted_in, cohorted) |>\n  adorn_totals(c(\"row\")) |>\n  mutate(`% opted in` = prettyNum(100.0 * opted_in/cohorted, digits = 4)) |>\n  rename(Yes = opted_in, No = not_opted_in, Total = cohorted) |>\n  flextable() |>\n  add_header_row(values=c(\"\", \"Opted in\", \"\"),\n                 colwidths = c(2, 2, 2))\n\n\nOpted invariantdaysYesNoTotal% opted inpre1414,48910,17324,66258.75post1411,22016,39727,61740.63Total2825,70926,57052,27949.18\n\n\n\nres <- \n  opt_in_df |> \n  select(-cohorted, -days) |>\n  column_to_rownames(var = \"variant\") |>\n  chisq.test(correct = FALSE)\n\nres$statistic\n\nX-squared \n 1712.075 \n\nres$p.value\n\n[1] 0\n\n\n\n\n7.4.5 Natural experiment analysis\n\nby_country <- \n  game_users |>\n  filter(country %in% c('United States', 'Canada')) |>\n  left_join(game_purchases, by = \"user_id\",\n            keep = TRUE,\n            suffix = c(\"\", \".y\")) |>\n  group_by(country) |>\n  summarize(cohorted = n_distinct(user_id),\n            opted_in = n_distinct(user_id.y),\n            .groups = \"drop\") |>\n  mutate(pct_purchased = 1.0 * opted_in/cohorted) |>\n  collect()\n\nby_country |> kable()\n\n\n\n\ncountry\ncohorted\nopted_in\npct_purchased\n\n\n\n\nUnited States\n45012\n4958\n0.1101484\n\n\nCanada\n20179\n5011\n0.2483275\n\n\n\n\n\n\nres <- \n  by_country |> \n  select(-pct_purchased) |>\n  column_to_rownames(var = \"country\") |>\n  chisq.test(correct = FALSE)\n\nres$statistic\n\nX-squared \n 1447.273 \n\nres$p.value\n\n[1] 1.122178e-316\n\n\nTo avoid a warning, we disconnect from the database once we’re done with it.\n\ndbDisconnect(db, shutdown = TRUE)\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "complex-data.html",
    "href": "complex-data.html",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "",
    "text": "library(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)"
  },
  {
    "objectID": "complex-data.html#code-organization",
    "href": "complex-data.html#code-organization",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.2 Code Organization",
    "text": "8.2 Code Organization\n\ndb <- dbConnect(duckdb::duckdb())\n\nearthquakes <- tbl(db, \"read_csv_auto('data/earthquakes*.csv')\")\n\nlegislators_terms <- \n  tbl(db, \"read_csv_auto('data/legislators_terms.csv')\") |>\n  compute(name = \"legislators_terms\")\n\nvideogame_sales <- \n  tbl(db, \"read_csv_auto('data/videogame_sales.csv')\") |>\n  compute(name = \"videogame_sales\")\n\n\nearthquakes |>\n  filter(date_part('year', time) >= 2019,\n         between(mag, 0, 1)) |>\n  mutate(place = case_when(grepl('CA', place) ~ 'California',\n                           grepl('AK', place) ~ 'Alaska',\n                           TRUE ~ trim(split_part(place, ',', 2L)))) |>\n  count(place, type, mag) |>\n  arrange(desc(n)) |>\n  collect(n = 10) |>\n  kable()\n\n\n\n\nplace\ntype\nmag\nn\n\n\n\n\nAlaska\nearthquake\n1.00\n4824\n\n\nAlaska\nearthquake\n0.90\n4153\n\n\nAlaska\nearthquake\n0.80\n3219\n\n\nCalifornia\nearthquake\n0.56\n2631\n\n\nAlaska\nearthquake\n0.70\n2061\n\n\nNevada\nearthquake\n1.00\n1688\n\n\nNevada\nearthquake\n0.80\n1681\n\n\nNevada\nearthquake\n0.90\n1669\n\n\nAlaska\nearthquake\n0.60\n1464\n\n\nCalifornia\nearthquake\n0.55\n1444\n\n\n\n\n\n\n8.2.1 Commenting\n\n\n8.2.2 Capitalization, Indentation, Parentheses, and Other Formatting Tricks\n\n\n8.2.3 Storing Code"
  },
  {
    "objectID": "complex-data.html#organizing-computations",
    "href": "complex-data.html#organizing-computations",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.3 Organizing Computations",
    "text": "8.3 Organizing Computations\n\n8.3.1 Understanding Order of SQL Clause Evaluation\nIn general, I think the order of evaluation is more intuitive when writing SQL using dbplyr. It is perhaps more confusing to someone who has written a lot of SQL without thinking about the order things are evaluated (e.g., WHERE comes early in evaluation, but relatively late in the SQL).\nIn dplyr the meaning of filter() is usually clear by where it is placed. Some times it is translated into WHERE and some times it is HAVING. In the example below, a WHERE-like filter would be placed just after legislators_terms (much as it is evaluated by the SQL query engine), while in this case we have filter being translated into HAVING because it is using the result of GROUP BY query. One hardly need give this much thought.\n\nterms_by_states <-\n  legislators_terms |>\n  group_by(state) |>\n  summarize(terms = n()) |>\n  filter(terms >= 1000) |>\n  arrange(desc(terms))\n\nterms_by_states |>\n  show_query()\n\n<SQL>\nSELECT state, COUNT(*) AS terms\nFROM legislators_terms\nGROUP BY state\nHAVING (COUNT(*) >= 1000.0)\nORDER BY terms DESC\n\nterms_by_states |>\n  collect(n = 10) |>\n  kable()\n\n\n\n\nstate\nterms\n\n\n\n\nNY\n4159\n\n\nPA\n3252\n\n\nOH\n2239\n\n\nCA\n2121\n\n\nIL\n2011\n\n\nTX\n1692\n\n\nMA\n1667\n\n\nVA\n1648\n\n\nNC\n1351\n\n\nMI\n1284\n\n\n\n\n\nThe way dplyr code is written makes it easy to look at the output each step in the series of pipes. In the query below, we can easily highlight the code up to the end of any pipe and evaluate it to see if it is doing what we want and expect it be doing. In this specific case, I find the dplyr code to be more intuitive than the SQL provided in the book, which uses an `a\n\nlegislators_terms |>\n  group_by(state) |>\n  summarize(terms = n(), .groups = \"drop\") |>\n  mutate(avg_terms = mean(terms, na.rm = TRUE)) |>\n  collect(n = 10) |>\n  kable(digits = 2)\n\n\n\n\nstate\nterms\navg_terms\n\n\n\n\nOH\n2239\n746.83\n\n\nWA\n489\n746.83\n\n\nMD\n961\n746.83\n\n\nDE\n227\n746.83\n\n\nPA\n3252\n746.83\n\n\nCA\n2121\n746.83\n\n\nID\n188\n746.83\n\n\nMN\n687\n746.83\n\n\nNJ\n1255\n746.83\n\n\nVT\n379\n746.83\n\n\n\n\n\n\nlegislators_terms |>\n  group_by(state) |>\n  summarize(terms = n(), .groups = \"drop\") |>\n  window_order(desc(terms)) |>\n  mutate(rank = row_number()) |>\n  arrange(rank) |>\n  collect(n = 10) |>\n  kable()\n\n\n\n\nstate\nterms\nrank\n\n\n\n\nNY\n4159\n1\n\n\nPA\n3252\n2\n\n\nOH\n2239\n3\n\n\nCA\n2121\n4\n\n\nIL\n2011\n5\n\n\nTX\n1692\n6\n\n\nMA\n1667\n7\n\n\nVA\n1648\n8\n\n\nNC\n1351\n9\n\n\nMI\n1284\n10\n\n\n\n\n\n\n\n8.3.2 Subqueries\nIn writing dbplyr code, it is more natural to think in terms of CTEs, even though the code you write will generally be translated into SQL using subqueries.\nThe query in the book written with LATERAL seems much more confusing to me than the following. (Also, adding EXPLAIN to each query suggests that LATERAL is more complicated for PostgreSQL.) I rewrote the LATERAL query using CTEs and got the following, which seems closer to the second SQL query included in the book.\n\nWITH \n\ncurrent_legislators AS (\n  SELECT distinct id_bioguide, party\n  FROM legislators_terms\n  WHERE term_end > '2020-06-01'),\n  \nparty_changers AS (\n  SELECT b.id_bioguide, min(term_start) as first_term\n  FROM legislators_terms b\n  INNER JOIN current_legislators AS a\n  ON b.id_bioguide = a.id_bioguide AND b.party <> a.party\n  GROUP BY 1)\n\nSELECT date_part('year', first_term) as first_year, party,\n  count(id_bioguide) as legislators\nFROM current_legislators\nINNER JOIN party_changers\nUSING (id_bioguide)\nGROUP BY 1, 2;\n\n\n3 records\n\n\nfirst_year\nparty\nlegislators\n\n\n\n\n2011\nLibertarian\n1\n\n\n1979\nRepublican\n1\n\n\n2015\nDemocrat\n1\n\n\n\n\n\nTranslating the CTE version into dbplyr is a piece of cake.\n\ncurrent_legislators <-\n  legislators_terms |>\n  filter(term_end > '2020-06-01') |>\n  distinct(id_bioguide, party)\n\nparty_changers <-\n  legislators_terms |>\n  inner_join(current_legislators, \n             join_by(id_bioguide)) |>\n  filter(party.x != party.y) |>\n  group_by(id_bioguide) |>\n  summarize(first_term = min(term_start, na.rm = TRUE), .groups = \"drop\")\n\ncurrent_legislators |>\n  inner_join(party_changers, by = \"id_bioguide\") |>\n  mutate(first_year = date_part('year', first_term)) |>\n  group_by(first_year, party) |>\n  summarize(legislators = n(), .groups = \"drop\") |>\n  collect() |>\n  kable()\n\n\n\n\nfirst_year\nparty\nlegislators\n\n\n\n\n2011\nLibertarian\n1\n\n\n1979\nRepublican\n1\n\n\n2015\nDemocrat\n1\n\n\n\n\n\n\n\n8.3.3 Temporary Tables\nCreating temporary tables with dbplyr is easy: simply append compute() at the end of the table definition. Generally, dbplyr will take care of details that likely do not matter much, such as choosing a name for the table (we don’t care because we can refer to the table below as current_legislators regardless of the name chosen for it).\n\ncurrent_legislators |>\n  show_query()\n\n<SQL>\nSELECT DISTINCT id_bioguide, party\nFROM legislators_terms\nWHERE (term_end > '2020-06-01')\n\ncurrent_legislators <-\n  legislators_terms |>\n  filter(term_end > '2020-06-01') |>\n  distinct(id_bioguide, party) |>\n  compute()\n\ncurrent_legislators |>\n  show_query()\n\n<SQL>\nSELECT *\nFROM dbplyr_001\n\n\nCreating temporary tables can lead to significant performance gains in some situations, as the query optimizer has a simpler object to work with. Note that dbplyr allows the creating of an index with temporary tables, which can improve performance even further.\nNot that some database administrators do not allow users to create temporary tables. In such cases, you can often use collect() followed by copy_inline() effectively. (This is also useful when you have data outside the database—so collect() is not relevant—but want to merge it with data in the database.1)\n\n\n8.3.4 Common Table Expressions\nWe have been using them throughout the book already. For the sake of completeness, I rewrite the query given in the book here.\n\nfirst_term <- \n  legislators_terms |>\n  group_by(id_bioguide) |>\n  summarize(first_term = min(term_start, na.rm = TRUE),\n            .groups = \"drop\")\n    \nfirst_term |>\n  inner_join(legislators_terms, by = \"id_bioguide\") |>\n  mutate(periods = date_part('year', age(term_start, first_term))) |>\n  group_by(periods) |>\n  summarize(cohort_retained = n_distinct(id_bioguide)) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\nperiods\ncohort_retained\n\n\n\n\n21\n280\n\n\n3\n1831\n\n\n6\n2385\n\n\n\n\n\n\n\n8.3.5 GROUPING SETS\nI don’t think there is a “pure” dbplyr way of doing these. However, not all is lost for the dedicated dbplyr user, as the following examples demonstrate. Note that DuckDB requires a more explicit statement of the GROUPING SETS.\n\nglobal_sales <-\n  tbl(db, sql(\"\n    (SELECT platform, genre, publisher,\n      sum(global_sales) as global_sales\n    FROM videogame_sales\n    GROUP BY GROUPING SETS ((platform, genre, publisher), \n                             platform, genre, publisher))\"))\n\nglobal_sales |>\n  arrange(desc(global_sales)) |>\n  collect(n = 10) |>\n  kable()\n\n\n\n\nplatform\ngenre\npublisher\nglobal_sales\n\n\n\n\nNA\nNA\nNintendo\n1786.56\n\n\nNA\nAction\nNA\n1751.18\n\n\nNA\nSports\nNA\n1330.93\n\n\nPS2\nNA\nNA\n1255.64\n\n\nNA\nNA\nElectronic Arts\n1110.32\n\n\nNA\nShooter\nNA\n1037.37\n\n\nX360\nNA\nNA\n979.96\n\n\nPS3\nNA\nNA\n957.84\n\n\nNA\nRole-Playing\nNA\n927.37\n\n\nWii\nNA\nNA\n926.71\n\n\n\n\n\n\nglobal_sales_cube <-\n  tbl(db, sql(\"\n    (SELECT coalesce(platform, 'All') as platform,\n      coalesce(genre,'All') AS genre,\n      coalesce(publisher,'All') AS publisher,\n      sum(global_sales) AS global_sales\n    FROM videogame_sales\n    GROUP BY cube (platform, genre, publisher))\"))\n\nglobal_sales_cube |>\n  arrange(platform, genre, publisher) |>\n  collect(n = 10) |>\n  kable()\n\n\n\n\nplatform\ngenre\npublisher\nglobal_sales\n\n\n\n\n2600\nAction\n20th Century Fox Video Games\n1.72\n\n\n2600\nAction\nActivision\n4.64\n\n\n2600\nAction\nAll\n29.34\n\n\n2600\nAction\nAnswer Software\n0.50\n\n\n2600\nAction\nAtari\n7.68\n\n\n2600\nAction\nAvalon Interactive\n0.17\n\n\n2600\nAction\nBomb\n0.22\n\n\n2600\nAction\nCBS Electronics\n0.31\n\n\n2600\nAction\nCPG Products\n0.54\n\n\n2600\nAction\nColeco\n1.26"
  },
  {
    "objectID": "complex-data.html#managing-data-set-size-and-privacy-concerns",
    "href": "complex-data.html#managing-data-set-size-and-privacy-concerns",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.4 Managing Data Set Size and Privacy Concerns",
    "text": "8.4 Managing Data Set Size and Privacy Concerns\n\n8.4.1 Sampling with %, mod\nOne can also use random() to similar effect.\n\n\n8.4.2 Reducing Dimensionality\n\nlegislators_terms |>\n  mutate(state_group = \n           case_when(state %in% c('CA', 'TX', 'FL', 'NY', 'PA') ~ state, \n                     TRUE ~ 'Other')) |>\n  group_by(state_group) |>\n  summarize(terms = n()) |>\n  arrange(desc(terms)) |>\n  collect(n = 6) |>\n  kable()\n\n\n\n\nstate_group\nterms\n\n\n\n\nOther\n31980\n\n\nNY\n4159\n\n\nPA\n3252\n\n\nCA\n2121\n\n\nTX\n1692\n\n\nFL\n859\n\n\n\n\n\n\ntop_states <-\n  legislators_terms |>\n  group_by(state) |>\n  summarize(n_reps = n_distinct(id_bioguide), .groups = \"drop\") |>\n  window_order(desc(n_reps)) |>\n  mutate(rank = row_number())\n\nlegislators_terms |>\n  inner_join(top_states, by = \"state\") |>\n  mutate(state_group = case_when(rank <= 5 ~ state,\n                                 TRUE ~ 'Other')) |>\n  group_by(state_group) |>\n  summarize(terms = n_distinct(id_bioguide)) |>\n  arrange(desc(terms)) |>\n  collect(n = 6) |>\n  kable()\n\n\n\n\nstate_group\nterms\n\n\n\n\nOther\n8317\n\n\nNY\n1494\n\n\nPA\n1075\n\n\nOH\n694\n\n\nIL\n509\n\n\nVA\n451\n\n\n\n\n\nNote that the CASE WHEN in the SQL in the book can be significantly simplified.\n\nWITH num_terms AS (\n  SELECT id_bioguide, count(term_id) as terms\n    FROM legislators_terms\n    GROUP BY 1)\n    \nSELECT terms >= 2 AS two_terms_flag,\n  count(*) as legislators\nFROM num_terms\nGROUP BY 1;\n\n\n2 records\n\n\ntwo_terms_flag\nlegislators\n\n\n\n\nTRUE\n8379\n\n\nFALSE\n4139\n\n\n\n\n\n\nnum_terms <- \n  legislators_terms |>\n  group_by(id_bioguide) |>\n  summarize(terms = n(), .groups = \"drop\")\n\nnum_terms |>\n  mutate(two_terms_flag = terms >= 2) |>\n  count(two_terms_flag) |>\n  collect() |>\n  kable()\n\n\n\n\ntwo_terms_flag\nn\n\n\n\n\nTRUE\n8379\n\n\nFALSE\n4139\n\n\n\n\n\nNow we can reuse the num_terms lazy table we created above.\n\nnum_terms |>\n  mutate(terms_level = case_when(terms >= 10 ~ '10+',\n                                 terms >= 2 ~ '2 - 9',\n                                 TRUE ~ '1')) |>\n  count(terms_level) |>\n  collect() |>\n  kable()\n\n\n\n\nterms_level\nn\n\n\n\n\n2 - 9\n7496\n\n\n10+\n883\n\n\n1\n4139\n\n\n\n\n\n\n\n8.4.3 PII and Data Privacy"
  },
  {
    "objectID": "complex-data.html#conclusion",
    "href": "complex-data.html#conclusion",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.5 Conclusion",
    "text": "8.5 Conclusion"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "9  Conclusion",
    "section": "",
    "text": "library(tidyverse)\nlibrary(DBI)\nlibrary(knitr)\nlibrary(dbplyr)"
  },
  {
    "objectID": "conclusion.html#funnel-analysis",
    "href": "conclusion.html#funnel-analysis",
    "title": "9  Conclusion",
    "section": "9.1 Funnel Analysis",
    "text": "9.1 Funnel Analysis\nThe issue described in this section of Tanimura (2021) is more difficult to get to using dbplyr. This seems the natural translation of the ideas represented in the SQL there.\n\nusers |>\n  left_join(step_one, by = \"user_id\",\n            keep = TRUE, suffix = c(\"\", \"_1\")) |>\n  left_join(step_two, by = \"user_id\",\n            keep = TRUE, suffix = c(\"\", \"_2\")) |>\n  summarize(all_users = n_distinct(user_id),\n            step_one_users = n_distinct(user_id_1),\n            step_two_users = n_distinct(user_id_2)) |>\n  mutate(pct_step_one = step_one_users/all_users,\n         pct_one_to_two = step_two_users / step_one_users)\n\nThis yields the second of the two options provided, which seems to be the implicitly preferred one if we want to capture all step_two_users—even if some customers go directly to step_two—though it does make pct_one_to_two a bit more difficult to interpret in such cases."
  },
  {
    "objectID": "conclusion.html#churn-lapse-and-other-definitions-of-departure",
    "href": "conclusion.html#churn-lapse-and-other-definitions-of-departure",
    "title": "9  Conclusion",
    "section": "9.2 Churn, Lapse, and Other Definitions of Departure",
    "text": "9.2 Churn, Lapse, and Other Definitions of Departure\nThis query seems easier to follow with a separate window specification (WINDOW w AS) and using a CTE. Because avg(INTERVAL) is not defined in DuckDB, I use the epoch() function to convert the interval to seconds.1\n\nWITH gap_intervals AS (\n  SELECT id_bioguide, term_start,\n    lag(term_start) OVER w AS prev,\n    age(term_start, lag(term_start) OVER w) as gap_interval\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  WINDOW w AS (partition BY id_bioguide \n               ORDER BY term_start))\n\nSELECT avg(epoch(gap_interval)) AS avg_gap\nFROM gap_intervals\nWHERE gap_interval IS NOT NULL;\n\n\n1 records\n\n\navg_gap\n\n\n\n\n69709258\n\n\n\n\n\nTranslating this query into dplyr is straightforward, with again most of the work being done in creation of gap_intervals. Note that we can refer to prev created in an earlier part of the mutate call in creating gap_interval. It seems that dbplyr detects this kind of situation and cleverly puts the calculation of prev into a subquery.\n\ngap_intervals <-\n  legislators_terms |>\n  filter(term_type == 'rep') |>\n  group_by(id_bioguide) |>\n  window_order(term_start) |>\n  mutate(prev = lag(term_start),\n         gap_interval = age(term_start, prev)) |>\n  ungroup() |>\n  select(id_bioguide, term_start, prev, gap_interval)\n\nThis makes it effectively a single line of code to transform the data into the average shown above. Note that, after collect(), we use as.duration() from the lubridate package to transform the result from a number of seconds back into a duration.2 The duration type from lubridate provides a user-friendly print method that is easier for humans to interpret.\n\ngap_intervals |>\n  summarize(avg_gap = mean(epoch(gap_interval))) |>\n  collect() |>\n  mutate(avg_gap = as.duration(avg_gap)) |>\n  kable()\n\n\n\n\navg_gap\n\n\n\n\n69709258.3546315s (~2.21 years)\n\n\n\n\n\nWith gap_intervals in our pocket, the following query is much simpler than the SQL shown in the book.\n\ngap_months <-\n  gap_intervals |>\n  mutate(gap_months = year(gap_interval) * 12 + month(gap_interval)) |>\n  count(gap_months, name = \"instances\") |>\n  arrange(gap_months) |>\n  ungroup()\n\ngap_months |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\ngap_months\ninstances\n\n\n\n\nNA\n11236\n\n\n1\n25\n\n\n2\n4\n\n\n\n\n\nThe following plot better matches what is shown in the book, where it seems the caption is incorrect.\n\ngap_months |>\n  filter(between(gap_months, 14, 49)) |>\n  ggplot(aes(x = gap_months, y = instances)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nlatest_date <- \n  legislators_terms |> \n  summarize(max = max(term_start)) |>\n  pull()\n\nintervals <-\n  legislators_terms |>\n  filter(term_type == 'rep') |>\n  group_by(id_bioguide) |>\n  summarize(max_date = max(term_start), .groups = \"drop\") |>\n  mutate(interval_since_last = age(latest_date, max_date))\n\nintervals |>\n  mutate(years_since_last = year(interval_since_last)) |>\n  count(years_since_last) |>\n  arrange(years_since_last) |>\n  collect(n = 3) |>\n  kable()\n\n\n\n\nyears_since_last\nn\n\n\n\n\n0\n6\n\n\n1\n440\n\n\n2\n1\n\n\n\n\n\n\nintervals |>\n  mutate(months_since_last = year(interval_since_last) * 12 +\n           month(interval_since_last)) |>\n  count(months_since_last, name = \"reps\") |>\n  mutate(status = case_when(months_since_last <= 23 ~ 'Current',\n                            months_since_last <= 48 ~ 'Lapsed',\n                            TRUE ~ 'Churned')) |>\n  group_by(status) |>\n  summarize(total_reps = sum(reps, na.rm = TRUE)) |>\n  kable()\n\n\n\n\nstatus\ntotal_reps\n\n\n\n\nChurned\n10685\n\n\nCurrent\n446\n\n\nLapsed\n105"
  },
  {
    "objectID": "conclusion.html#basket-analysis",
    "href": "conclusion.html#basket-analysis",
    "title": "9  Conclusion",
    "section": "9.3 Basket Analysis",
    "text": "9.3 Basket Analysis\nThis seems like a case where the programmability of R and the SQL-generation capability of dbplyr to address this more comprehensively. To make this tangible, let’s make some data and put it in the database:\n\npurchases <- tribble(\n  ~customer_id, ~product,\n  1, \"bananas\",\n  2, \"apples\",\n  2, \"oranges\",\n  3, \"apples\",\n  3, \"oranges\",\n  3, \"bananas\",\n  4, \"bananas\",\n  4, \"apples\",\n  4, \"oranges\",\n  4, \"passionfruit\") |>\n  mutate(customer_id = as.integer(customer_id)) |>\n  copy_inline(db, df = _)\n\nNow, let’s make a function that performs as much of the analysis in the database as possible. The only data we bring into R relates to the distinct number of products purchased by customers. So let’s say some customers purchase one product, others two, three, or four, while some customers purchase six products. In this case n_prods = c(1, 2, 3, 4, 6) and that is the only data we need in R from the database. If p = 2 (i.e., we’re interested in product pairs), we then create all the combinations of 2 products for each value in n_prods. For a customer with one product, there is no pair. For a customer with two products, there is one pair. For a customer with three products, there are two pairs: {1, 2} and {2, 3}. These combinations are created in R and the resulting table (combos) is passed back to the database.\nFrom there, everything is processed in the database.\nIn creating cust_prods, each unique product purchased by each customer is given a within-customer ID (prod_num) using the row_number() window function, and note is made of the number of unique products purchased by the customer (n_products). We then join cust_prods with combos using (customer_id, n_products) to create base, which contains customer IDs, as well as the “baskets” of products represented in the form of prod_num values stored in p_1, p_2, etc.\nNext, we want to add the p actual products in each basket from cust_prods to the data stored in base. For example, we want to turn p_1 of 1 into product_1 of \"apples\". The get_prod() function does this for one product at a time and we use Reduce from base R to repeatedly apply this idea p times.\nFinally, we collapse the product_i fields into a single basket field analogous to what is done in Tanimura (2021).\n\nget_baskets <- function(df, p) {\n  \n  get_combos <- function(n, p) {\n    if (n < p) return(NULL)\n    combos <- t(matrix(combn(sort(n), p), nrow = p))\n    colnames(combos) <- paste0(\"p_\", 1:p)\n    combos <- as_tibble(combos)\n    combos$row_num <- 1:nrow(combos)\n    cross_join(tibble(n_products = n), combos)\n  }\n\n  n_prods <- \n    df |>\n    distinct(customer_id, product) |>\n    group_by(customer_id) |>\n    summarize(n_products = n()) |>\n    select(n_products) |>\n    distinct() |>\n    pull()\n\n  combos <-\n    n_prods |> \n    lapply(get_combos, p = p) |>\n    bind_rows() |>\n    copy_inline(db, df = _)\n\n  cust_prods <-\n    df |> \n    group_by(customer_id) |> \n    window_order(product) |> \n    mutate(prod_num = row_number()) |>\n    ungroup() |>\n    group_by(customer_id) |> \n    mutate(n_products = n()) |>\n    ungroup()\n\n  base <- \n    cust_prods |>\n    select(customer_id, n_products) |>\n    distinct() |>\n    inner_join(combos, by = join_by(n_products)) |>\n    select(-n_products)\n\n  get_prod <- function(i) {\n    cust_prods |>\n      rename_with(function(x) paste0(\"p_\", i), \"prod_num\") |>\n      inner_join(base) |>\n      rename_with(function(x) paste0(\"product_\", i), \"product\") |>\n      select(customer_id, row_num, starts_with(\"product_\"))\n  }\n\n  Reduce(inner_join, lapply(1:p, get_prod)) |>\n    arrange(customer_id, row_num) |>\n    mutate(basket = \n             sql(paste0(\"concat(\", paste0(\"product_\", 1:p, collapse = \", ', ', \"), \")\"))) |>\n    compute() |>\n    select(customer_id, basket)\n}\n\nNow we can apply our function to our data to get two-item baskets.\n\npairs <- get_baskets(purchases, 2) \n\npairs |> kable()\n\n\n\n\ncustomer_id\nbasket\n\n\n\n\n2\napples, oranges\n\n\n3\napples, bananas\n\n\n3\napples, oranges\n\n\n3\nbananas, oranges\n\n\n4\napples, bananas\n\n\n4\napples, oranges\n\n\n4\napples, passionfruit\n\n\n4\nbananas, oranges\n\n\n4\nbananas, passionfruit\n\n\n4\noranges, passionfruit\n\n\n\n\n\nAnd we can identify the most popular baskets easily:\n\npairs |>\n  count(basket) |>\n  arrange(desc(n)) |>\n  kable()\n\n\n\n\nbasket\nn\n\n\n\n\napples, oranges\n3\n\n\napples, bananas\n2\n\n\nbananas, oranges\n2\n\n\napples, passionfruit\n1\n\n\nbananas, passionfruit\n1\n\n\noranges, passionfruit\n1\n\n\n\n\n\nBut the function also works for three-item baskets …\n\nget_baskets(purchases, 3) |> kable()\n\n\n\n\ncustomer_id\nbasket\n\n\n\n\n3\napples, bananas, oranges\n\n\n4\napples, bananas, oranges\n\n\n4\napples, bananas, passionfruit\n\n\n4\napples, oranges, passionfruit\n\n\n4\nbananas, oranges, passionfruit\n\n\n\n\n\n… and four-item baskets too.\n\nget_baskets(purchases, 4) |> kable()\n\n\n\n\ncustomer_id\nbasket\n\n\n\n\n4\napples, bananas, oranges, passionfruit"
  },
  {
    "objectID": "conclusion.html#resources",
    "href": "conclusion.html#resources",
    "title": "9  Conclusion",
    "section": "9.4 Resources",
    "text": "9.4 Resources"
  },
  {
    "objectID": "conclusion.html#final-thoughts",
    "href": "conclusion.html#final-thoughts",
    "title": "9  Conclusion",
    "section": "9.5 Final Thoughts",
    "text": "9.5 Final Thoughts\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "library(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\nlibrary(duckdb)"
  },
  {
    "objectID": "appendix.html#sec-dates",
    "href": "appendix.html#sec-dates",
    "title": "Appendix",
    "section": "Date, Datetime, and Time Manipulations",
    "text": "Date, Datetime, and Time Manipulations\n\nINSTALL 'icu';\nLOAD 'icu';\n\n\nTime Zone Conversions\nTanimura (2021) points out that often “timestamps in the database are not encoded with the time zone, and you will need to consult with the source or developer to figure out how your data was stored.” When pushing data to a PostgreSQL database, I use the timestamp with time zone type as much as possible.\nTanimura (2021) provides the following example, which is interesting because the west coast of the United States would not be on the PST time zone at that time of year. Instead, it would be on PDT.\n\nSELECT timestamptz '2020-09-01 00:00:00 -0' AT TIME ZONE 'PST' AS time;\n\n\n1 records\n\n\ntime\n\n\n\n\n2020-08-31 17:00:00\n\n\n\n\n\nIn PostgreSQL, we would get a different answer with the following query, but in DuckDB it seems that PDT is not recognized as a time zone abbreviation at all, so we just get the original UTC timestamp back.\n\nSELECT timestamptz '2020-09-01 00:00:00 -0' AT TIME ZONE 'PDT' AS time;\n\n\n1 records\n\n\ntime\n\n\n\n\n2020-09-01\n\n\n\n\n\nI think most people barely know the difference between PST and PDT and even fewer would know the exact dates that one switches from one to the other. A better approach is to use a time zone that encodes information about when PDT is used and when PST is used. In PostgreSQL and DuckDB, the function pg_timezone_names() returns a table with the information that we need.\nHowever, it seems that there are inconsistencies between PostgreSQL and DuckDB in terms of the abbreviations used. As such it’s probably safer to use the name form (e.g., US/Pacific) whenever possible. Given the widespread confusion about the meaning of terms like EST, EDT, and so on, just use US/Eastern, etc.\n\nSELECT name, abbrev\nFROM pg_timezone_names()\nWHERE regexp_matches(name, '^US/');\n-- use WHERE name ~ '^US/'; in PostgreSQL\n\n\nDisplaying records 1 - 10\n\n\nname\nabbrev\n\n\n\n\nUS/Alaska\nAST\n\n\nUS/Aleutian\nUS/Aleutian\n\n\nUS/Arizona\nPNT\n\n\nUS/Central\nCST\n\n\nUS/East-Indiana\nIET\n\n\nUS/Eastern\nUS/Eastern\n\n\nUS/Hawaii\nUS/Hawaii\n\n\nUS/Indiana-Starke\nUS/Indiana-Starke\n\n\nUS/Michigan\nUS/Michigan\n\n\nUS/Mountain\nNavajo\n\n\n\n\n\nThe following queries demonstrate that daylight savings information is encoded in the database.\n\nSELECT '2020-09-01 17:00:01 US/Pacific'::timestamptz AS t1,\n       '2020-09-02 10:00:01 Australia/Melbourne'::timestamptz  AS t2;\n\n\n1 records\n\n\nt1\nt2\n\n\n\n\n2020-09-02 00:00:01\n2020-09-02 00:00:01\n\n\n\n\n\n\nSELECT '2020-12-01 16:00:01 US/Pacific'::timestamptz AS t1,\n       '2020-12-02 11:00:01 Australia/Melbourne'::timestamptz  AS t2;\n\n\n1 records\n\n\nt1\nt2\n\n\n\n\n2020-12-02 00:00:01\n2020-12-02 00:00:01\n\n\n\n\n\n\nsql <-\n  \"(SELECT     \n    '2020-12-01 16:00:01 US/Pacific'::timestamptz AS t1)\"\n\na_time <- tbl(db, sql(sql))\na_time |>\n  kable()\n\n\n\n\nt1\n\n\n\n\n2020-12-02 00:00:01\n\n\n\n\n\n\na_time_r <-\n  a_time |>\n  select(t1) |>\n  pull()\n\nprint(a_time_r, tz = \"UTC\")\n\n[1] \"2020-12-02 00:00:01 UTC\"\n\nprint(a_time_r, tz = \"US/Pacific\")\n\n[1] \"2020-12-01 16:00:01 PST\"\n\nSys.timezone()\n\n[1] \"America/New_York\"\n\nprint(a_time_r, tz = Sys.timezone())\n\n[1] \"2020-12-01 19:00:01 EST\"\n\n\nThe above examples illustrate a few key ideas.\nFirst, while we supply the literal form '2020-09-01 17:00:01 US/Pacific'::timestamptz, it seems that once a variable has been encoded as TIMESTAMP WITH TIME ZONE, it behaves as though it is actually being stored as a timestamp in the UTC time zone, just with the displayed time perhaps being different.\nSecond, columns of type TIMESTAMP WITH TIME ZONE come into R with the associated time-zone information, which is what we want (especially if we will later put timestamp data back into PostgreSQL).\nThird, we can see that we can choose to display information in a different time zone without changing the underlying data.\nSome care is needed with timestamp data. I think the AT TIME ZONE queries provided in Tanimura (2021) are actually pretty dangerous, as can be seen in the following query. While we supply 2020-09-01 00:00:01 as UTC and then render it AT TIME ZONE 'US/Pacific', it turns out that the returned value is interpreted as a TIMESTAMP WITHOUT TIME ZONE and subsequent queries lead to confusing behaviour. In the query below, the second application of AT TIME ZONE interprets the TIMESTAMP WITHOUT TIME ZONE as though it came from the stated time zone and the results seem to have AT TIME ZONE doing the opposite of what it did when given a TIMESTAMP WITH TIME ZONE (as in the initial literal '2020-09-01 00:00:01 -0').\n\nWITH q1 AS\n (SELECT timestamptz '2020-09-01 00:00:01-00' AT TIME ZONE 'US/Pacific' AS t1,\n         timestamp '2020-09-01 00:00:01' AT TIME ZONE 'US/Pacific' AS t2)\n \nSELECT \n  t1,\n  t1::varchar AS t1_char,\n  t1 AT TIME ZONE 'UTC' AS t3,\n  t2 AT TIME ZONE 'UTC' AS t4,\n  typeof(t1),\n  typeof(t2)\nFROM q1\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\nt1\nt1_char\nt3\nt4\ntypeof(t1)\ntypeof(t2)\n\n\n\n\n2020-08-31 17:00:01\n2020-08-31 17:00:01\n2020-08-31 17:00:01\n2020-09-01 07:00:01\nTIMESTAMP\nTIMESTAMP WITH TIME ZONE\n\n\n\n\n\nIt seems that TIMESTAMP WITHOUT TIME ZONE values should be converted to a time zone as quickly as possible to avoid confusion and that care is needed with AT TIME ZONE given that it does very different (essentially opposite) things according to the supplied data type.\n\nWITH q1 AS\n (SELECT '2020-09-01 00:00:01-00'::timestamptz AS t1)\n \nSELECT t1,\n  t1::varchar AS t2,\n  typeof(t1)\nFROM q1\n\n\n1 records\n\n\nt1\nt2\ntypeof(t1)\n\n\n\n\n2020-09-01 00:00:01\n2020-08-31 20:00:01-04\nTIMESTAMP WITH TIME ZONE\n\n\n\n\n\nStrange behaviour can result from values stored as TIMESTAMP WITHOUT TIME ZONE. Below we see that t1 is printed as UTC no matter what, while the behaviour of t2 seems easier to understand.\n\nsql <-\n  \"(SELECT     \n    '2020-12-01 00:00:01-00' AS t1,\n    '2020-12-01 00:00:01-00'::timestamptz AS t2)\"\n\ntwo_times_notz <- tbl(db, sql(sql))\ntwo_times_notz |> kable()\n\n\n\n\nt1\nt2\n\n\n\n\n2020-12-01 00:00:01-00\n2020-12-01 00:00:01\n\n\n\n\n\n\ntwo_times_notz_r <-\n  collect(two_times_notz)\n  \nprint(two_times_notz_r$t1)\n\n[1] \"2020-12-01 00:00:01-00\"\n\nSys.timezone()\n\n[1] \"America/New_York\"\n\nprint(two_times_notz_r$t1, tz = Sys.timezone())\n\n[1] \"2020-12-01 00:00:01-00\"\n\nprint(two_times_notz_r$t2)\n\n[1] \"2020-12-01 00:00:01 UTC\"\n\nSys.timezone()\n\n[1] \"America/New_York\"\n\nprint(two_times_notz_r$t2, tz = Sys.timezone())\n\n[1] \"2020-11-30 19:00:01 EST\"\n\n\nAs pointed out by Tanimura (2021), one drawback to storing information as UTC is that localtime information may be lost. But it seems it would be more prudent to store information as TIMESTAMP WITH TIME ZONE and keep local time zone information as a separate column to avoid confusion. For example, if the orders table is stored as TIMESTAMP WITHOUT TIME ZONE based on the local time of the customer, which might be Australia/Melbourne and the shipping table uses TIMESTAMP WITH TIME ZONE, then an analyst of time-to-ship data would be confused by orders apparently being shipped before they are made. If shipping table uses TIMESTAMP WITH TIME ZONE using timestamps in the time zone of the East Bay warehouse (so US/Pacific), things would be even worse.\nI think that fully fleshing out the issues here would require a separate chapter. In fact, nothing in the core part of Chapter 3 of Tanimura (2021) (which focuses on the retail_sales table) really uses timestamp information, so we can put these issues aside for now.\n\n\nDate and Timestamp Format Conversions\nAs discussed in Tanimura (2021), PostgreSQL has a rich array of functions for converting dates and times and extracting such information as months and days of the week.\n\nSELECT date_trunc('month','2020-10-04 12:33:35 -00'::timestamptz);\n\n\n1 records\n\n\n\n\n\ndate_trunc(‘month’, CAST(‘2020-10-04 12:33:35 -00’ AS TIMESTAMP WITH TIME ZONE))\n\n\n\n\n2020-10-01 04:00:00\n\n\n\n\n\nOne such function\n\na_time_df <- tbl(db, sql(\"(SELECT '2020-10-04 12:33:35'::timestamp AS a_time)\"))\n\na_time_df |> \n  mutate(a_trunced_time = date_trunc('month', a_time))\n\n# Source:   SQL [1 x 2]\n# Database: DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n  a_time              a_trunced_time\n  <dttm>              <date>        \n1 2020-10-04 12:33:35 2020-10-01    \n\na_time_df |> \n  mutate(a_trunced_time = date_trunc('month', a_time)) |>\n  show_query()\n\n<SQL>\nSELECT *, date_trunc('month', a_time) AS a_trunced_time\nFROM (SELECT '2020-10-04 12:33:35'::timestamp AS a_time)\n\na_time_df |>\n  collect()\n\n# A tibble: 1 × 1\n  a_time             \n  <dttm>             \n1 2020-10-04 12:33:35\n\n\n\n\nDate Math\n\n\nTime Math\n\na_time_df <- tbl(db, sql(\"(SELECT '2020-10-04 12:33:35 US/Pacific'::timestamptz AS a_time)\"))\n\na_time_df |> \n  mutate(a_trunced_time = date_trunc('month', a_time)) \n\n# Source:   SQL [1 x 2]\n# Database: DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.3/:memory:]\n  a_time              a_trunced_time     \n  <dttm>              <dttm>             \n1 2020-10-04 19:33:35 2020-10-01 04:00:00\n\na_time_df |> \n  mutate(a_trunced_time = date_trunc('month', a_time)) |>\n  show_query()\n\n<SQL>\nSELECT *, date_trunc('month', a_time) AS a_trunced_time\nFROM (SELECT '2020-10-04 12:33:35 US/Pacific'::timestamptz AS a_time)\n\na_time_df |>\n  collect()\n\n# A tibble: 1 × 1\n  a_time             \n  <dttm>             \n1 2020-10-04 19:33:35\n\n\n\na_time_df |>\n  mutate(new_time = a_time + sql(\"interval '3 hours'\")) |>\n  collect()\n\n# A tibble: 1 × 2\n  a_time              new_time           \n  <dttm>              <dttm>             \n1 2020-10-04 19:33:35 2020-10-04 22:33:35\n\n\n\n\nThe Retail Sales Data Set\nAs discussed in Tanimura (2021), the data set used in this chapter comes from the website of the US Census Bureau. The data set is a little messy, but not too large, so we can easily grab it directly from the website and clean it up in much the same way that Cathy has done for us.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Use tmpdir = \".\" or known directory if you have trouble with \n# this part.\nmrtssales <- tempfile(fileext = \".xlsx\")\nurl <- paste0(\"https://www.census.gov/retail/mrts/www/\",\n              \"mrtssales92-present.xlsx\")\ndownload.file(url, mrtssales)\n\nread_tab <- function(year) {\n  \n  # Initially we read all columns as text, as we want to process\n  # more precise than read_excel() would do unsupervised.\n  temp <- read_excel(mrtssales,\n                     range = \"A4:N71\", \n                     sheet = as.character(year),\n                     col_types = \"text\",\n                     col_names = paste0(\"v\", 1:14))\n  \n  # The third row has the dates for columns 3:14\n  names(temp) <- c(\"naics_code\", \"kind_of_business\",\n                   as.character(temp[2, 3:14]))\n  \n  # The actual data are found after row 3\n  temp <- temp[-1:-3, ]\n  \n  # Now pivot the data and convert sales to numeric values.\n  # Also convert sales_month to dates (start of respective month).\n  df <-\n    temp |>\n    pivot_longer(names_to = \"sales_month\",\n                 values_to = \"sales\",\n                 cols = -1:-2) |>\n    mutate(sales_month = paste(\"01\", str_remove(sales_month, \"\\\\.\")),\n           sales_month = as.Date(sales_month, \"%d %b %Y\")) |>\n    mutate(reason_for_null = case_when(sales == \"(NA)\" ~ \"Not Available\",\n                                       sales == \"(S)\" ~ \"Supressed\",\n                                       TRUE ~ NA),\n           sales = case_when(sales == \"(NA)\" ~ NA,\n                             sales == \"(S)\" ~ NA,\n                             TRUE ~ sales)) |>\n    mutate(sales = as.double(sales)) |>\n    select(sales_month, naics_code, kind_of_business,\n           reason_for_null, sales)\n  df\n}\n\nretail_sales_local <- bind_rows(lapply(1992:2020, read_tab)) \n\nNote that there are differences between the retail_sales_local above and the data set we used in , as US Census economic data is continually being revised even after being released.\n\n\nEarthquake data\n\nweeks <- \n  tibble(start_date = seq(as.Date(\"2010-01-01\"), \n                          as.Date(\"2020-12-01\"),\n                          by = 7)) |>\n  mutate(end_date = start_date + days(7))\n  \nget_quake_data <- function(start, end) {\n  db <- dbConnect(duckdb::duckdb(), \"earthquakes_new.duckdb\")\n  url <- paste0(\"https://earthquake.usgs.gov/fdsnws/event/1/\",\n                \"query?format=csv&starttime=\", as.character(start), \n                \"&endtime=\", as.character(end))\n  df <- read_csv(url, show_col_types = FALSE)\n  dbWriteTable(db, \"earthquakes_new2\", df,\n               append = TRUE, row.names = FALSE)\n  dbDisconnect(db, shutdown = TRUE)\n  return(TRUE)\n}\n\ndb <- dbConnect(duckdb::duckdb(), \"earthquakes_new.duckdb\")\ndbExecute(db, \"DROP TABLE IF EXISTS earthquakes\")\ndbDisconnect(db, shutdown = TRUE)\n\nres <- map2(weeks$start_date,\n            weeks$end_date, \n            get_quake_data,\n            .progress = TRUE)\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Tanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  }
]