[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SQL for Data Analysis using R",
    "section": "",
    "text": "Preface\nThis “book” is a collection of notes made as I work through Tanimura (2021). An alternative title might have been “Data Analysis with Data Stored in Databases”; while SQL is the language used in Tanimura (2021) to do the analysis, this is in some ways a minor detail. My view is that one can easy do “SQL” analysis using the dplyr package in R (this uses dbplyr behind the scenes … mostly). The dplyr package will quietly translate R code into SQL for the user.\nAn advantage of using dplyr rather than SQL directly is that one doesn’t need to learn as much SQL. In principle, one could use dplyr without knowing any SQL. Given that dplyr and R can be used to analyse data from other data sources, this reduces the amount that is needed to be learnt to do analysis. Additionally, one could write code to analyse data from a database and then easily reuse the code for data from a CSV file.\nNotwithstanding the discussion in the previous paragraph, I recommend that people who find themselves using SQL-driven databases a lot learn some SQL. This view is implicitly endorsed by Hadley Wickham with the inclusion of a significant amount of material intended to teach SQL to readers of “R for Data Science”. While we include a brief primer on SQL for dplyr users here, clearly an excellent source for learning SQL is Tanimura (2021) itself.\nAnother benefit of using R is that we can do more with R. Tanimura (2021) includes many plots of data produced using SQL, but the code to make the plots is not included. In contrast, here I will include code to produce the data as well as code to make plots (where applicable). In Chapter 7, we will see that using R avoids the need to go off to an “online calculator” to calculate \\(p\\)-values and the like.\nMy view is that the material here might be useful to someone who is using Tanimura (2021) and wants to see how the ideas there can be implemented using R, while at the same time being useful to someone who knows (or is looking to learn) R and is looking for realistic data sets to work on."
  },
  {
    "objectID": "index.html#the-structure-of-this-book",
    "href": "index.html#the-structure-of-this-book",
    "title": "SQL for Data Analysis using R",
    "section": "The structure of this book",
    "text": "The structure of this book\nFirst of all, I should note that this “book” is not a standalone book. To make sense of what I have here, you really need to get a copy of Tanimura (2021) itself. This book is more of a companion guide to someone who is more familiar with R than SQL, or coming from SQL and looking to learn R in the most effective way possible, or simply looking for a way to get more out of Tanimura (2021).\nWith that out of the way, the book evolves in its relation with Tanimura (2021). In Chapters 1 and 2, there is not much to say here, in part because there’s not a lot of SQL code to translate (the main feature of this book). For these two chapters, I mostly limit myself to some general observations on PostgreSQL, R, and dplyr/dbplyr.\nIn Chapters 3 and 4, I focus on the task of translating SQL to dbplyr equivalents. As a convenience, I include SQL and equivalent R code for each code example provided in Tanimura (2021). In some cases, I tweak the SQL found in Tanimura (2021) to better conform to my tastes or to make the translation exercise easier (e.g., I tend to use CTEs much more than Tanimura (2021) does).\nFrom Chapter 5 onwards, I generally omit SQL and just present the dplyr equivalents of the SQL provided in Tanimura (2021)."
  },
  {
    "objectID": "index.html#sql-and-me",
    "href": "index.html#sql-and-me",
    "title": "SQL for Data Analysis using R",
    "section": "SQL and me",
    "text": "SQL and me\nComputer Information Systems I was a required subject in my undergraduate degree program.1 While I do not recall many of the details of the subject, I do recall a lecturer whose English was difficult to understand (I am not sure how many lectures I attended, as I thought at the time that lectures are a very inefficient way to learn … as I still do) and a group project.\nThe group project required the construction of an SQL database for a hypothetical business. My group comprised three people—myself, a good friend, and someone who was working full-time and studying a degree specializing in information systems. I was a self-supporting full-time student, so I could not afford the software we used to build the database (my memory tells me it was about $60, but I know enough not to trust memories), but my friend had a copy. I recall that some parts of the project were completed over the phone (this was before the internet), perhaps not the best way to write SQL. In the end, I guess we got the project done and submitted.2\nIn any case, after that subject I continued on with the rest of my degree, but I don’t think I used SQL or any data analysis tools except for Excel for the remainder of my Bachelor of Commerce (the Bachelor of Laws was free of quantitative analysis as far as I recall).\nWhen I graduated I joined a “strategy consulting” firm. My first project involved trying to explain the factors affecting branch profitability for a regional firm offering banking and insurance products. I don’t remember all the details, but there were many data analyses and some involved making requests for SQL queries to be run to supply the data that I used.\nMy second significant project involved analyses of product and customer profitability. Again the details are hazy, but I recall that analyses required joining multiple tables involving data on mapping of products to cost centres, products to orders, orders to customers, and so on. I would guess that I used Microsoft Access on a laptop that was underpowered (for its time).3\nA later project had a reputation that preceded it. Several people had joined this project before deciding to leave the firm. My recollection is that a partner had been hired from another firm with the understanding that he would lead an “insurance benchmarking study”. The assigned junior person on the project would be waiting until the very busy partner had managed to extract complete surveys from various insurance firms. Faced with this prospect, I tried to engineer my way off this project by creating a Visual Basic program that would take completed surveys and, with assistance of a Microsoft Access database, create a Powerpoint slide automatically. My program worked OK, but I ended up needing to resign to get off the project myself.\nA later freelance consulting project had me working with the financial planning team of a major Australian bank. Their existing planning process involved sending out numerous Excel spreadsheet templates to various units (branches, project managers) and then carefully stitching back the completed spreadsheets into a single large spreadsheet. This process would take weeks because the returned spreadsheets had been quite mangled. I helped the team reengineer the process to use a Microsoft Access database fed by locked templates. The resulting process took hours instead of weeks.\nMaking the fateful—and much regretted—decision to leave business for a “career” in business academia, my exposure to SQL has not gone down. For the first several years after entering the PhD program at Stanford, PROC SQL in SAS was a mainstay of my analysis pipeline. In 2011, I decided to migrate to an alternative database, as I found SAS restrictive. After trying a number of alternatives (SQLite and MySQL), I settled on PostgreSQL as the backend of my data analysis workflow.\nInitially, I was writing a lot of SQL and using other programs (e.g., Stata and R) to analyse data. When dplyr and dbplyr emerged, I immediately found this to be very intuitive and facile for analysis. Nowadays, I rarely write SQL directly and rely on dbplyr to do that for me. Of course, it’s still very helpful to know SQL well and I suspect I still “think in SQL” even though I type in dbplyr commands. Today, I essentially never use “data files”; all my data go through a PostgreSQL database.\nThe point of boring any reader with the autobiographical details above is to illustrate that one can go a lot of places in data analysis and not get very far from SQL.\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "SQL for Data Analysis using R",
    "section": "",
    "text": "Bachelor of Commerce/Bachelor of Laws at the University of New South Wales.↩︎\nI vaguely recall getting a lower grade than my friend for the group assignment, which was inexplicable since there was no indication who did what. I survived.↩︎\nI recall that partners had the higher-powered laptops—necessary for writing Lotus Notes and reading Powerpoint slides—while analysts like me had older laptops. It didn’t make sense to me at the time, and still doesn’t.↩︎"
  },
  {
    "objectID": "chapter_1.html#what-is-data-analysis",
    "href": "chapter_1.html#what-is-data-analysis",
    "title": "1  Analysis with SQL",
    "section": "1.1 What is Data Analysis?",
    "text": "1.1 What is Data Analysis?"
  },
  {
    "objectID": "chapter_1.html#why-sql",
    "href": "chapter_1.html#why-sql",
    "title": "1  Analysis with SQL",
    "section": "1.2 Why SQL?",
    "text": "1.2 Why SQL?\n\n1.2.1 What is SQL?\n\n\n1.2.2 Benefits of SQL\n\n\n1.2.3 SQL versus R or Python\n\n\n1.2.4 SQL as Part of the Data Analysis Workflow"
  },
  {
    "objectID": "chapter_1.html#database-types-and-how-to-work-with-them",
    "href": "chapter_1.html#database-types-and-how-to-work-with-them",
    "title": "1  Analysis with SQL",
    "section": "1.3 Database Types and How to Work with Them",
    "text": "1.3 Database Types and How to Work with Them\n\n1.3.1 Row-Store Databases\n\n\n1.3.2 Column-Store Databases\n\n\n1.3.3 Other Types of Data Infrastructure"
  },
  {
    "objectID": "chapter_1.html#conclusion",
    "href": "chapter_1.html#conclusion",
    "title": "1  Analysis with SQL",
    "section": "1.4 Conclusion",
    "text": "1.4 Conclusion"
  },
  {
    "objectID": "chapter_2.html#types-of-data",
    "href": "chapter_2.html#types-of-data",
    "title": "2  Preparing Data for Analysis",
    "section": "2.1 Types of Data",
    "text": "2.1 Types of Data\n\n2.1.1 Database Data Types\nR has all the types listed in Chapter 2. PostgreSQL has a richer set of data types than base R has.1\nIn general, data retains the equivalent type when transferred from R to PostgreSQL or vice versa. Sometimes data makes round trips (e.g., data is pulled from PostgreSQL to R for computations and then sent back to R) and it is important to check that data types are retained in the process (e.g., that timestamps don’t shift to a different time zone).\n\n\n2.1.2 Structured versus Unstructured\n\n\n2.1.3 Quantitative versus Qualitative Data\n\n\n2.1.4 First-, Second-, and Third-Party Data\n\n\n2.1.5 Sparse Data"
  },
  {
    "objectID": "chapter_2.html#sql-query-structure",
    "href": "chapter_2.html#sql-query-structure",
    "title": "2  Preparing Data for Analysis",
    "section": "2.2 SQL Query Structure",
    "text": "2.2 SQL Query Structure"
  },
  {
    "objectID": "chapter_2.html#profiling-distributions",
    "href": "chapter_2.html#profiling-distributions",
    "title": "2  Preparing Data for Analysis",
    "section": "2.3 Profiling: Distributions",
    "text": "2.3 Profiling: Distributions\n\n2.3.1 Histograms and Frequencies\n\n\n2.3.2 Binning\n\n\n2.3.3 n-Tiles"
  },
  {
    "objectID": "chapter_2.html#profiling-data-quality",
    "href": "chapter_2.html#profiling-data-quality",
    "title": "2  Preparing Data for Analysis",
    "section": "2.4 Profiling: Data Quality",
    "text": "2.4 Profiling: Data Quality\n\n2.4.1 Detecting Duplicates\n\n\n2.4.2 Deduplication with GROUP BY and DISTINCT"
  },
  {
    "objectID": "chapter_2.html#preparing-data-cleaning",
    "href": "chapter_2.html#preparing-data-cleaning",
    "title": "2  Preparing Data for Analysis",
    "section": "2.5 Preparing: Data Cleaning",
    "text": "2.5 Preparing: Data Cleaning\n\n2.5.1 Cleaning Data with CASE Transformations\n\n\n2.5.2 Type Conversions and Casting\n\n\n2.5.3 Dealing with Nulls: coalesce, nullif, nvl Functions\n\n\n2.5.4 Missing data\nThe SQL in the book generally uses the form x::date rather than the more standard SQL CAST(x AS DATE). In dbplyr, we would use as.Date(x) and dbplyr would translate as CAST(x AS DATE). The following code and output demonstrates how dbplyr translated from dplyr to SQL.\nThe table stored in dates_processed below is equivalent to that created and stored in the database as date_dim in the code supplied with book. This date_dim table is only used in #sec-time-series of the book and we will not even use it there (for reasons to be explained).\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(knitr)\n\n\npg &lt;- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)"
  },
  {
    "objectID": "chapter_2.html#preparing-shaping-data",
    "href": "chapter_2.html#preparing-shaping-data",
    "title": "2  Preparing Data for Analysis",
    "section": "2.6 Preparing: Shaping Data",
    "text": "2.6 Preparing: Shaping Data\n\n2.6.1 For Which Output: BI, Visualization, Statistics, ML\n\n\n2.6.2 Pivoting with CASE Statements\n\n\n2.6.3 Unpivoting with UNION Statements\nA user of dplyr has access to the functions pivot_wider and pivot_longer, which make it much easier to “pivot” and “unpivot” tables than using CASE statements, which could become long and tedious.\nTo illustrate the dplyr way of doing things, I will create ctry_pops to match the data discussed in Chapter 2. First, I create the data set using the tribble() function from dplyr.\n\nctry_pops &lt;-\n  tribble(\n  ~country, ~year_1980,  ~year_1990, ~year_2000, ~year_2010,\n  \"Canada\", 24593, 27791, 31100, 34207,\n  \"Mexico\", 68347, 84634, 99775, 114061,\n  \"United States\", 227225, 249623, 282162, 309326\n)\n\nSecond, I pivot the local data frame using pivot_longer.\n\nctry_pops_long &lt;-\n  ctry_pops %&gt;%\n  pivot_longer(cols = -country, \n               names_to = \"year\",\n               names_prefix = \"year_\",\n               values_ptypes = integer(),\n               values_to = \"population\") \nctry_pops_long %&gt;%\n  kable()\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\nCanada\n1980\n24593\n\n\nCanada\n1990\n27791\n\n\nCanada\n2000\n31100\n\n\nCanada\n2010\n34207\n\n\nMexico\n1980\n68347\n\n\nMexico\n1990\n84634\n\n\nMexico\n2000\n99775\n\n\nMexico\n2010\n114061\n\n\nUnited States\n1980\n227225\n\n\nUnited States\n1990\n249623\n\n\nUnited States\n2000\n282162\n\n\nUnited States\n2010\n309326\n\n\n\n\n\nNext, I copy the data to PostgreSQL, so that it’s a (temporary) table inside the database.2\n\nctry_pops_db &lt;- copy_to(pg, ctry_pops)\nctry_pops_db\n\n# Source:   table&lt;ctry_pops&gt; [3 x 5]\n# Database: postgres  [igow@/tmp:5432/igow]\n  country       year_1980 year_1990 year_2000 year_2010\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Canada            24593     27791     31100     34207\n2 Mexico            68347     84634     99775    114061\n3 United States    227225    249623    282162    309326\n\n\n\nctry_pops_db_long &lt;-\n  ctry_pops_db %&gt;%\n  pivot_longer(cols = -country, \n               names_to = \"year\",\n               names_prefix = \"year_\",\n               values_to = \"population\") \n\nFrom the output below, we can see that dbplyr has taken care of the tedious business of constructing several statements for us.\n\nctry_pops_db_long %&gt;%\n  show_query()\n\n&lt;SQL&gt;\n(\n  (\n    (\n      SELECT \"country\", '1980' AS \"year\", \"year_1980\" AS \"population\"\n      FROM \"ctry_pops\"\n    )\n    UNION ALL\n    (\n      SELECT \"country\", '1990' AS \"year\", \"year_1990\" AS \"population\"\n      FROM \"ctry_pops\"\n    )\n  )\n  UNION ALL\n  (\n    SELECT \"country\", '2000' AS \"year\", \"year_2000\" AS \"population\"\n    FROM \"ctry_pops\"\n  )\n)\nUNION ALL\n(\n  SELECT \"country\", '2010' AS \"year\", \"year_2010\" AS \"population\"\n  FROM \"ctry_pops\"\n)\n\n\nAnd from the following, we can see that the result is the same as it was when using dplyr on a local data frame.\n\nctry_pops_db_long %&gt;%\n  kable()\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\nCanada\n1980\n24593\n\n\nMexico\n1980\n68347\n\n\nUnited States\n1980\n227225\n\n\nCanada\n1990\n27791\n\n\nMexico\n1990\n84634\n\n\nUnited States\n1990\n249623\n\n\nCanada\n2000\n31100\n\n\nMexico\n2000\n99775\n\n\nUnited States\n2000\n282162\n\n\nCanada\n2010\n34207\n\n\nMexico\n2010\n114061\n\n\nUnited States\n2010\n309326\n\n\n\n\n\nAnd we can reverse the pivot_longer() using pivot_wider().\n\nctry_pops_db_long %&gt;%\n  compute() %&gt;%\n  pivot_wider(names_from = year, \n              values_from = population, \n              names_prefix = \"year_\") %&gt;%\n  kable()\n\n\n\n\ncountry\nyear_2010\nyear_1990\nyear_2000\nyear_1980\n\n\n\n\nMexico\n114061\n84634\n99775\n68347\n\n\nCanada\n34207\n27791\n31100\n24593\n\n\nUnited States\n309326\n249623\n282162\n227225\n\n\n\n\n\n\n\n2.6.4 pivot and unpivot Functions\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_2.html#footnotes",
    "href": "chapter_2.html#footnotes",
    "title": "2  Preparing Data for Analysis",
    "section": "",
    "text": "Some packages offer additional data types, so it is difficult to be definitive where R has fewer types once packages are included.↩︎\n“Temporary” here means that it will disappear once we close our connection to the database.↩︎"
  },
  {
    "objectID": "chapter_3.html#sec-retail-data",
    "href": "chapter_3.html#sec-retail-data",
    "title": "3  Time Series Analysis",
    "section": "3.1 The Retail Sales Data Set",
    "text": "3.1 The Retail Sales Data Set\nWe read the data\n\nretail_sales_local &lt;- read_csv(\"data/us_retail_sales.csv\",\n                               show_col_types = FALSE)\n\nNow that we have the data in R, a few questions arise.\nFirst, why would we move it to a database? Second, how can we move it to a database? Related to the previous question will be: which database to we want to move it to?\nTaking these questions in turn, the first one is a good question. With retail_sales as a local data frame, we can run almost all the analyses below with only the slightest modifications. The modifications needed are to replace every instance of window_order() with arrange().1 The “almost all” relates to the moving average, which relies on window_frame() from dbplyr, which has no exact equivalent in dplyr.\nSo the first point is that “almost all” implies an advantage for using dbplyr. On occasion, the SQL engine provided by PostgreSQL will allow us to do some data manipulations more easily than we can in R. But of course, there are many cases when the opposite is true. That said, why not have both? Store data in a database and collect() as necessary to do things that R is better at?\nSecond, performance can be better using SQL. Compiling a version of this chapter using dplyr with a local data frame for the remainder took just under 14 seconds. Using a PostgreSQL backend, it took 8.5 seconds. Adding back in the queries with window_order() that I removed so that I could compile with dplyr and using DuckDB as the backend, the document took 8.3 seconds to compile (this beat out PostgreSQL doing the same in 9.6 seconds). While these differences are not practically very significant, they could be with more demanding tasks. Also, a database will not always beat R or Python for performance, but often will and having the option to use a database backend is a good thing.\nThird, having the data in a database allows you to interrogate the data using SQL. If you are more familiar with SQL, or just know how to do a particular task in SQL, this can be beneficial.2\nFourth, I think there is merit in separating the tasks of acquiring and cleaning data from the task of analysing those data. Many data analysts have a work flow that entails ingesting and cleaning data for each analysis task. My experience is that it is often better to do the ingesting and cleaning once and then reuse the cleaned data in subsequent analyses. A common pattern involves reusing the same data in many analyses and it can be helpful to divide the tasks in a way that using an SQL database encourages. Also the skills in ingesting and cleaning data can be different from those for analysing those data, so sometimes it makes sense for one person to do one task, push the data to a database, and then have someone else do some or all of the analysis.\nRegarding the second question, there are a few options. But for this chapter we will use duckdb\nTo install DuckDB, all we have to do is install.packages(\"duckdb\").\nThen we can create a connection as follows.\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nHere we use the default of an in-memory database. At the end of this chapter, we discuss how we could store the data in a file (say, legislators.duckdb) if we want persistent storage.\n\nretail_sales &lt;- \n  retail_sales_local %&gt;%\n  copy_to(db, ., name = \"retail_sales\")"
  },
  {
    "objectID": "chapter_3.html#trending-the-data",
    "href": "chapter_3.html#trending-the-data",
    "title": "3  Time Series Analysis",
    "section": "3.2 Trending the Data",
    "text": "3.2 Trending the Data\n\n3.2.1 Simple Trends\n\nSELECT sales_month, sales\nFROM retail_sales\nWHERE kind_of_business = 'Retail and food services sales, total'\nORDER BY 1\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nsales\n\n\n\n\n1992-01-01\n146376\n\n\n1992-02-01\n147079\n\n\n1992-03-01\n159336\n\n\n1992-04-01\n163669\n\n\n1992-05-01\n170068\n\n\n1992-06-01\n168663\n\n\n1992-07-01\n169890\n\n\n1992-08-01\n170364\n\n\n1992-09-01\n164617\n\n\n1992-10-01\n173655\n\n\n\n\n\n\nretail_sales %&gt;%\n    filter(kind_of_business == 'Retail and food services sales, total') %&gt;%\n  select(sales_month, sales) %&gt;%\n  ggplot(aes(x = sales_month, y = sales)) +\n  geom_line()\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year,\n    sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business = 'Retail and food services sales, total'\nGROUP BY 1\n;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nsales\n\n\n\n\n1992\n2014102\n\n\n1993\n2153095\n\n\n1994\n2330235\n\n\n1995\n2450628\n\n\n1996\n2603794\n\n\n1997\n2726131\n\n\n1998\n2852956\n\n\n1999\n3086990\n\n\n2000\n3287537\n\n\n2001\n3378906\n\n\n\n\n\n\nretail_sales %&gt;%\n  filter(kind_of_business == 'Retail and food services sales, total') %&gt;%\n  mutate(sales_year = year(sales_month)) %&gt;%\n  group_by(sales_year) %&gt;%\n  summarize(sales = sum(sales, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = sales_year, y = sales)) +\n  geom_line()\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year, \n  kind_of_business, sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business IN \n          ('Book stores',\n           'Sporting goods stores',\n           'Hobby, toy, and game stores')\nGROUP BY 1,2\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nkind_of_business\nsales\n\n\n\n\n1992\nBook stores\n8327\n\n\n1992\nSporting goods stores\n15583\n\n\n1992\nHobby, toy, and game stores\n11251\n\n\n1993\nBook stores\n9108\n\n\n1993\nSporting goods stores\n16791\n\n\n1993\nHobby, toy, and game stores\n11651\n\n\n1994\nBook stores\n10107\n\n\n1994\nSporting goods stores\n18825\n\n\n1994\nHobby, toy, and game stores\n12850\n\n\n1995\nBook stores\n11196\n\n\n\n\n\n\n\n3.2.2 Comparing Components\n\nretail_sales %&gt;%\n  filter(kind_of_business %in% \n           c('Book stores',\n             'Sporting goods stores',\n             'Hobby, toy, and game stores')) %&gt;%\n  mutate(sales_year = year(sales_month)) %&gt;%\n  group_by(sales_year, kind_of_business) %&gt;%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\nWHERE kind_of_business IN ('Men''s clothing stores','Women''s clothing stores')\nORDER BY 1,2;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nkind_of_business\nsales\n\n\n\n\n1992-01-01\nMen’s clothing stores\n701\n\n\n1992-01-01\nWomen’s clothing stores\n1873\n\n\n1992-02-01\nMen’s clothing stores\n658\n\n\n1992-02-01\nWomen’s clothing stores\n1991\n\n\n1992-03-01\nMen’s clothing stores\n731\n\n\n1992-03-01\nWomen’s clothing stores\n2403\n\n\n1992-04-01\nMen’s clothing stores\n816\n\n\n1992-04-01\nWomen’s clothing stores\n2665\n\n\n1992-05-01\nMen’s clothing stores\n856\n\n\n1992-05-01\nWomen’s clothing stores\n2752\n\n\n\n\n\n\nretail_sales %&gt;%\n  filter(kind_of_business %in% c(\"Men's clothing stores\",\n                                 \"Women's clothing stores\")) %&gt;%\n  select(sales_month, kind_of_business, sales) %&gt;%\n  ggplot(aes(x = sales_month, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year,\n  kind_of_business, sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business IN \n        ('Men''s clothing stores',\n        'Women''s clothing stores')\nGROUP BY 1, 2\nORDER BY 1, 2;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nkind_of_business\nsales\n\n\n\n\n1992\nMen’s clothing stores\n10179\n\n\n1992\nWomen’s clothing stores\n31815\n\n\n1993\nMen’s clothing stores\n9962\n\n\n1993\nWomen’s clothing stores\n32350\n\n\n1994\nMen’s clothing stores\n10032\n\n\n1994\nWomen’s clothing stores\n30585\n\n\n1995\nMen’s clothing stores\n9315\n\n\n1995\nWomen’s clothing stores\n28696\n\n\n1996\nMen’s clothing stores\n9546\n\n\n1996\nWomen’s clothing stores\n28238\n\n\n\n\n\n\nretail_sales %&gt;%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %&gt;%\n  mutate(sales_year = year(sales_month)) %&gt;%\n  group_by(sales_year, kind_of_business) %&gt;%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT date_part('year', sales_month) AS sales_year,\n  sum(CASE WHEN kind_of_business = 'Women''s clothing stores' \n          then sales \n          END) AS womens_sales,\n  sum(CASE WHEN kind_of_business = 'Men''s clothing stores' \n          then sales \n          END) AS mens_sales\nFROM retail_sales\nWHERE kind_of_business IN \n   ('Men''s clothing stores',\n    'Women''s clothing stores')\nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nwomens_sales\nmens_sales\n\n\n\n\n1992\n31815\n10179\n\n\n1993\n32350\n9962\n\n\n1994\n30585\n10032\n\n\n1995\n28696\n9315\n\n\n1996\n28238\n9546\n\n\n1997\n27822\n10069\n\n\n1998\n28332\n10196\n\n\n1999\n29549\n9667\n\n\n2000\n31447\n9507\n\n\n2001\n31453\n8625\n\n\n\n\n\n\npivoted_sales &lt;-\n  retail_sales %&gt;%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %&gt;%\n  mutate(kind_of_business = if_else(kind_of_business == \"Women's clothing stores\",\n                                    \"womens\", \"mens\"),\n         sales_year = year(sales_month)) %&gt;%\n  group_by(sales_year, kind_of_business) %&gt;%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  pivot_wider(id_cols = \"sales_year\",\n              names_from = \"kind_of_business\",\n              names_glue = \"{kind_of_business}_{.value}\",\n              values_from = \"sales\")  \n\npivoted_sales %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  sales_year,\n  MAX(CASE WHEN (kind_of_business = 'mens') THEN sales END) AS mens_sales,\n  MAX(CASE WHEN (kind_of_business = 'womens') THEN sales END) AS womens_sales\nFROM (\n  SELECT sales_year, kind_of_business, SUM(sales) AS sales\n  FROM (\n    SELECT\n      sales_month,\n      naics_code,\n      CASE WHEN (kind_of_business = 'Women''s clothing stores') THEN 'womens' WHEN NOT (kind_of_business = 'Women''s clothing stores') THEN 'mens' END AS kind_of_business,\n      reason_for_null,\n      sales,\n      EXTRACT(year FROM sales_month) AS sales_year\n    FROM retail_sales\n    WHERE (kind_of_business IN ('Men''s clothing stores', 'Women''s clothing stores'))\n  ) q01\n  GROUP BY sales_year, kind_of_business\n) q02\nGROUP BY sales_year\n\npivoted_sales %&gt;%\n  arrange(sales_year) %&gt;%\n  collect(n = 10) %&gt;%\n  kable()\n\n\n\n\nsales_year\nmens_sales\nwomens_sales\n\n\n\n\n1992\n10179\n31815\n\n\n1993\n9962\n32350\n\n\n1994\n10032\n30585\n\n\n1995\n9315\n28696\n\n\n1996\n9546\n28238\n\n\n1997\n10069\n27822\n\n\n1998\n10196\n28332\n\n\n1999\n9667\n29549\n\n\n2000\n9507\n31447\n\n\n2001\n8625\n31453\n\n\n\n\n\n\npivoted_sales %&gt;%\n  filter(sales_year &lt;= 2019) %&gt;%\n  group_by(sales_year) %&gt;%\n  mutate(womens_minus_mens = womens_sales - mens_sales,\n         mens_minus_womens = mens_sales - womens_sales) %&gt;%\n  select(sales_year, womens_minus_mens, mens_minus_womens) %&gt;%\n  ggplot(aes(y = womens_minus_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\npivoted_sales %&gt;%\n  filter(sales_year &lt;= 2019) %&gt;%\n  group_by(sales_year) %&gt;%\n  mutate(womens_times_of_mens = womens_sales / mens_sales) %&gt;%\n  ggplot(aes(y = womens_times_of_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\npivoted_sales %&gt;%\n  filter(sales_year &lt;= 2019) %&gt;%\n  group_by(sales_year) %&gt;%\n  mutate(womens_pct_of_mens = (womens_sales / mens_sales - 1) * 100) %&gt;%\n  ggplot(aes(y = womens_pct_of_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\n\n3.2.3 Percent of Total Calculations\n\nretail_sales %&gt;%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %&gt;%\n  group_by(sales_month) %&gt;%\n  mutate(total_sales = sum(sales, na.rm = TRUE))  %&gt;%\n  ungroup() %&gt;%\n  mutate(pct_total_sales = sales * 100 / total_sales) %&gt;%\n  select(sales_month, kind_of_business, pct_total_sales) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\nsales_month\nkind_of_business\npct_total_sales\n\n\n\n\n1992-06-01\nMen’s clothing stores\n26.02991\n\n\n1992-06-01\nWomen’s clothing stores\n73.97009\n\n\n1992-08-01\nMen’s clothing stores\n22.62667\n\n\n\n\n\n\nretail_sales %&gt;%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %&gt;%\n  group_by(sales_month) %&gt;%\n  mutate(total_sales = sum(sales, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct_total_sales = sales * 100 / total_sales) %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT *, (sales * 100.0) / total_sales AS pct_total_sales\nFROM (\n  SELECT *, SUM(sales) OVER (PARTITION BY sales_month) AS total_sales\n  FROM retail_sales\n  WHERE (kind_of_business IN ('Men''s clothing stores', 'Women''s clothing stores'))\n) q01\n\n\n\nretail_sales %&gt;%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %&gt;%\n  group_by(sales_month) %&gt;%\n  mutate(total_sales = sum(sales, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct_total_sales = sales * 100 / total_sales) %&gt;%\n  ggplot(aes(y = pct_total_sales, x = sales_month, color = kind_of_business)) +\n  geom_line()\n\n\n\n\n\n\n3.2.4 Indexing to See Percent Change over Time\n\nretail_sales %&gt;%\n  filter(kind_of_business == \"Women's clothing stores\") %&gt;%\n  mutate(sales_year = year(sales_month)) %&gt;%\n  group_by(sales_year) %&gt;%\n  summarize(sales = sum(sales, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  window_order(sales_year) %&gt;%\n  mutate(index_sales = first(sales),\n         pct_from_index = (sales/index_sales - 1) * 100) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\nsales_year\nsales\nindex_sales\npct_from_index\n\n\n\n\n1992\n31815\n31815\n0.000000\n\n\n1993\n32350\n31815\n1.681597\n\n\n1994\n30585\n31815\n-3.866101\n\n\n\n\n\n\nretail_sales %&gt;%\n  filter(kind_of_business %in% c(\"Women's clothing stores\",\n                                 \"Men's clothing stores\"),\n         sales_month &lt;= '2019-12-31') %&gt;%\n  mutate(sales_year = year(sales_month)) %&gt;%\n  group_by(kind_of_business, sales_year) %&gt;%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(kind_of_business) %&gt;%\n  window_order(sales_year) %&gt;%\n  mutate(index_sales = first(sales),\n         pct_from_index = (sales/index_sales - 1) * 100) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(y = pct_from_index, x = sales_year, color = kind_of_business)) +\n  geom_line()"
  },
  {
    "objectID": "chapter_3.html#rolling-time-windows",
    "href": "chapter_3.html#rolling-time-windows",
    "title": "3  Time Series Analysis",
    "section": "3.3 Rolling Time Windows",
    "text": "3.3 Rolling Time Windows\n\n3.3.1 Calculating Rolling Time Windows\n\nmvg_avg &lt;-\n  retail_sales %&gt;%\n  filter(kind_of_business == \"Women's clothing stores\") %&gt;%\n  window_order(sales_month) %&gt;%\n  window_frame(-11, 0) %&gt;%\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n()) %&gt;%\n  filter(sales_month &gt;= '1993-01-01') \n\nmvg_avg %&gt;%\n  select(sales_month, moving_avg, records_count) %&gt;%\n  collect(n = 3) %&gt;%\n  kable(digits = 2)\n\n\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n1993-01-01\n2672.08\n12\n\n\n1993-02-01\n2673.25\n12\n\n\n1993-03-01\n2676.50\n12\n\n\n\n\n\n\nmvg_avg %&gt;%\n  ggplot(aes(x = sales_month)) +\n  geom_line(aes(y = sales, colour = \"Sales\")) +\n  geom_line(aes(y = moving_avg, colour = \"Moving average\")) \n\n\n\n\n\nSELECT \n  sales_month,\n  avg(sales) over w AS moving_avg,\n  count(sales) over w AS records_count\nFROM retail_sales\nWHERE kind_of_business = 'Women''s clothing stores'\nWINDOW w AS (order by sales_month \n             rows between 11 preceding and current row)\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n1992-01-01\n1873.000\n1\n\n\n1992-02-01\n1932.000\n2\n\n\n1992-03-01\n2089.000\n3\n\n\n1992-04-01\n2233.000\n4\n\n\n1992-05-01\n2336.800\n5\n\n\n1992-06-01\n2351.333\n6\n\n\n1992-07-01\n2354.429\n7\n\n\n1992-08-01\n2392.250\n8\n\n\n1992-09-01\n2410.889\n9\n\n\n1992-10-01\n2445.300\n10\n\n\n\n\n\n\nretail_sales %&gt;%\n  filter(kind_of_business == \"Women's clothing stores\") %&gt;%\n  window_order(sales_month) %&gt;%\n  window_frame(-11, 0) %&gt;%\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n()) %&gt;%\n  select(sales_month, moving_avg, records_count) %&gt;%\n  collect(n = 10) %&gt;%\n  kable()\n\n\ndate_dim &lt;-\n  tibble(date = seq(as.Date('1993-01-01'), \n                    as.Date('2020-12-01'), \n                    by = \"1 month\")) %&gt;%\n  copy_to(db, ., overwrite = TRUE, name = \"date_dim\")\n\n\nWITH jan_jul AS (\n  SELECT sales_month, sales\n  FROM retail_sales \n  WHERE kind_of_business = 'Women''s clothing stores'\n     AND date_part('month', sales_month) IN (1, 7))\n     \nSELECT a.date, b.sales_month, b.sales\nFROM date_dim a\nINNER JOIN jan_jul b \nON b.sales_month BETWEEN a.date - interval '11 months' AND a.date\nWHERE a.date BETWEEN '1993-01-01' AND '2020-12-01';\n\n\nDisplaying records 1 - 10\n\n\ndate\nsales_month\nsales\n\n\n\n\n1993-01-01\n1992-07-01\n2373\n\n\n1993-02-01\n1992-07-01\n2373\n\n\n1993-03-01\n1992-07-01\n2373\n\n\n1993-04-01\n1992-07-01\n2373\n\n\n1993-05-01\n1992-07-01\n2373\n\n\n1993-06-01\n1992-07-01\n2373\n\n\n1993-01-01\n1993-01-01\n2123\n\n\n1993-02-01\n1993-01-01\n2123\n\n\n1993-03-01\n1993-01-01\n2123\n\n\n1993-04-01\n1993-01-01\n2123\n\n\n\n\n\n\njan_jul &lt;-\n  retail_sales %&gt;%\n  filter(kind_of_business == \"Women's clothing stores\",\n         month(sales_month) %in% c(1, 7)) %&gt;%\n  select(sales_month, sales)\n\ndate_dim %&gt;%\n  mutate(date_start = date - months(11)) %&gt;%\n  inner_join(jan_jul, \n             join_by(between(y$sales_month, x$date_start, x$date))) %&gt;%\n  select(date, sales_month, sales) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\ndate\nsales_month\nsales\n\n\n\n\n1993-01-01\n1992-07-01\n2373\n\n\n1993-02-01\n1992-07-01\n2373\n\n\n1993-03-01\n1992-07-01\n2373\n\n\n\n\n\n\ndate_dim %&gt;%\n  mutate(date_start = date - months(11)) %&gt;%\n  inner_join(jan_jul, \n             join_by(between(y$sales_month, x$date_start, x$date))) %&gt;%\n  group_by(date) %&gt;%\n  summarize(moving_avg = mean(sales, na.rm = TRUE),\n            records = n()) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\ndate\nmoving_avg\nrecords\n\n\n\n\n1993-01-01\n2248\n2\n\n\n1993-02-01\n2248\n2\n\n\n1993-03-01\n2248\n2\n\n\n\n\n\n\nWITH sales_months AS (\n  SELECT distinct sales_month\n  FROM retail_sales\n  WHERE sales_month between '1993-01-01' and '2020-12-01')\n\nSELECT a.sales_month, avg(b.sales) as moving_avg\nFROM sales_months a\nJOIN retail_sales b \non b.sales_month between \n    a.sales_month - interval '11 months' and a.sales_month\n  and b.kind_of_business = 'Women''s clothing stores' \nGROUP BY 1\nORDER BY 1\nLIMIT 3;\n\n\n3 records\n\n\nsales_month\nmoving_avg\n\n\n\n\n1993-01-01\n2672.083\n\n\n1993-02-01\n2673.250\n\n\n1993-03-01\n2676.500\n\n\n\n\n\n\nsales_months &lt;-\n  retail_sales %&gt;%\n  filter(between(sales_month, \n                 as.Date('1993-01-01'), \n                 as.Date('2020-12-01'))) %&gt;%\n  distinct(sales_month)\n\nsales_months %&gt;%\n  mutate(month_start = sales_month - months(11)) %&gt;%\n  inner_join(retail_sales, \n             join_by(between(y$sales_month, x$month_start, x$sales_month)),\n                     suffix = c(\"\", \"_y\")) %&gt;%\n  filter(kind_of_business == \"Women's clothing stores\") %&gt;%\n  group_by(sales_month) %&gt;%\n  summarize(moving_avg = mean(sales, na.rm = TRUE)) %&gt;%\n  arrange(sales_month) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\nsales_month\nmoving_avg\n\n\n\n\n1993-01-01\n2672.083\n\n\n1993-02-01\n2673.250\n\n\n1993-03-01\n2676.500\n\n\n\n\n\n\n\n3.3.2 Calculating Cumulative Values\n\nSELECT sales_month, sales,\n  sum(sales) OVER w AS sales_ytd\nFROM retail_sales\nWHERE kind_of_business = 'Women''s clothing stores'\nWINDOW w AS (PARTITION BY date_part('year', sales_month) \n             ORDER BY sales_month)\nLIMIT 3;\n\n\n3 records\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n2017-01-01\n2454\n2454\n\n\n2017-02-01\n2763\n5217\n\n\n2017-03-01\n3485\n8702\n\n\n\n\n\n\nytd_sales &lt;-\n  retail_sales %&gt;%\n  filter(kind_of_business == \"Women's clothing stores\") %&gt;%\n  mutate(year = year(sales_month)) %&gt;%\n  group_by(year) %&gt;%\n  window_order(sales_month) %&gt;%\n  mutate(sales_ytd = cumsum(sales)) %&gt;%\n  ungroup() %&gt;%\n  select(sales_month, sales, sales_ytd) \n\nytd_sales %&gt;%\n  filter(month(sales_month) %in% c(1:3, 12)) %&gt;%\n  collect(n = 6) %&gt;%\n  kable()\n\n\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n2017-01-01\n2454\n2454\n\n\n2019-01-01\n2511\n2511\n\n\n2017-02-01\n2763\n5217\n\n\n2019-02-01\n2680\n5191\n\n\n2017-03-01\n3485\n8702\n\n\n2019-03-01\n3585\n8776\n\n\n\n\n\n\nfactor &lt;- 40/4.5\n\nytd_sales %&gt;%\n  filter(year(sales_month) %in% 2016:2020) %&gt;%\n  ggplot(aes(x = sales_month, y = sales_ytd)) +\n  geom_bar(stat = \"identity\") +\n  geom_line(aes(y = sales * factor, colour = I(\"blue\"))) +\n  scale_y_continuous(\n    \"Sales YTD\", \n    sec.axis = sec_axis(~ . / factor, name = \"Monthly Sales\")\n  )\n\n\n\n\n\nSELECT a.sales_month, a.sales,\n  sum(b.sales) AS sales_ytd\nFROM retail_sales a\nINNER JOIN retail_sales b ON \n date_part('year',a.sales_month) = date_part('year',b.sales_month)\n AND b.sales_month &lt;= a.sales_month\n AND b.kind_of_business = 'Women''s clothing stores'\nWHERE a.kind_of_business = 'Women''s clothing stores'\nGROUP BY 1,2;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n1992-12-01\n4416\n31815\n\n\n1993-12-01\n4170\n32350\n\n\n1992-11-01\n2946\n27399\n\n\n1993-11-01\n2923\n28180\n\n\n1992-10-01\n2755\n24453\n\n\n1993-10-01\n2713\n25257\n\n\n1992-09-01\n2560\n21698\n\n\n1993-09-01\n2622\n22544\n\n\n1992-08-01\n2657\n19138\n\n\n1993-08-01\n2626\n19922\n\n\n\n\n\n\nretail_sales_yr &lt;-\n  retail_sales %&gt;%\n  mutate(year = year(sales_month))\n  \nretail_sales_yr %&gt;%\n  filter(kind_of_business == \"Women's clothing stores\") %&gt;%\n  inner_join(retail_sales_yr, \n             join_by(year, kind_of_business,\n                     sales_month &gt;= sales_month),\n             suffix = c(\"\", \"_y\")) %&gt;%\n  group_by(sales_month, sales) %&gt;%\n  summarize(sales_ytd = sum(sales_y, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\n  filter(month(sales_month) %in% c(1:3, 12)) %&gt;%\n  collect(n = 6) %&gt;%\n  kable()\n\n\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n1992-02-01\n1991\n3864\n\n\n1993-02-01\n2005\n4128\n\n\n1994-02-01\n1970\n3756\n\n\n1992-03-01\n2403\n6267\n\n\n1993-03-01\n2442\n6570\n\n\n1994-03-01\n2560\n6316"
  },
  {
    "objectID": "chapter_3.html#analyzing-with-seasonality",
    "href": "chapter_3.html#analyzing-with-seasonality",
    "title": "3  Time Series Analysis",
    "section": "3.4 Analyzing with Seasonality",
    "text": "3.4 Analyzing with Seasonality\n\nretail_sales %&gt;%\n  filter(kind_of_business %in% c(\"Jewelry stores\",\n                                 \"Book stores\",\n                                 \"Grocery stores\")) %&gt;%\n  ggplot(aes(x = sales_month, y = sales)) +\n  geom_line() +\n  facet_wrap(vars(kind_of_business), nrow = 3, scales = \"free\")\n\n\n\n\n\n3.4.1 Period-over-Period Comparisons: YoY and MoM\n\nSELECT kind_of_business, sales_month, sales,\n  lag(sales_month) OVER w AS prev_month,\n  lag(sales) OVER w AS prev_month_sales\nFROM retail_sales\nWHERE kind_of_business = 'Book stores'\nWINDOW w AS (PARTITION BY kind_of_business ORDER BY sales_month)\n\n\nDisplaying records 1 - 10\n\n\nkind_of_business\nsales_month\nsales\nprev_month\nprev_month_sales\n\n\n\n\nBook stores\n1992-01-01\n790\nNA\nNA\n\n\nBook stores\n1992-02-01\n539\n1992-01-01\n790\n\n\nBook stores\n1992-03-01\n535\n1992-02-01\n539\n\n\nBook stores\n1992-04-01\n523\n1992-03-01\n535\n\n\nBook stores\n1992-05-01\n552\n1992-04-01\n523\n\n\nBook stores\n1992-06-01\n589\n1992-05-01\n552\n\n\nBook stores\n1992-07-01\n592\n1992-06-01\n589\n\n\nBook stores\n1992-08-01\n894\n1992-07-01\n592\n\n\nBook stores\n1992-09-01\n861\n1992-08-01\n894\n\n\nBook stores\n1992-10-01\n645\n1992-09-01\n861\n\n\n\n\n\n\nbooks_w_lag &lt;-\n  retail_sales %&gt;%\n  filter(kind_of_business == 'Book stores') %&gt;%\n  group_by(kind_of_business) %&gt;%\n  window_order(sales_month) %&gt;%\n  mutate(prev_month = lag(sales_month),\n         prev_month_sales = lag(sales)) %&gt;%\n  select(kind_of_business,\n         sales_month, sales,\n         prev_month, prev_month_sales)\n\nbooks_w_lag %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\nkind_of_business\nsales_month\nsales\nprev_month\nprev_month_sales\n\n\n\n\nBook stores\n1992-01-01\n790\nNA\nNA\n\n\nBook stores\n1992-02-01\n539\n1992-01-01\n790\n\n\nBook stores\n1992-03-01\n535\n1992-02-01\n539\n\n\n\n\n\n\nbooks_monthly &lt;-\n  books_w_lag %&gt;%\n  mutate(pct_growth = (sales / prev_month_sales - 1) * 100) %&gt;%\n  select(-prev_month, -prev_month_sales)\n\n\nbooks_yearly &lt;- \n  retail_sales %&gt;%\n  filter(kind_of_business == 'Book stores') %&gt;%\n  mutate(sales_year = year(sales_month)) %&gt;%\n  group_by(sales_year) %&gt;%\n  summarize(yearly_sales = sum(sales, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\n  window_order(sales_year) %&gt;%\n  mutate(prev_year_sales = lag(yearly_sales),\n         pct_growth = (yearly_sales/prev_year_sales - 1) * 100)\n\nbooks_yearly %&gt;%\n  collect(n = 3) %&gt;%\n  kable(digits = 2)\n\n\n\n\nsales_year\nyearly_sales\nprev_year_sales\npct_growth\n\n\n\n\n1992\n8327\nNA\nNA\n\n\n1993\n9108\n8327\n9.38\n\n\n1994\n10107\n9108\n10.97\n\n\n\n\n\n\nbooks_monthly %&gt;%\n  filter(!is.na(pct_growth)) %&gt;%\n  ggplot(aes(x = sales_month, y = pct_growth)) +\n  geom_line()\n\n\n\n\n\n\n3.4.2 Period-over-Period Comparisons: Same Month Versus Last Year\n\nbooks_lagged_year_month &lt;-\n  retail_sales %&gt;%\n  filter(kind_of_business == 'Book stores') %&gt;%\n  mutate(month = month(sales_month)) %&gt;%\n  group_by(month) %&gt;%\n  window_order(sales_month) %&gt;%\n  mutate(prev_year_month = lag(sales_month),\n         prev_year_sales = lag(sales)) %&gt;%\n  ungroup() %&gt;%\n  select(sales_month, sales, prev_year_month, prev_year_sales)\n\nbooks_lagged_year_month %&gt;%\n  filter(month(sales_month) &lt;= 2, \n         year(sales_month) &lt;= 1994) %&gt;%\n  arrange(sales_month) %&gt;%\n  collect(n = 6) %&gt;%\n  kable()\n\n\n\n\nsales_month\nsales\nprev_year_month\nprev_year_sales\n\n\n\n\n1992-01-01\n790\nNA\nNA\n\n\n1992-02-01\n539\nNA\nNA\n\n\n1993-01-01\n998\n1992-01-01\n790\n\n\n1993-02-01\n568\n1992-02-01\n539\n\n\n1994-01-01\n1053\n1993-01-01\n998\n\n\n1994-02-01\n635\n1993-02-01\n568\n\n\n\n\n\n\nbooks_lagged_year_month %&gt;%\n  mutate(dollar_diff = sales - prev_year_sales,\n         pct_diff = dollar_diff/prev_year_sales * 100) %&gt;%\n  select(-prev_year_month, -prev_year_sales) %&gt;%\n  filter(month(sales_month) == 1) %&gt;%\n  collect(n = 3) %&gt;%\n  kable(digits = 2)\n\n\n\n\nsales_month\nsales\ndollar_diff\npct_diff\n\n\n\n\n1992-01-01\n790\nNA\nNA\n\n\n1993-01-01\n998\n208\n26.33\n\n\n1994-01-01\n1053\n55\n5.51\n\n\n\n\n\n\nbooks_lagged_year_month %&gt;%\n  filter(!is.na(prev_year_sales)) %&gt;%\n  mutate(dollar_diff = sales - prev_year_sales,\n         pct_diff = dollar_diff/prev_year_sales * 100) %&gt;%\n  select(sales_month, sales, dollar_diff, pct_diff) %&gt;%\n  pivot_longer(-sales_month) %&gt;%\n  collect() %&gt;%\n  mutate(name = fct_inorder(name)) %&gt;%\n  ggplot(aes(x = sales_month, y = value)) +\n  geom_line() +\n  facet_wrap(. ~ name, nrow = 3, scales = \"free\")\n\n\n\n\nWith PostgreSQL, we would use to_char(sales_month,'Month'); with DuckDB, the equivalent is monthname(sales_month). To use an approach that works with either backend, we draw on arguments to the lubridate function month().\n\nsales_92_94 &lt;-\n  retail_sales %&gt;%\n  filter(kind_of_business == 'Book stores',\n         year(sales_month) %in% 1992:1994) %&gt;%\n  select(sales_month, sales) %&gt;%\n  mutate(month_number = month(sales_month),\n         month_name = month(sales_month, \n                            label = TRUE, abbr = FALSE),\n         year = as.integer(year(sales_month)))\n\nsales_92_94 %&gt;%\n  pivot_wider(id_cols = c(month_number, month_name),\n              names_from = year,\n              names_prefix = \"sales_\",\n              values_from = sales) %&gt;%\n  kable()\n\n\n\n\nmonth_number\nmonth_name\nsales_1992\nsales_1993\nsales_1994\n\n\n\n\n1\nJanuary\n790\n998\n1053\n\n\n2\nFebruary\n539\n568\n635\n\n\n3\nMarch\n535\n602\n634\n\n\n4\nApril\n523\n583\n610\n\n\n5\nMay\n552\n612\n684\n\n\n6\nJune\n589\n618\n724\n\n\n7\nJuly\n592\n607\n678\n\n\n8\nAugust\n894\n983\n1154\n\n\n9\nSeptember\n861\n903\n1022\n\n\n10\nOctober\n645\n669\n732\n\n\n11\nNovember\n642\n692\n772\n\n\n12\nDecember\n1165\n1273\n1409\n\n\n\n\n\n\nsales_92_94 %&gt;%\n  collect() %&gt;%\n  mutate(year = factor(year),\n         month_name = month(month_number, label = TRUE)) %&gt;%\n  ggplot(aes(x = month_name, y = sales, \n             group = year, colour = year)) +\n  geom_line()\n\n\n\n\n\n\n3.4.3 Comparing to Multiple Prior Periods\n\nprev_three &lt;-\n  retail_sales %&gt;%\n  filter(kind_of_business == 'Book stores') %&gt;%\n  mutate(month = month(sales_month)) %&gt;%\n  group_by(month) %&gt;%\n  window_order(sales_month) %&gt;%\n  mutate(prev_sales_1 = lag(sales, 1),\n         prev_sales_2 = lag(sales, 2),\n         prev_sales_3 = lag(sales, 3)) %&gt;%\n  ungroup()\n\nprev_three %&gt;%\n  filter(month == 1) %&gt;%\n  select(sales_month, sales, starts_with(\"prev_sales\")) %&gt;%\n  collect(n = 5) %&gt;%\n  kable()\n\n\n\n\nsales_month\nsales\nprev_sales_1\nprev_sales_2\nprev_sales_3\n\n\n\n\n1992-01-01\n790\nNA\nNA\nNA\n\n\n1993-01-01\n998\n790\nNA\nNA\n\n\n1994-01-01\n1053\n998\n790\nNA\n\n\n1995-01-01\n1308\n1053\n998\n790\n\n\n1996-01-01\n1373\n1308\n1053\n998\n\n\n\n\n\n\nprev_three %&gt;%\n  mutate(avg_prev_three = (prev_sales_1 + \n                         prev_sales_2 + \n                         prev_sales_3)/3,\n         pct_of_prev_3 = 100 * sales/avg_prev_three) %&gt;%\n  select(sales_month, sales, pct_of_prev_3) %&gt;%\n  filter(month(sales_month) == 1,\n         year(sales_month) %in% c(1995:1997, 2017:2019)) %&gt;%\n  collect(n = 10) %&gt;%\n  kable(digits = 2)\n\n\n\n\nsales_month\nsales\npct_of_prev_3\n\n\n\n\n1995-01-01\n1308\n138.12\n\n\n1996-01-01\n1373\n122.63\n\n\n1997-01-01\n1558\n125.17\n\n\n2017-01-01\n1386\n94.63\n\n\n2018-01-01\n1217\n84.95\n\n\n2019-01-01\n1004\n74.74\n\n\n\n\n\n\nprev_three_win &lt;-\n  retail_sales %&gt;%\n  filter(kind_of_business == 'Book stores') %&gt;%\n  mutate(month = month(sales_month)) %&gt;%\n  group_by(month) %&gt;%\n  window_order(sales_month) %&gt;%\n  window_frame(-3, -1) %&gt;%\n  mutate(avg_prev_three = mean(sales, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct_of_prev_3 = 100 * sales/avg_prev_three)\n\n\nprev_three_win %&gt;%\n  select(sales_month, sales, pct_of_prev_3) %&gt;%\n  filter(month(sales_month) == 1,\n         year(sales_month) %in% c(1995:1997, 2017:2019)) %&gt;%\n  collect(n = 10) %&gt;%\n  kable(digits = 2)\n\n\n\n\nsales_month\nsales\npct_of_prev_3\n\n\n\n\n1995-01-01\n1308\n138.12\n\n\n1996-01-01\n1373\n122.63\n\n\n1997-01-01\n1558\n125.17\n\n\n2017-01-01\n1386\n94.63\n\n\n2018-01-01\n1217\n84.95\n\n\n2019-01-01\n1004\n74.74"
  },
  {
    "objectID": "chapter_3.html#persistent-storage",
    "href": "chapter_3.html#persistent-storage",
    "title": "3  Time Series Analysis",
    "section": "3.5 Persistent storage",
    "text": "3.5 Persistent storage\nHaving created a database connection, we can write the local data frame to the database using (say) copy_to().\nWe could specify temporary = FALSE if we wanted the data to be there permanently.3\n\n3.5.1 Using PostgreSQL\n\n\n3.5.2 Read-only databases\nIn some cases, you will have access to a database, but no write privileges for that database. In such a case, copy_inline() can be useful.4 Note that it seems you cannot interrogate a table created using copy_inline() using SQL, though it will behave in most respects just like a table created using copy_to() when using dbplyr. It is useful to note that copy_inline() is probably not a good solution if your data are hundreds of thousands of rows or more because the table is effectively turned into literal SQL.\n\nretail_sales_alt &lt;- copy_inline(db, retail_sales_local)\n\n\n\n3.5.3 Closing the database connection\n\ndbDisconnect(db, shutdown=TRUE)\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_3.html#footnotes",
    "href": "chapter_3.html#footnotes",
    "title": "3  Time Series Analysis",
    "section": "",
    "text": "I requested a tweak to dplyr that would have avoided the need to do this, but my request was denied. Given how awesome dbplyr/dplyr is, I cannot complain.↩︎\nThough I will argue later that transitioning to dplyr/dbplyr is actually not difficult.↩︎\nObviously this would not make sense if db is a connection to an in-memory database.↩︎\nI requested this function for a common use case I have. Thank you to the dbplyr team for making it happen.↩︎"
  },
  {
    "objectID": "chapter_4.html#cohorts-a-useful-analysis-framework",
    "href": "chapter_4.html#cohorts-a-useful-analysis-framework",
    "title": "4  Cohorts",
    "section": "4.1 Cohorts: A Useful Analysis Framework",
    "text": "4.1 Cohorts: A Useful Analysis Framework\nChapter 4 examines the fascinating topic of cohorts, where a cohort is a group of observations (often people) who acquire a shared characteristic at (approximately) the same time. For example, children entering kindergarten in New Zealand in 2017 or the Harvard MBA Class of 2002.\nWhile cohort analysis has some attractive features, I guess that data do not often come in a format that facilitates such analysis. Instead, as is the case with the legislators data set studied in Chapter 4, the data analyst needs to rearrange the data to support cohort analysis.\nI found Chapter 4 a little confusing on a first pass through it.1 The chapter launches into some SQL code intended to create cohorts, but it’s a little unclear why we’re doing what we’re doing, and we quickly see that our cohort analysis does not make sense (e.g., we have more of our original cohort in period 5 than we had in period 4) and we must have done something wrong. I think I see the idea Cathy is going for here: one needs to think carefully about how to arrange the data to avoid subtle mistakes. The challenge I see is that it’s not obvious that everyone would make the same mistake and one is too deep in the weeds of the code to really see the forest for the trees.\nSo before I launch into the code, I will spend a little time thinking about things conceptually. I will start with a different example from that used in Chapter 4, but one that I think brings out some of the issues.\nFor some reason I associate cohort analysis with life expectancy. The people who are born today form a cohort and one might ask: How long do we expect members of this cohort to live? One often hears life expectancy statistics quoted as something like: “In Australia, a boy born in 2018–2020 can expect to live to the age of 81.2 years and a girl would be expected to live to 85.3 years compared to 51.1 for boys and 54.8 years for girls born in in 1891–1900.”2\nThe people who construct the life expectancies for the children must be veritable polymaths. They need to anticipate future developments in medical care and technology. Skin cancer is a significant cause of death in Australia, due to a mismatch between the median complexion and the intensity of the sun. But the analysts calculating life expectancy need to think about how medical technology is likely to affect rates of death from carcinoma in the future. I can imagine whole-body scanners a bit like the scanners in US airports that detect skin cancers before they become problematic. These analysts also need to understand how road safety will evolve. Will children today all be in driverless vehicles in fifty years time and will accidents then be a rarity? And what about war? The data analyst needs to be able to forecast the possibility of World War III breaking out and shortening life spans. Who are these people?\nOf course it seems unlikely these über-analysts exist. Rather they surely do something more prosaic. Here is my guess as to how life expectancies are constructed.3 I guess that the data analyst gathers data on cohorts notionally formed at some point in the past and then looks at survival rates for that cohort over some period, then aggregates those data into a life expectancy.\nFor example, the data analyst might gather data on people who turned 21 in 2018 and then data on whether those people surived to their 22nd birthday. The proportion of such people who make their 22nd birthday could be interpreted as a survival probability \\(p_{21}\\). Repeat that for each age-based cohort to get probabilities \\(\\left\\{p_i: i = 0, 1, \\dots, 119, 120 \\right\\}\\). Now to find the median life expectancy, we could calculate something like this:4\n\\[ \\left\\{j: \\arg \\min_{i} \\left(\\prod_{0}^{i} p_i\\right) \\leq \\frac{1}{2} \\right\\} \\] So we have a (fairly) well-defined procedure here. There are obviously some details to be worked out. For example, do we focus on one year (2018 in this case)? Or collect data over multiple years? Does it make sense to form cohorts by years? Or would grouping into larger cohorts (e.g., 20–25) make more sense? Do we identify people by birthdays? Or just use some kind of census date? (People who are 21 on 1 July might have just turned 21, or might be about to turn 22.)\nBut what exactly have we calculated? In a sense it’s a nonsensical number. Why would survival rates for 88-year-olds in 2018 be relevant for the life expectancy of newborns today, who will face a very different world when they turn 88 in 2111. First, perhaps the analysts really don’t calculate it in this way (though I’m doubtful they are polymaths). Second, even though it’s a “meaningless” number, it probably still has attractive properties, such as the ability to represent in a one or two numbers a lot about the quality of life in Australia.\nA final note is that it is not clear to me where the “51.1 for boys and 54.8 years for girls born in in 1891–1900” values come from. Are these the equivalent life expectancies calculated using data available around 1900? Or are these the observed lifespans of people born in 1891–1900? If the latter, how accurate were the former as estimates of these values?"
  },
  {
    "objectID": "chapter_4.html#the-legislators-data-set",
    "href": "chapter_4.html#the-legislators-data-set",
    "title": "4  Cohorts",
    "section": "4.2 The Legislators Data Set",
    "text": "4.2 The Legislators Data Set\nNow that we understand cohorts, let’s move onto the legislators data set.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(duckdb)\n\nOtherwise, the easiest way to get the legislators data set is to get the data from the GitHub site provided with Tanimura (2021).\n\nurl &lt;- paste0(\"https://raw.githubusercontent.com/cathytanimura/\",\n              \"sql_book/master/Chapter%204%3A%20Cohorts/\")\nlegislators_df &lt;- read_csv(paste0(url, \"legislators.csv\"),\n                           show_col_types = FALSE)\nlegislators_terms_df &lt;- read_csv(paste0(url, \"legislators_terms.csv\"),\n                                 show_col_types = FALSE)\n\n\nlegislators_df &lt;- read_csv(\"data/legislators.csv\",\n                           show_col_types = FALSE)\nlegislators_terms_df &lt;- read_csv(\"data/legislators_terms.csv\",\n                                 show_col_types = FALSE)\n\n\ndb &lt;- dbConnect(duckdb::duckdb())\nlegislators &lt;- copy_to(db, legislators_df, \"legislators\")\nlegislators_terms &lt;- copy_to(db, legislators_terms_df, \n                             \"legislators_terms\")"
  },
  {
    "objectID": "chapter_4.html#retention",
    "href": "chapter_4.html#retention",
    "title": "4  Cohorts",
    "section": "4.3 Retention",
    "text": "4.3 Retention\n\n4.3.1 SQL for a Basic Retention Curve\n\nSELECT id_bioguide, min(term_start) AS first_term\nFROM legislators_terms \nGROUP BY 1\nLIMIT 3;\n\n\n3 records\n\n\nid_bioguide\nfirst_term\n\n\n\n\nF000062\n1992-11-10\n\n\nT000464\n2007-01-04\n\n\nA000360\n2003-01-07\n\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1) \n\nSELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n  COUNT(DISTINCT a.id_bioguide) AS cohort_retained\nFROM first_terms a\nINNER JOIN legislators_terms b\nUSING (id_bioguide)\nGROUP BY 1\nORDER BY 1\nLIMIT 4;\n\n\n4 records\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n3600\n\n\n2\n3619\n\n\n3\n1831\n\n\n\n\n\n\nfirst_terms &lt;- \n  legislators_terms %&gt;%\n  group_by(id_bioguide) %&gt;%\n  summarize(first_term = min(term_start, na.rm = TRUE),\n            .groups = \"drop\")\n\ncohorts &lt;-\n  legislators_terms %&gt;%\n  inner_join(first_terms, by = \"id_bioguide\") %&gt;%\n  mutate(period = year(age(term_start, first_term))) %&gt;%\n  group_by(period) %&gt;%\n  summarize(cohort_retained = sql(\"count(distinct id_bioguide)\")) \n\ncohorts %&gt;%\n  arrange(period)\n\n\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n3600\n\n\n2\n3619\n\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts AS (\n  SELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n    COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b \n  USING (id_bioguide)\n  GROUP BY 1)\n  \nSELECT period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / \n    first_value(cohort_retained) OVER w AS prop_retained\nFROM cohorts\nWINDOW w AS (ORDER BY period)\nLIMIT 3;\n\n\n3 records\n\n\nperiod\ncohort_size\ncohort_retained\nprop_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n3600\n0.2875859\n\n\n2\n12518\n3619\n0.2891037\n\n\n\n\n\n\nretained_data &lt;-\n  cohorts %&gt;%\n  window_order(period) %&gt;%\n  mutate(cohort_size = first(cohort_retained)) %&gt;%\n  mutate(pct_retained = cohort_retained * 1.0/cohort_size) %&gt;%\n  select(period, cohort_size, cohort_retained, pct_retained) \n\nretained_data\n\n\n\n\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n3600\n0.2875859\n\n\n2\n12518\n3619\n0.2891037\n\n\n\n\n\n\nretained_data %&gt;%\n  ggplot(aes(x = period, y = pct_retained)) +\n  geom_line()\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts AS (\n  SELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n      COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  GROUP BY 1),\n  \nretained_data AS (\n  SELECT period,\n    first_value(cohort_retained) OVER w AS cohort_size,\n    cohort_retained,\n    cohort_retained * 1.0 / first_value(cohort_retained) OVER w AS pct_retained\n  FROM cohorts\n  WINDOW w AS (ORDER BY period))\n\nSELECT cohort_size,\n  max(CASE WHEN period = 0 THEN pct_retained END) AS yr0,\n  max(CASE WHEN period = 1 THEN pct_retained END) AS yr1,\n  max(CASE WHEN period = 2 THEN pct_retained END) AS yr2,\n  max(CASE WHEN period = 3 THEN pct_retained END) AS yr3,\n  max(CASE WHEN period = 4 THEN pct_retained END) AS yr4\nFROM retained_data\nGROUP BY 1;\n\n\n1 records\n\n\ncohort_size\nyr0\nyr1\nyr2\nyr3\nyr4\n\n\n\n\n12518\n1\n0.2875859\n0.2891037\n0.1462694\n0.2564307\n\n\n\n\n\n\nretained_data %&gt;%\n  select(period, pct_retained) %&gt;%\n  filter(period &lt;= 4) %&gt;%\n  collect() %&gt;%\n  arrange(period) %&gt;%\n  pivot_wider(names_from = period, \n              names_prefix = \"yr\",\n              values_from = pct_retained)\n\n\n\n\nyr0\nyr1\nyr2\nyr3\nyr4\n\n\n\n\n1\n0.2875859\n0.2891037\n0.1462694\n0.2564307\n\n\n\n\n\n\n\n4.3.2 Adjusting Time Series to Increase Retention Accuracy\nUse copy_inline() here if using a read-only database.\n\nyear_ends &lt;-\n  tibble(date = seq(as.Date('1770-12-31'), \n                    as.Date('2030-12-31'), \n                    by = \"1 year\")) %&gt;%\n  copy_to(db, ., overwrite = TRUE, name = \"year_ends\")\n\n\nWITH\n\nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT a.id_bioguide, a.first_term, b.term_start, b.term_end,\n  c.date,\n  date_part('year', age(c.date, a.first_term)) AS period\nFROM first_terms a\nJOIN legislators_terms b USING (id_bioguide)\nLEFT JOIN year_ends c\nON c.date BETWEEN b.term_start and b.term_end\nORDER BY id_bioguide\nLIMIT 3;\n\n\n3 records\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\ndate\nperiod\n\n\n\n\nA000001\n1951-01-03\n1951-01-03\n1953-01-03\n1951-12-31\n0\n\n\nA000001\n1951-01-03\n1951-01-03\n1953-01-03\n1952-12-31\n1\n\n\nA000002\n1947-01-03\n1971-01-21\n1973-01-03\n1972-12-31\n25\n\n\n\n\n\n\ncohorts &lt;-\n  first_terms %&gt;%\n  inner_join(legislators_terms, by = join_by(id_bioguide)) %&gt;%\n  left_join(year_ends, \n            by = join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  mutate(period = date_part('year', age(date, first_term))) %&gt;%\n  select(id_bioguide, first_term, term_start, term_end, date, period) \n\ncohorts\n\n\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\ndate\nperiod\n\n\n\n\nT000165\n2020-05-19\n2020-05-19\n2021-01-03\n2020-12-31\n0\n\n\nG000061\n2020-05-19\n2020-05-19\n2021-01-03\n2020-12-31\n0\n\n\nM000687\n1987-01-06\n2020-05-05\n2021-01-03\n2020-12-31\n33\n\n\n\n\n\n\nWITH\n\nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts_retained AS (        \n    SELECT coalesce(date_part('year', age(c.date, a.first_term)), 0) AS period,\n      COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n    FROM first_terms a\n    JOIN legislators_terms b\n    USING (id_bioguide)\n    LEFT JOIN year_ends c ON c.date BETWEEN b.term_start AND b.term_end\n    GROUP BY 1)\n    \nSELECT period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0/first_value(cohort_retained) OVER w AS pct_retained\nFROM cohorts_retained\nWINDOW w AS (ORDER BY period);\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n12328\n0.9848219\n\n\n2\n12518\n8166\n0.6523406\n\n\n3\n12518\n8069\n0.6445918\n\n\n4\n12518\n5862\n0.4682857\n\n\n5\n12518\n5795\n0.4629334\n\n\n6\n12518\n4361\n0.3483783\n\n\n7\n12518\n4339\n0.3466209\n\n\n8\n12518\n3521\n0.2812750\n\n\n9\n12518\n3485\n0.2783991\n\n\n\n\n\n\ncohorts_retained &lt;-\n  cohorts %&gt;%\n  mutate(period = coalesce(date_part('year', age(date, first_term)), 0)) %&gt;%\n  select(period, id_bioguide) %&gt;%\n  distinct() %&gt;%\n  group_by(period) %&gt;%\n  summarize(cohort_retained = n()) \n\npct_retained &lt;-\n  cohorts_retained %&gt;%\n  window_order(period) %&gt;%\n  mutate(cohort_size = first(cohort_retained),\n         cohort_retained = as.double(cohort_retained),\n         pct_retained = cohort_retained/cohort_size) \n\npct_retained %&gt;%\n  arrange(period)\n\n\n\n\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12328\n12518\n0.9848219\n\n\n2\n8166\n12518\n0.6523406\n\n\n\n\n\n\npct_retained %&gt;%\n  ggplot(aes(x = period, y = pct_retained)) + \n  geom_line()\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT id_bioguide, a.first_term, b.term_start,\n  CASE WHEN b.term_type = 'rep' THEN b.term_start + interval '2 years'\n       WHEN b.term_type = 'sen' THEN b.term_start + interval '6 years'\n  END AS term_end\nFROM first_terms a\nJOIN legislators_terms b USING (id_bioguide)\nLIMIT 3;\n\n\n3 records\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\n\n\n\n\nF000062\n1992-11-10\n2019-01-03\n2025-01-03\n\n\nT000464\n2007-01-04\n2019-01-03\n2025-01-03\n\n\nA000360\n2003-01-07\n2015-01-06\n2021-01-06\n\n\n\n\n\n\nfirst_terms %&gt;%\n  inner_join(legislators_terms, by = join_by(id_bioguide)) %&gt;%\n  mutate(term_end = \n           case_when(term_type == 'rep' ~ term_start + years(2),\n                     term_type == 'sen' ~ term_start + years(6))) %&gt;%\n  select(id_bioguide, first_term, term_start, term_end)\n\n\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\n\n\n\n\nF000062\n1992-11-10\n2019-01-03\n2025-01-03\n\n\nT000464\n2007-01-04\n2019-01-03\n2025-01-03\n\n\nA000360\n2003-01-07\n2015-01-06\n2021-01-06\n\n\n\n\n\nFor now, I have omitted the query after the paragraph starting “A second option …”.\n\n\n4.3.3 Cohorts Derived from the Time Series Itself\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT date_part('year', a.first_term) AS first_year,\n  COALESCE(date_part('year', age(c.date, a.first_term)), 0) AS period,\n  COUNT(DISTINCT a.id_bioguide) AS cohort_retained\nFROM first_terms a\nINNER JOIN legislators_terms b \nUSING (id_bioguide)\nLEFT JOIN year_ends c \nON c.date BETWEEN b.term_start AND b.term_end \nGROUP BY 1, 2\nORDER BY 1, 2\nLIMIT 3;\n\n\n3 records\n\n\nfirst_year\nperiod\ncohort_retained\n\n\n\n\n1789\n0\n89\n\n\n1789\n1\n89\n\n\n1789\n2\n57\n\n\n\n\n\n\nyr_cohort_retaineds &lt;-\n  first_terms %&gt;%\n  inner_join(legislators_terms, by = \"id_bioguide\") %&gt;%\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  mutate(first_year = year(first_term),\n         period = coalesce(year(age(date, first_term)), 0)) %&gt;%\n  group_by(first_year, period) %&gt;%\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\") \n\nyr_cohort_retaineds %&gt;%\n  arrange(first_year, period)\n\n\n\n\nfirst_year\nperiod\ncohort_retained\n\n\n\n\n1789\n0\n89\n\n\n1789\n1\n89\n\n\n1789\n2\n57\n\n\n\n\n\n\nyr_cohort_retaineds %&gt;%\n  group_by(first_year) %&gt;%\n  window_order(period) %&gt;%\n  mutate(cohort_size = first(cohort_retained),\n         pct_retained = 1.0 * cohort_retained/cohort_size) %&gt;%\n  select(first_year, period, cohort_size, cohort_retained, pct_retained) %&gt;%\n  arrange(first_year, period)\n\n\n\n\nfirst_year\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n1789\n0\n89\n89\n1.0000000\n\n\n1789\n1\n89\n89\n1.0000000\n\n\n1789\n2\n89\n57\n0.6404494\n\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n                \nfirst_centuries AS (\n  SELECT \n    date_part('century', a.first_term) AS first_century,\n    coalesce(date_part('year', age(c.date, a.first_term)), 0) AS period,\n    COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n  FROM first_terms AS a\n  INNER JOIN legislators_terms b \n  USING (id_bioguide)\n  LEFT JOIN year_ends c \n  ON c.date BETWEEN b.term_start AND b.term_end \n  GROUP BY 1, 2)\n\nSELECT first_century, period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / first_value(cohort_retained) OVER w AS prop_retained\nFROM first_centuries\nWINDOW w AS (PARTITION BY first_century ORDER BY period)\nORDER BY 1, 2\nLIMIT 3;\n\n\n3 records\n\n\nfirst_century\nperiod\ncohort_size\ncohort_retained\nprop_retained\n\n\n\n\n18\n0\n368\n368\n1.0000000\n\n\n18\n1\n368\n360\n0.9782609\n\n\n18\n2\n368\n242\n0.6576087\n\n\n\n\n\n\ncen_cohort_retaineds &lt;-\n  first_terms %&gt;%\n  inner_join(legislators_terms, by = \"id_bioguide\") %&gt;%\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  mutate(first_century = century(first_term),\n         period = coalesce(year(age(date, first_term)), 0)) %&gt;%\n  group_by(first_century, period) %&gt;%\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\")\n\n\ncen_pct_retaineds &lt;-\n  cen_cohort_retaineds %&gt;%\n  group_by(first_century) %&gt;%\n  window_order(period) %&gt;%\n  mutate(cohort_size = first(cohort_retained),\n         pct_retained = 1.0 * cohort_retained/cohort_size) %&gt;%\n  ungroup() %&gt;%\n  select(first_century, period, cohort_size, cohort_retained, pct_retained) \n\ncen_pct_retaineds %&gt;%\n  arrange(first_century, period)\n\n\n\n\nfirst_century\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n18\n0\n368\n368\n1.0000000\n\n\n18\n1\n368\n360\n0.9782609\n\n\n18\n2\n368\n242\n0.6576087\n\n\n\n\n\n\ncen_pct_retaineds %&gt;%\n  collect() %&gt;%\n  mutate(first_century = factor(first_century)) %&gt;%\n  ggplot(aes(x = period, y = pct_retained,\n             color = first_century,\n             linetype = first_century,\n             group = first_century)) + \n  geom_line()\n\n\n\n\n\nSELECT DISTINCT id_bioguide,\n  min(term_start) OVER w AS first_term,\n  first_value(state) OVER w AS first_state\nFROM legislators_terms \nWINDOW w AS (PARTITION BY id_bioguide ORDER BY term_start)\nORDER BY id_bioguide\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\nfirst_state\n\n\n\n\nA000001\n1951-01-03\nND\n\n\nA000002\n1947-01-03\nVA\n\n\nA000003\n1817-12-01\nGA\n\n\nA000004\n1843-12-04\nMA\n\n\nA000005\n1887-12-05\nTX\n\n\nA000006\n1868-01-01\nNC\n\n\nA000007\n1875-12-06\nMA\n\n\nA000008\n1857-12-07\nME\n\n\nA000009\n1973-01-03\nSD\n\n\nA000010\n1954-01-01\nNE\n\n\n\n\n\n\nfirst_states &lt;-\n  first_terms &lt;- \n  legislators_terms %&gt;%\n  group_by(id_bioguide) %&gt;%\n  window_order(term_start) %&gt;%\n  mutate(first_term = min(term_start, na.rm = TRUE),\n         first_state = first(state)) %&gt;%\n  ungroup() %&gt;%\n  select(id_bioguide, first_term, first_state) %&gt;%\n  distinct()\n\n\nWITH first_states AS (\n  SELECT DISTINCT id_bioguide,\n    min(term_start) OVER w AS first_term,\n    first_value(state) OVER w AS first_state\n  FROM legislators_terms \n  WINDOW w AS (PARTITION BY id_bioguide ORDER BY term_start)),\n\nstate_first_periods AS (\n  SELECT \n    first_state,\n    coalesce(date_part('year', age(c.date, a.first_term)), 0) AS period,\n    COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n  FROM first_states AS a\n  INNER JOIN legislators_terms b \n  USING (id_bioguide)\n  LEFT JOIN year_ends c \n  ON c.date BETWEEN b.term_start AND b.term_end \n  GROUP BY 1, 2)\n\nSELECT first_state, period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / first_value(cohort_retained) OVER w AS prop_retained\nFROM state_first_periods\nWINDOW w AS (PARTITION BY first_state ORDER BY period)\nORDER BY 1, 2\nLIMIT 3;\n\n\n3 records\n\n\nfirst_state\nperiod\ncohort_size\ncohort_retained\nprop_retained\n\n\n\n\nAK\n0\n19\n19\n1.0000000\n\n\nAK\n1\n19\n19\n1.0000000\n\n\nAK\n2\n19\n15\n0.7894737\n\n\n\n\n\n\nstate_first_periods &lt;-\n  first_states %&gt;%\n  inner_join(legislators_terms, by = \"id_bioguide\") %&gt;%\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  mutate(period = coalesce(year(age(date, first_term)), 0)) %&gt;%\n  group_by(first_state, period) %&gt;%\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\")\n\n\nstate_pct_retaineds &lt;- \n  state_first_periods %&gt;%\n  select(first_state, period, cohort_retained) %&gt;%\n  group_by(first_state) %&gt;%\n  window_order(period) %&gt;%\n  mutate(cohort_size = first(cohort_retained),\n         prop_retained = cohort_retained * 1.0 / cohort_size) %&gt;%\n  ungroup()\n\nstate_pct_retaineds %&gt;%\n  arrange(first_state, period) \n\n\n\n\nfirst_state\nperiod\ncohort_retained\ncohort_size\nprop_retained\n\n\n\n\nAK\n0\n19\n19\n1.0000000\n\n\nAK\n1\n19\n19\n1.0000000\n\n\nAK\n2\n15\n19\n0.7894737\n\n\n\n\n\n\ntop_5_states &lt;- c(\"NY\", \"PA\", \"OH\", \"IL\", \"MA\")\n\nstate_pct_retaineds %&gt;%\n  filter(first_state %in% top_5_states) %&gt;%\n  ggplot(aes(x = period, \n             y = prop_retained,\n             color = first_state,\n             linetype = first_state,\n             group = first_state)) + \n  geom_line()\n\n\n\n\n\n\n4.3.4 Defining the Cohort from a Separate Table\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT d.gender,\n  coalesce(date_part('year',age(c.date,a.first_term)),0) AS period,\n  count(distinct a.id_bioguide) AS cohort_retained\nFROM first_terms a\nJOIN legislators_terms b ON a.id_bioguide = b.id_bioguide \nLEFT JOIN year_ends c ON c.date BETWEEN b.term_start AND b.term_end \nINNER JOIN legislators d ON a.id_bioguide = d.id_bioguide\nGROUP BY 1,2\nORDER BY 2,1\nLIMIT 4;\n\n\n4 records\n\n\ngender\nperiod\ncohort_retained\n\n\n\n\nF\n0\n366\n\n\nM\n0\n12152\n\n\nF\n1\n349\n\n\nM\n1\n11979\n\n\n\n\n\n\ncohorts &lt;-\n  first_terms %&gt;%\n  inner_join(legislators_terms, by = \"id_bioguide\") %&gt;%\n  left_join(year_ends, join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  inner_join(legislators, by = \"id_bioguide\") %&gt;%\n  mutate(period = coalesce(year(age(date, first_term)), 0)) %&gt;%\n  group_by(gender, period) %&gt;%\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\")\n\ncohorts %&gt;%\n  arrange(period, gender) %&gt;%\n  collect(n = 4) \n\n\n\n\ngender\nperiod\ncohort_retained\n\n\n\n\nF\n0\n366\n\n\nM\n0\n12152\n\n\nF\n1\n349\n\n\nM\n1\n11979\n\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n  \ncohorts AS (\n  SELECT d.gender,\n    coalesce(date_part('year',age(c.date,a.first_term)),0) as period,\n    count(distinct a.id_bioguide) as cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  LEFT JOIN year_ends c on c.date between b.term_start and b.term_end \n  JOIN legislators d on a.id_bioguide = d.id_bioguide\n  GROUP BY 1, 2)\n  \nSELECT gender, period,\n  cohort_retained,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained * 1.0 / \n  first_value(cohort_retained) OVER w AS pct_retained\nFROM cohorts aa\nWINDOW w AS (partition by gender order by period)\nORDER BY 2, 1\nLIMIT 4;\n\n\n4 records\n\n\ngender\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\nF\n0\n366\n366\n1.0000000\n\n\nM\n0\n12152\n12152\n1.0000000\n\n\nF\n1\n349\n366\n0.9535519\n\n\nM\n1\n11979\n12152\n0.9857637\n\n\n\n\n\n\ncohorts %&gt;%\n  group_by(gender) %&gt;%\n  window_order(period) %&gt;%\n  mutate(cohort_size = first(cohort_retained)) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct_retained = 1.0 * cohort_retained / cohort_size) %&gt;%\n    arrange(period, gender) %&gt;%\n  collect(n = 4)\n\n\n\n\ngender\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\nF\n0\n366\n366\n1.0000000\n\n\nM\n0\n12152\n12152\n1.0000000\n\n\nF\n1\n349\n366\n0.9535519\n\n\nM\n1\n11979\n12152\n0.9857637\n\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n  \ncohorts AS (\n  SELECT d.gender,\n    coalesce(date_part('year',age(c.date, a.first_term)),0) as period,\n    count(distinct a.id_bioguide) as cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  LEFT JOIN year_ends c on c.date between b.term_start and b.term_end \n  JOIN legislators d on a.id_bioguide = d.id_bioguide\n  WHERE first_term BETWEEN '1917-01-01' AND '1999-12-31'\n  GROUP BY 1, 2)\n  \nSELECT gender, period,\n  cohort_retained,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained * 1.0 / \n  first_value(cohort_retained) OVER w AS pct_retained\nFROM cohorts aa\nWINDOW w AS (PARTITION BY gender ORDER BY period)\nORDER BY 2, 1\nLIMIT 4;\n\n\n4 records\n\n\ngender\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\nF\n0\n200\n200\n1.0000000\n\n\nM\n0\n3833\n3833\n1.0000000\n\n\nF\n1\n187\n200\n0.9350000\n\n\nM\n1\n3769\n3833\n0.9833029\n\n\n\n\n\n\ncohorts &lt;-\n  first_terms %&gt;%\n  filter(between(first_term, '1917-01-01', '1999-12-31')) %&gt;%\n  inner_join(legislators_terms, by = \"id_bioguide\") %&gt;%\n  left_join(year_ends, join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  inner_join(legislators, by = \"id_bioguide\") %&gt;%\n  mutate(period = coalesce(year(age(date, first_term)), 0)) %&gt;%\n  group_by(gender, period) %&gt;%\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\") \n\ncohorts %&gt;%\n  group_by(gender) %&gt;%\n  window_order(period) %&gt;%\n  mutate(cohort_size = first(cohort_retained)) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct_retained = 1.0 * cohort_retained / cohort_size) %&gt;%\n  arrange(period, gender) %&gt;%\n  collect(n = 4)\n\n\n\n\ngender\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\nF\n0\n200\n200\n1.0000000\n\n\nM\n0\n3833\n3833\n1.0000000\n\n\nF\n1\n187\n200\n0.9350000\n\n\nM\n1\n3769\n3833\n0.9833029\n\n\n\n\n\n\n\n4.3.5 Dealing with Sparse Cohorts\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS term_start,\n  FROM legislators_terms\n  GROUP BY 1),\n\nfirst_states AS (\n  SELECT id_bioguide, gender,\n    term_start AS first_term, \n    state AS first_state\n  FROM first_terms\n  INNER JOIN legislators_terms\n  USING (id_bioguide, term_start)\n  INNER JOIN legislators\n  USING (id_bioguide)\n  WHERE term_start between '1917-01-01' and '1999-12-31'),\n  \ncohorts AS (\n  SELECT first_state, gender,\n    coalesce(date_part('year', age(date, first_term)), 0) AS period,\n    COUNT(DISTINCT id_bioguide) AS cohort_retained\n  FROM first_states\n  INNER JOIN legislators_terms\n  USING (id_bioguide)\n  LEFT JOIN year_ends\n  ON date BETWEEN term_start AND term_end\n  GROUP BY 1, 2, 3),\n\nperiods AS (\n  SELECT generate_series as period \n   FROM generate_series(0, 20, 1)),\n\ncohort_sizes AS (\n  SELECT gender, first_state,\n    COUNT(DISTINCT id_bioguide) AS cohort_size\n  FROM first_states\n  GROUP BY 1, 2),\n  \npct_retaineds AS (\n  SELECT gender, first_state, period, \n    cohort_size,\n    coalesce(cohort_retained, 0) AS cohort_retained,\n    coalesce(cohort_retained, 0) * 1.0 / cohort_size AS pct_retained\n  FROM cohort_sizes\n  CROSS JOIN periods\n  LEFT JOIN cohorts\n  USING (gender, first_state, period))\n  \nSELECT gender, first_state, cohort_size,\n  max(case when period = 0 then pct_retained end) as yr0,\n  max(case when period = 2 then pct_retained end) as yr2,\n  max(case when period = 4 then pct_retained end) as yr4,\n  max(case when period = 6 then pct_retained end) as yr6,\n  max(case when period = 8 then pct_retained end) as yr8,\n  max(case when period = 10 then pct_retained end) as yr10\nFROM pct_retaineds\nWHERE first_state IN ('AL', 'AR', 'CA')\nGROUP BY 1, 2, 3\nORDER BY 1, 2\nLIMIT 3;\n\n\n3 records\n\n\ngender\nfirst_state\ncohort_size\nyr0\nyr2\nyr4\nyr6\nyr8\nyr10\n\n\n\n\nF\nAL\n3\n1\n0.00\n0.0\n0.00\n0.00\n0.00\n\n\nF\nAR\n5\n1\n0.80\n0.2\n0.40\n0.40\n0.40\n\n\nF\nCA\n25\n1\n0.92\n0.8\n0.64\n0.68\n0.68\n\n\n\n\n\n\nfirst_terms &lt;- \n  legislators_terms %&gt;%\n  group_by(id_bioguide) %&gt;%\n  summarize(term_start = min(term_start, na.rm = TRUE),\n            .groups = \"drop\")\n\nfirst_states &lt;-\n  first_terms %&gt;%\n  inner_join(legislators_terms, join_by(id_bioguide, term_start)) %&gt;%\n  inner_join(legislators, by = \"id_bioguide\") %&gt;%\n  rename(first_term = term_start,\n         first_state = state) %&gt;%\n  filter(between(first_term, '1917-01-01', '1999-12-31')) %&gt;%\n  select(id_bioguide, gender, first_term, first_state)\n\n\ncohorts &lt;-\n  first_states %&gt;% \n  inner_join(legislators_terms, by = \"id_bioguide\") %&gt;%\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  mutate(period = coalesce(year(age(date, first_term)), 0)) %&gt;%\n  group_by(first_state, gender, period) %&gt;%\n  summarize(cohort_retained = n_distinct(id_bioguide), \n            .groups = \"drop\")\n\nFrom the query below, it seems we’re having an issue whereby legislators are “returning from the dead” in a sense.\n\ncohorts %&gt;% \n  filter(first_state == \"AR\", gender == \"F\") %&gt;% \n  select(period, cohort_retained) %&gt;%\n  arrange(period) %&gt;%\n  collect(n = 8)\n\n\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n5\n\n\n1\n5\n\n\n2\n4\n\n\n3\n4\n\n\n4\n1\n\n\n5\n1\n\n\n6\n2\n\n\n7\n2\n\n\n\n\n\nDoes this mean something is wrong with our query? Let’s check.\nOf course, unlike cohorts of living people, dropping out of the cohort of legislators does not prevent you from returning later on. We can pull out a portion of the query creating cohorts and take a closer look. We first get the id_bioguide values for the female representatives from Arkansas (AR) who are around in period == 4. We then look at some data from legislators and legislators_terms for these two representatives.\n\nweird_id_bioguides &lt;-\n  first_states %&gt;% \n  filter(first_state == \"AR\", gender == \"F\") %&gt;%\n  inner_join(legislators_terms, by = \"id_bioguide\") %&gt;%\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  mutate(period = coalesce(year(age(date, first_term)), 0)) %&gt;%\n  filter(period &gt;= 4) %&gt;%\n  select(id_bioguide) %&gt;%\n  distinct() %&gt;%\n  pull()\n\nlegislators %&gt;%\n  filter(id_bioguide %in% weird_id_bioguides) %&gt;%\n  inner_join(legislators_terms, by = \"id_bioguide\") %&gt;%\n  select(id_bioguide, full_name, term_type, term_start, term_end) %&gt;%\n  arrange(id_bioguide, term_start) %&gt;%\n  collect()\n\n\n\n\n\n\n\n\n\n\n\nid_bioguide\nfull_name\nterm_type\nterm_start\nterm_end\n\n\n\n\nC000138\nHattie Wyatt Caraway\nsen\n1931-12-07\n1933-03-03\n\n\nC000138\nHattie Wyatt Caraway\nsen\n1933-03-09\n1939-01-03\n\n\nC000138\nHattie Wyatt Caraway\nsen\n1939-01-03\n1945-01-03\n\n\nL000035\nBlanche Lambert Lincoln\nrep\n1993-01-05\n1995-01-03\n\n\nL000035\nBlanche Lambert Lincoln\nrep\n1995-01-04\n1997-01-03\n\n\nL000035\nBlanche Lambert Lincoln\nsen\n1999-01-06\n2005-01-03\n\n\nL000035\nBlanche Lambert Lincoln\nsen\n2005-01-04\n2011-01-03\n\n\n\n\n\nFrom the above, it seems that Blanche Lambert Lincoln would have dropped out of the cohort in periods 4 and 5, then returned in 6 and 7.\nTo aid this kind of “debugging” of our queries, we could easily have retained some underlying data in cohorts. For example, by added cohort_ids = array_agg(id_bioguide) to the query, we can easily interrogate cohorts for the underlying legislator IDs.\n\ncohorts &lt;-\n  first_states %&gt;% \n  inner_join(legislators_terms, by = \"id_bioguide\") %&gt;%\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  mutate(period = coalesce(year(age(date, first_term)), 0)) %&gt;%\n  group_by(first_state, gender, period) %&gt;%\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            cohort_ids = array_agg(id_bioguide),\n            .groups = \"drop\")\n\n\ncohorts %&gt;%\n  filter(first_state == \"AR\", gender == \"F\", \n         between(period, 3, 8)) %&gt;%\n  mutate(cohort_ids = as.character(cohort_ids)) %&gt;%\n  select(period, cohort_retained, cohort_ids) %&gt;%\n  arrange(period) %&gt;%\n  collect()\n\n\n\n\nperiod\ncohort_retained\ncohort_ids\n\n\n\n\n3\n4\n[L000035, C000138, W000634, O000061]\n\n\n4\n1\n[C000138]\n\n\n5\n1\n[C000138]\n\n\n6\n2\n[L000035, C000138]\n\n\n7\n2\n[L000035, C000138]\n\n\n8\n2\n[L000035, C000138]\n\n\n\n\n\n\nperiods &lt;- \n  tibble(period = 1:20) %&gt;%\n  copy_to(db, .)\n\n\ncohort_sizes &lt;-\n  first_states %&gt;%\n  group_by(gender, first_state) %&gt;%\n  summarize(cohort_size = n_distinct(id_bioguide),\n            .groups = \"drop\")\n\n\npct_retaineds &lt;-\n  cohort_sizes %&gt;%\n  cross_join(periods) %&gt;%\n  left_join(cohorts, \n            join_by(gender, first_state, period)) %&gt;%\n  mutate(cohort_retained = coalesce(cohort_retained, 0),\n         pct_retained = cohort_retained * 1.0/ cohort_size) %&gt;%\n  select(gender, first_state, period, cohort_size,\n         cohort_retained, pct_retained)\n\n\npct_retaineds %&gt;%\n  filter(period %in% c(2, 4, 6, 8, 10),\n         first_state %in% c('AL', 'AR', 'CA')) %&gt;%\n  select(gender, first_state, cohort_size, period, pct_retained) %&gt;%\n  pivot_wider(names_from = \"period\",\n              names_prefix = \"yr\",\n              values_from = \"pct_retained\") %&gt;%\n  arrange(gender, first_state)\n\n\n\n\ngender\nfirst_state\ncohort_size\nyr2\nyr4\nyr6\nyr8\nyr10\n\n\n\n\nF\nAL\n3\n0.00\n0.0\n0.00\n0.00\n0.00\n\n\nF\nAR\n5\n0.80\n0.2\n0.40\n0.40\n0.40\n\n\nF\nCA\n25\n0.92\n0.8\n0.64\n0.68\n0.68\n\n\n\n\n\n\npct_retaineds %&gt;%\n  filter(first_state %in% c('AL', 'AR', 'CA')) %&gt;%\n  ggplot(aes(x = period, y = pct_retained, \n             colour = gender, group = gender)) +\n  geom_line() +\n  facet_wrap(vars(first_state), ncol = 1)\n\n\n\n\n\n\n4.3.6 Defining Cohorts from Dates other than the First Date\n\nWITH first_terms AS (\n  SELECT DISTINCT id_bioguide, term_type,\n    '2000-01-01'::date AS first_term,\n    min(term_start) AS min_start\n  FROM legislators_terms \n  WHERE term_start &lt;= '2000-12-31' and term_end &gt;= '2000-01-01'\n  GROUP BY 1, 2, 3),\n  \ncohort_dates AS (\n  SELECT id_bioguide, date\n  FROM legislators_terms\n  LEFT JOIN year_ends ON date BETWEEN term_start AND term_end),\n  \ncohorts AS (\n  SELECT term_type,\n    coalesce(date_part('year', age(date, first_term)), 0) AS period,\n    COUNT(DISTINCT id_bioguide) AS cohort_retained\n  FROM first_terms\n  JOIN cohort_dates\n  USING (id_bioguide)\n  GROUP BY 1, 2)\n  \nSELECT term_type, period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / \n    first_value(cohort_retained) OVER w AS pct_retained\nFROM cohorts\nWHERE period &gt;= 0\nWINDOW w AS (PARTITION BY term_type ORDER BY period)\nORDER BY 2, 1\nLIMIT 4;\n\n\n4 records\n\n\nterm_type\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\nrep\n0\n440\n440\n1.0000000\n\n\nsen\n0\n101\n101\n1.0000000\n\n\nrep\n1\n440\n392\n0.8909091\n\n\nsen\n1\n101\n89\n0.8811881\n\n\n\n\n\n\nmin_year &lt;- 2000L\n\nfirst_terms &lt;-\n  legislators_terms %&gt;%\n  filter(year(term_start) &lt;= min_year, \n         year(term_end) &gt;= min_year) %&gt;%\n  mutate(first_term = as.Date(paste0(min_year, '-01-01'))) %&gt;%\n  group_by(id_bioguide, term_type, first_term) %&gt;%\n  summarize(min_start = min(term_start, na.rm = TRUE),\n            .groups = \"drop\")\n\n\ncohort_dates &lt;-\n  legislators_terms %&gt;%\n  left_join(year_ends, \n            join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  filter(year(date) &gt;= min_year) %&gt;%\n  select(id_bioguide, date)\n\n\ncohorts &lt;-\n  first_terms %&gt;%\n  inner_join(cohort_dates, by = \"id_bioguide\") %&gt;%\n  mutate(period = coalesce(year(age(date, first_term)), 0)) %&gt;%\n  group_by(term_type, period) %&gt;%\n  summarize(cohort_retained = n_distinct(id_bioguide),\n            .groups = \"drop\")\n\n\npct_retained_2000 &lt;-\n  cohorts %&gt;% \n  group_by(term_type) %&gt;%\n  window_order(period) %&gt;%\n  mutate(cohort_size = first(cohort_retained),\n         pct_retained = cohort_retained * 1.0 / cohort_size) %&gt;%\n  select(term_type, period, cohort_size, cohort_retained,\n         pct_retained)\n\npct_retained_2000 %&gt;%\n  arrange(period, term_type) %&gt;%\n  collect(n = 4)\n\n# A tibble: 4 × 5\n# Groups:   term_type [2]\n  term_type period cohort_size cohort_retained pct_retained\n  &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 rep            0         439             439        1    \n2 sen            0         101             101        1    \n3 rep            1         439             392        0.893\n4 sen            1         101              89        0.881\n\n\n\npct_retained_2000 %&gt;%\n  filter(period &lt;= 20) %&gt;%\n  mutate(pct_retained = pct_retained * 100) %&gt;%\n  ggplot(aes(x = period, y = pct_retained,\n             colour = term_type, group = term_type)) +\n  geom_line()"
  },
  {
    "objectID": "chapter_4.html#related-cohort-analyses",
    "href": "chapter_4.html#related-cohort-analyses",
    "title": "4  Cohorts",
    "section": "4.4 Related Cohort Analyses",
    "text": "4.4 Related Cohort Analyses\n\n4.4.1 Survivorship\n\nWITH first_centuries AS (\n   SELECT id_bioguide,\n    date_part('century', min(term_start)) AS first_century,\n    min(term_start) AS first_term,\n    max(term_start) AS last_term,\n    date_part('year', age(max(term_start), min(term_start))) AS tenure\n  FROM legislators_terms\n  GROUP BY 1)\n  \nSELECT first_century,\n  count(distinct id_bioguide) as cohort_size,\n  count(distinct case when tenure &gt;= 10 then id_bioguide end) as survived_10,\n  count(distinct case when tenure &gt;= 10 then id_bioguide end) * 1.0 \n / count(distinct id_bioguide) as pct_survived_10\nFROM first_centuries\nGROUP BY 1\nORDER BY 1;\n\n\n4 records\n\n\nfirst_century\ncohort_size\nsurvived_10\npct_survived_10\n\n\n\n\n18\n368\n83\n0.2255435\n\n\n19\n6299\n892\n0.1416098\n\n\n20\n5091\n1853\n0.3639756\n\n\n21\n760\n119\n0.1565789\n\n\n\n\n\n\nWITH first_centuries AS (\n  SELECT id_bioguide,\n    date_part('century', min(term_start)) AS first_century,\n    count(term_start) AS total_terms\n  FROM legislators_terms\n  GROUP BY 1)\n  \nSELECT first_century,\n  COUNT(DISTINCT id_bioguide) AS cohort_size,\n  COUNT(DISTINCT CASE WHEN total_terms &gt;= 5 THEN id_bioguide END) AS survived_5,\n  COUNT(DISTINCT CASE WHEN total_terms &gt;= 5 THEN id_bioguide END) * 1.0\n    / count(distinct id_bioguide) AS pct_survived_5_terms\nFROM first_centuries\nGROUP BY 1\nORDER BY 1;\n\n\n4 records\n\n\nfirst_century\ncohort_size\nsurvived_5\npct_survived_5_terms\n\n\n\n\n18\n368\n63\n0.1711957\n\n\n19\n6299\n711\n0.1128751\n\n\n20\n5091\n2153\n0.4229032\n\n\n21\n760\n205\n0.2697368\n\n\n\n\n\n\nWITH first_centuries AS (\n  SELECT id_bioguide,\n    date_part('century',min(term_start)) AS first_century,\n    count(term_start) AS total_terms\n  FROM legislators_terms\n  GROUP BY 1),\n\nterms AS (\n  SELECT generate_series as terms \n  FROM generate_series(1, 20, 1))\n  \nSELECT first_century,\n  terms,\n  count(distinct id_bioguide) as cohort,\n  count(distinct case when total_terms &gt;= terms then id_bioguide end) as cohort_survived,\n  count(distinct case when total_terms &gt;= terms then id_bioguide end) * 1.0 \n / count(distinct id_bioguide) as pct_survived\nFROM first_centuries\nCROSS JOIN terms\nGROUP BY 1, 2\nORDER BY first_century, terms\nLIMIT 3;\n\n\n3 records\n\n\nfirst_century\nterms\ncohort\ncohort_survived\npct_survived\n\n\n\n\n18\n1\n368\n368\n1.0000000\n\n\n18\n2\n368\n249\n0.6766304\n\n\n18\n3\n368\n153\n0.4157609\n\n\n\n\n\n\nfirst_centuries &lt;-\n  legislators_terms %&gt;%\n  group_by(id_bioguide) %&gt;%\n  summarize(first_century = century(min(term_start, na.rm = TRUE)),\n            total_terms = n())\n\n\nterms &lt;- \n  tibble(terms = 1:20) %&gt;%\n  copy_to(db, ., name = \"terms\")\n\n\nfirst_centuries %&gt;%\n  cross_join(terms) %&gt;%\n  mutate(survived_id = if_else(total_terms &gt;= terms, id_bioguide, NA)) %&gt;%\n  group_by(first_century, terms) %&gt;%\n  summarize(cohort = n_distinct(id_bioguide),\n            cohort_survived = n_distinct(survived_id), \n            .groups = \"drop\") %&gt;%\n  mutate(pct_survived = 1.0 * cohort_survived / cohort) %&gt;%\n  arrange(first_century, terms)\n\n\n\n\nfirst_century\nterms\ncohort\ncohort_survived\npct_survived\n\n\n\n\n18\n1\n368\n368\n1.0000000\n\n\n18\n2\n368\n249\n0.6766304\n\n\n18\n3\n368\n153\n0.4157609\n\n\n\n\n\n\n\n4.4.2 Returnship, or Repeat Purchase Behaviour\n\nWITH rep_first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  GROUP BY 1)\n      \nSELECT date_part('century', first_term)::int AS cohort_century,\n  count(id_bioguide) AS reps\nFROM rep_first_terms\nGROUP BY 1\nORDER BY 1\n\n\n4 records\n\n\ncohort_century\nreps\n\n\n\n\n18\n299\n\n\n19\n5773\n\n\n20\n4481\n\n\n21\n683\n\n\n\n\n\n\nWITH rep_first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  GROUP BY 1)\n\nSELECT date_part('century', first_term) AS cohort_century,\n  count(id_bioguide) as reps\nFROM rep_first_terms\nGROUP BY 1\nORDER BY 1;\n\n\n4 records\n\n\ncohort_century\nreps\n\n\n\n\n18\n299\n\n\n19\n5773\n\n\n20\n4481\n\n\n21\n683\n\n\n\n\n\n\nWITH rep_first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  GROUP BY 1),\n\nnum_reps AS (\n  SELECT date_part('century', first_term) as cohort_century,\n    count(id_bioguide) as reps\n  FROM rep_first_terms\n  GROUP BY 1),\n  \nsens AS (\n  SELECT date_part('century',b.first_term) as cohort_century,\n    count(distinct b.id_bioguide) as rep_and_sen\n  FROM rep_first_terms b\n  JOIN legislators_terms c on b.id_bioguide = c.id_bioguide\n  and c.term_type = 'sen' and c.term_start &gt; b.first_term\n  GROUP BY 1)\n  \nSELECT aa.cohort_century,\n  bb.rep_and_sen * 1.0 / aa.reps as pct_rep_and_sen\nFROM num_reps aa\nLEFT JOIN sens bb \non aa.cohort_century = bb.cohort_century;\n\n\n4 records\n\n\ncohort_century\npct_rep_and_sen\n\n\n\n\n20\n0.0566838\n\n\n21\n0.0366032\n\n\n18\n0.1906355\n\n\n19\n0.0569894\n\n\n\n\n\n\nWITH \n\nrep_first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  GROUP BY 1),\n  \nreps AS (\n  SELECT date_part('century',a.first_term) as cohort_century,\n    count(id_bioguide) as reps\n  FROM rep_first_terms a\n  WHERE first_term &lt;= '2009-12-31'\n  GROUP BY 1),\n  \nsens AS (\n  SELECT date_part('century',b.first_term) as cohort_century,\n    count(distinct b.id_bioguide) as rep_and_sen\n  FROM rep_first_terms b\n  JOIN legislators_terms c \n  on b.id_bioguide = c.id_bioguide\n    and c.term_type = 'sen' and c.term_start &gt; b.first_term\n    WHERE age(c.term_start, b.first_term) &lt;= interval '10 years'\n  GROUP BY 1)\n  \nSELECT aa.cohort_century\n,bb.rep_and_sen * 1.0 / aa.reps as pct_rep_and_sen\nFROM reps aa\nLEFT JOIN sens bb \nUSING (cohort_century);\n\n\n4 records\n\n\ncohort_century\npct_rep_and_sen\n\n\n\n\n20\n0.0348137\n\n\n21\n0.0763636\n\n\n18\n0.0969900\n\n\n19\n0.0244240\n\n\n\n\n\n\nWITH \n\nrep_first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  GROUP BY 1),\n  \nreps AS (\n  SELECT date_part('century', first_term) as cohort_century,\n    count(id_bioguide) as reps\n  FROM rep_first_terms\n  WHERE first_term &lt;= '2009-12-31'\n  GROUP BY 1),\n  \nsen_terms AS (\n  SELECT id_bioguide, term_type, term_start\n  FROM legislators_terms\n  WHERE term_type = 'sen'),\n  \ngaps AS (\n  SELECT id_bioguide, first_term, \n    age(term_start, first_term) AS gap\n  FROM rep_first_terms \n  JOIN sen_terms \n  USING (id_bioguide)\n  WHERE term_start &gt; first_term),\n  \ngap_indicators AS (\n  SELECT date_part('century', first_term) as cohort_century,\n    count(distinct case when gap &lt;= interval '5 years' then id_bioguide end) as rep_and_sen_5_yrs,\n    count(distinct case when gap &lt;= interval '10 years' then id_bioguide end) as rep_and_sen_10_yrs,\n    count(distinct case when gap &lt;= interval '15 years' then id_bioguide end) as rep_and_sen_15_yrs\n  FROM gaps\n  GROUP BY 1)\n\nSELECT cohort_century::int as cohort_century,\n  round(rep_and_sen_5_yrs * 1.0 / reps, 4) as pct_5_yrs,\n  round(rep_and_sen_10_yrs * 1.0 / reps, 4) as pct_10_yrs,\n  round(rep_and_sen_15_yrs * 1.0 / reps, 4) as pct_15_yrs\nFROM reps\nLEFT JOIN gap_indicators\nUSING (cohort_century)\nORDER BY cohort_century\n\n\n4 records\n\n\ncohort_century\npct_5_yrs\npct_10_yrs\npct_15_yrs\n\n\n\n\n18\n0.0502\n0.0970\n0.1438\n\n\n19\n0.0088\n0.0244\n0.0409\n\n\n20\n0.0100\n0.0348\n0.0478\n\n\n21\n0.0400\n0.0764\n0.0873\n\n\n\n\n\n\n\n4.4.3 Cumulative Calculations\n\nWITH types AS (\n  SELECT distinct id_bioguide,\n    first_value(term_type) over w as first_type,\n    min(term_start) over w as first_term,\n    (min(term_start) over w) + interval '10 years' as first_plus_10\n  FROM legislators_terms\n  WINDOW w AS (partition by id_bioguide order by term_start))\n  \nSELECT date_part('century',a.first_term)::int as century,\n  first_type,\n  count(distinct a.id_bioguide) as cohort,\n  count(b.term_start) as terms\nFROM types a\nLEFT JOIN legislators_terms b \non a.id_bioguide = b.id_bioguide and \n  b.term_start between a.first_term and a.first_plus_10\nGROUP BY 1, 2;\n\n\n8 records\n\n\ncentury\nfirst_type\ncohort\nterms\n\n\n\n\n20\nrep\n4473\n16203\n\n\n19\nrep\n5744\n12165\n\n\n20\nsen\n618\n1008\n\n\n19\nsen\n555\n795\n\n\n18\nsen\n71\n101\n\n\n21\nsen\n77\n118\n\n\n21\nrep\n683\n2203\n\n\n18\nrep\n297\n760\n\n\n\n\n\n\nWITH a AS (\n  SELECT DISTINCT id_bioguide,\n    first_value(term_type) OVER w AS first_type,\n    min(term_start) OVER w AS first_term\n  FROM legislators_terms\n  WINDOW w AS (PARTITION BY id_bioguide ORDER BY term_start)),\n  \ncohort_terms AS (\n  SELECT date_part('century',a.first_term)::int AS century,\n    first_type,\n    COUNT(distinct a.id_bioguide) AS cohort,\n    COUNT(b.term_start) as terms,\n    COUNT(b.term_start) * 1.0 / COUNT(DISTINCT a.id_bioguide) AS cohort_term\n  FROM a\n  LEFT JOIN legislators_terms b \n  on a.id_bioguide = b.id_bioguide AND \n    b.term_start BETWEEN a.first_term AND a.first_term + interval '10 years'\n  GROUP BY 1, 2)\n  \nSELECT century,\n  max(case when first_type = 'rep' then cohort end) as rep_cohort,\n  max(case when first_type = 'rep' then cohort_term end) as avg_rep_terms,\n  max(case when first_type = 'sen' then cohort end) as sen_cohort,\n  max(case when first_type = 'sen' then cohort_term end) as avg_sen_terms\nFROM cohort_terms\nGROUP BY 1;\n\n\n4 records\n\n\ncentury\nrep_cohort\navg_rep_terms\nsen_cohort\navg_sen_terms\n\n\n\n\n20\n4473\n3.622401\n618\n1.631068\n\n\n19\n5744\n2.117862\n555\n1.432432\n\n\n21\n683\n3.225476\n77\n1.532468\n\n\n18\n297\n2.558923\n71\n1.422535"
  },
  {
    "objectID": "chapter_4.html#cross-section-analysis-through-a-cohort-lens",
    "href": "chapter_4.html#cross-section-analysis-through-a-cohort-lens",
    "title": "4  Cohorts",
    "section": "4.5 Cross-Section Analysis, Through a Cohort Lens",
    "text": "4.5 Cross-Section Analysis, Through a Cohort Lens\n\nSELECT b.date, count(distinct a.id_bioguide) as legislators\nFROM legislators_terms a\nJOIN year_ends b on b.date between a.term_start and a.term_end\nand b.date &lt;= '2020-12-31'\nGROUP BY 1\n;\n\n\nDisplaying records 1 - 10\n\n\ndate\nlegislators\n\n\n\n\n1960-12-31\n550\n\n\n1959-12-31\n549\n\n\n1962-12-31\n555\n\n\n1958-12-31\n544\n\n\n1957-12-31\n545\n\n\n1955-12-31\n538\n\n\n1956-12-31\n541\n\n\n1954-12-31\n549\n\n\n1953-12-31\n543\n\n\n1952-12-31\n550\n\n\n\n\n\n\nSELECT b.date\n,date_part('century',first_term)::int as century\n,count(distinct a.id_bioguide) as legislators\nFROM legislators_terms a\nJOIN year_ends b on b.date between a.term_start and a.term_end\nand b.date &lt;= '2020-12-31'\nJOIN\n(\n        SELECT id_bioguide, min(term_start) as first_term\n        FROM legislators_terms\n        GROUP BY 1\n) c on a.id_bioguide = c.id_bioguide        \nGROUP BY 1,2\n;\n\n\nDisplaying records 1 - 10\n\n\ndate\ncentury\nlegislators\n\n\n\n\n1853-12-31\n19\n315\n\n\n1843-12-31\n19\n295\n\n\n1847-12-31\n19\n301\n\n\n1842-12-31\n19\n320\n\n\n1819-12-31\n19\n238\n\n\n1829-12-31\n19\n273\n\n\n1879-12-31\n19\n390\n\n\n1899-12-31\n19\n470\n\n\n1911-12-31\n20\n404\n\n\n1913-12-31\n20\n476\n\n\n\n\n\n\nSELECT date\n,century\n,legislators\n,sum(legislators) over (partition by date) as cohort\n,legislators * 100.0 / sum(legislators) over (partition by date) as pct_century\nFROM\n(\n        SELECT b.date\n        ,date_part('century',first_term)::int as century\n        ,count(distinct a.id_bioguide) as legislators\n        FROM legislators_terms a\n        JOIN year_ends b on b.date between a.term_start and a.term_end\nand b.date &lt;= '2020-01-01'\n        JOIN\n        (\n                SELECT id_bioguide, min(term_start) as first_term\n                FROM legislators_terms\n                GROUP BY 1\n        ) c on a.id_bioguide = c.id_bioguide        \n        GROUP BY 1,2\n) a\nORDER BY 1,2\n;\n\n\nDisplaying records 1 - 10\n\n\ndate\ncentury\nlegislators\ncohort\npct_century\n\n\n\n\n1789-12-31\n18\n89\n89\n100\n\n\n1790-12-31\n18\n95\n95\n100\n\n\n1791-12-31\n18\n99\n99\n100\n\n\n1792-12-31\n18\n101\n101\n100\n\n\n1793-12-31\n18\n141\n141\n100\n\n\n1794-12-31\n18\n140\n140\n100\n\n\n1795-12-31\n18\n145\n145\n100\n\n\n1796-12-31\n18\n150\n150\n100\n\n\n1797-12-31\n18\n152\n152\n100\n\n\n1798-12-31\n18\n155\n155\n100\n\n\n\n\n\n\nWITH \n\nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) as first_term\n                FROM legislators_terms\n                GROUP BY 1),\n                \naa AS (\n  SELECT b.date,\n    date_part('century',first_term)::int as century,\n    count(distinct a.id_bioguide) as legislators\n  FROM legislators_terms a\n  JOIN year_ends b \n  ON b.date between a.term_start and a.term_end\n    and b.date &lt;= '2020-01-01'\n  JOIN first_terms c \n  USING (id_bioguide)\n  GROUP BY 1,2) \n                \nSELECT date,\n  coalesce(sum(case when century = 18 then legislators end) * 100.0 / sum(legislators),0) as pct_18,\n  coalesce(sum(case when century = 19 then legislators end) * 100.0 / sum(legislators),0) as pct_19,\n  coalesce(sum(case when century = 20 then legislators end) * 100.0 / sum(legislators),0) as pct_20,\n  coalesce(sum(case when century = 21 then legislators end) * 100.0 / sum(legislators),0) as pct_21\nFROM aa\nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\ndate\npct_18\npct_19\npct_20\npct_21\n\n\n\n\n1789-12-31\n100\n0\n0\n0\n\n\n1790-12-31\n100\n0\n0\n0\n\n\n1791-12-31\n100\n0\n0\n0\n\n\n1792-12-31\n100\n0\n0\n0\n\n\n1793-12-31\n100\n0\n0\n0\n\n\n1794-12-31\n100\n0\n0\n0\n\n\n1795-12-31\n100\n0\n0\n0\n\n\n1796-12-31\n100\n0\n0\n0\n\n\n1797-12-31\n100\n0\n0\n0\n\n\n1798-12-31\n100\n0\n0\n0\n\n\n\n\n\n\nSELECT id_bioguide, date,\n  count(date) over (partition by id_bioguide order by date rows between unbounded preceding and current row) as cume_years\nFROM (\n  SELECT distinct id_bioguide, date\n  FROM legislators_terms\n  JOIN year_ends \n  ON date between term_start and term_end\n    and date &lt;= '2020-01-01');\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\ndate\ncume_years\n\n\n\n\nA000009\n1973-12-31\n1\n\n\nA000009\n1974-12-31\n2\n\n\nA000009\n1975-12-31\n3\n\n\nA000009\n1976-12-31\n4\n\n\nA000009\n1977-12-31\n5\n\n\nA000009\n1978-12-31\n6\n\n\nA000009\n1979-12-31\n7\n\n\nA000009\n1980-12-31\n8\n\n\nA000009\n1981-12-31\n9\n\n\nA000009\n1982-12-31\n10\n\n\n\n\n\n\nSELECT date, cume_years,\n  count(distinct id_bioguide) as legislators\nFROM\n(\n    SELECT id_bioguide, date\n    ,count(date) over (partition by id_bioguide order by date rows between unbounded preceding and current row) as cume_years\n    FROM\n    (\n        SELECT distinct a.id_bioguide, b.date\n        FROM legislators_terms a\n        JOIN year_ends b \n        on b.date between a.term_start and a.term_end and b.date &lt;= '2020-01-01'\n        GROUP BY 1,2\n    ) aa\n) aaa\nGROUP BY 1,2\n;\n\n\nDisplaying records 1 - 10\n\n\ndate\ncume_years\nlegislators\n\n\n\n\n2004-12-31\n4\n50\n\n\n1921-12-31\n9\n50\n\n\n2018-12-31\n2\n61\n\n\n1959-12-31\n9\n43\n\n\n1895-12-31\n3\n103\n\n\n1878-12-31\n4\n119\n\n\n1921-12-31\n7\n66\n\n\n1814-12-31\n2\n112\n\n\n1959-12-31\n5\n49\n\n\n1955-12-31\n3\n82\n\n\n\n\n\n\nWITH aa AS (\n  SELECT DISTINCT a.id_bioguide, b.date\n  FROM legislators_terms a\n  JOIN year_ends b \n    on b.date BETWEEN a.term_start AND a.term_end AND b.date &lt;= '2020-01-01'\n  GROUP BY 1,2),\n  \naaa AS (\n  SELECT id_bioguide, date,\n    count(date) OVER (PARTITION BY id_bioguide ORDER BY date) AS cume_years\n  FROM aa),\n  \naaaa AS (\n  SELECT date, cume_years,\n    COUNT(DISTINCT id_bioguide) as legislators\n  FROM aaa\n  GROUP BY 1,2)\nSELECT date, count(*) as tenures\nFROM aaaa\nGROUP BY 1;\n\n\nDisplaying records 1 - 10\n\n\ndate\ntenures\n\n\n\n\n1973-12-31\n35\n\n\n1974-12-31\n36\n\n\n1975-12-31\n36\n\n\n1976-12-31\n36\n\n\n1977-12-31\n34\n\n\n1978-12-31\n34\n\n\n1979-12-31\n31\n\n\n1980-12-31\n32\n\n\n1981-12-31\n31\n\n\n1982-12-31\n32\n\n\n\n\n\n\nWITH term_dates AS (\n  SELECT distinct a.id_bioguide, b.date\n  FROM legislators_terms a\n  JOIN year_ends b \n  on b.date between a.term_start and a.term_end and b.date &lt;= '2020-01-01'),\n\ncum_term_dates AS (\n  SELECT id_bioguide, date,\n    count(date) over (partition by id_bioguide order by date rows between unbounded preceding and current row) as cume_years\n  FROM term_dates),\n  \ncum_term_bands AS (\n  SELECT date,\n    case when cume_years &lt;= 4 then '1 to 4'\n         when cume_years &lt;= 10 then '5 to 10'\n        when cume_years &lt;= 20 then '11 to 20'\n         else '21+' end as tenure,\n    count(distinct id_bioguide) as legislators\n  FROM cum_term_dates\n  GROUP BY 1,2)\n  \nSELECT date, tenure,\n  legislators * 100.0 / sum(legislators) over w as pct_legislators \nFROM cum_term_bands\nWINDOW w AS (partition by date)\nORDER BY date DESC;\n\n\nDisplaying records 1 - 10\n\n\ndate\ntenure\npct_legislators\n\n\n\n\n2019-12-31\n1 to 4\n29.98138\n\n\n2019-12-31\n11 to 20\n20.11173\n\n\n2019-12-31\n5 to 10\n32.02980\n\n\n2019-12-31\n21+\n17.87710\n\n\n2018-12-31\n5 to 10\n33.76623\n\n\n2018-12-31\n11 to 20\n21.33581\n\n\n2018-12-31\n21+\n19.29499\n\n\n2018-12-31\n1 to 4\n25.60297\n\n\n2017-12-31\n21+\n19.33085\n\n\n2017-12-31\n1 to 4\n24.53532\n\n\n\n\n\n\nterm_dates &lt;-\n  legislators_terms %&gt;%\n  inner_join(year_ends %&gt;% filter(date &lt;= '2020-01-01'),\n             join_by(between(y$date, x$term_start, x$term_end))) %&gt;%\n  distinct(id_bioguide, date) \n\n\ncum_term_dates &lt;-\n  term_dates %&gt;%\n  group_by(id_bioguide) %&gt;%\n  window_order(date) %&gt;%\n  window_frame(-Inf, 0) %&gt;%\n  mutate(cume_years = n()) %&gt;%\n  ungroup() %&gt;%\n  select(id_bioguide, date, cume_years) \n\n\ncum_term_bands &lt;-\n  cum_term_dates %&gt;% \n  mutate(tenure = case_when(cume_years &lt;= 4 ~ '1 to 4',\n                            cume_years &lt;= 10 ~ '5 to 10',\n                            cume_years &lt;= 20 ~ '11 to 20',\n                            TRUE ~ '21+')) %&gt;%\n  group_by(date, tenure) %&gt;%\n  summarize(legislators = n_distinct(id_bioguide),\n            .groups = \"drop\") \n\n\ntotal_legs &lt;-\n  cum_term_bands %&gt;%\n  group_by(date) %&gt;%\n  summarize(num_legs = sum(legislators, na.rm = TRUE),\n            .groups = \"drop\")\n\n\ncum_term_bands %&gt;%\n  inner_join(total_legs, by = \"date\") %&gt;%\n  mutate(pct_legislators = legislators * 100.0 / num_legs) %&gt;%\n  select(date, tenure, pct_legislators) %&gt;%\n  arrange(desc(date)) %&gt;%\n  collect(n = 8)\n\n\n\n\ndate\ntenure\npct_legislators\n\n\n\n\n2019-12-31\n5 to 10\n32.02980\n\n\n2019-12-31\n1 to 4\n29.98138\n\n\n2019-12-31\n21+\n17.87710\n\n\n2019-12-31\n11 to 20\n20.11173\n\n\n2018-12-31\n1 to 4\n25.60297\n\n\n2018-12-31\n21+\n19.29499\n\n\n2018-12-31\n5 to 10\n33.76623\n\n\n2018-12-31\n11 to 20\n21.33581\n\n\n\n\n\n\ndbDisconnect(db, shutdown = TRUE)\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_4.html#footnotes",
    "href": "chapter_4.html#footnotes",
    "title": "4  Cohorts",
    "section": "",
    "text": "In writing this sentence, I am still working through the chapter.↩︎\nSee here for the source for these data.↩︎\nI made no effort to research how they are constructed because (1) I am lazy and (2) my imagined version is probably better for my current purposes.↩︎\nThere are some details I’m glossing over here, such as the fact that there will be some \\(j\\) where the cumulative survival probability is just above one-half, but where that for \\(j + 1\\) is just below one-half, so some interpolation will be required.↩︎"
  },
  {
    "objectID": "chapter_5.html#why-text-analysis-with-sql",
    "href": "chapter_5.html#why-text-analysis-with-sql",
    "title": "5  Text Analysis",
    "section": "5.1 Why Text Analysis with SQL",
    "text": "5.1 Why Text Analysis with SQL\nI would actually reframe this question as “why store text data in a database?” and offer different reasons from those offered in Tanimura (2021). To structure my answer I will use a representative textual analysis problem (really set of problems) I’ve managed in the past.\nPublic companies routinely communicate with investors or their representatives through conference calls. Most public companies hold conference calls when they announce their earnings results for a quarter or year. The typical earnings conference call starts with a presentation of results by management, typically the CEO or CEO, followed by the “Q&A” portion of the call during which call participants can ask questions of management Apart from representatives of the company, the typical participant in a conference call is an equity analyst. Equity analysts typically cover a relatively small numbers of companies, typically in a single industry, and provide insights and investment recommendations and related to their covered companies and industries.\nAnalyst recommendations usually come from valuation analyses that draw on projections of future financial performance. An analyst’s valuation model is usually constructed using a spreadsheet and to some extent an analyst’s questions on a conference call will seek information that can be used for model inputs.\nTranscripts of conference calls are collected by a number of data providers, who presumably supply them to various users, including investors and academic researchers. I have used transcripts of conference calls in my own research. The data provider in my case provided a continuous stream of transcripts in XML files. Each call is contained in its own XML file with a file name that indicates the unique identifier of the call. Some elements of the call are contained in structured XML, but the bulk of the data in a call are in a single unstructured XML field.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\n\n\ndb &lt;- dbConnect(duckdb::duckdb(), read_only = TRUE)\n\ncsv_to_db &lt;- function(file) {\n  df &lt;- read_csv(file, show_col_types = FALSE)\n  dbWriteTable(db, df, name = \"ufo\", append = TRUE)\n}\n\nfiles &lt;- list.files(\"data\", \"ufo.*csv\",\n                    full.names = TRUE)\nres &lt;- lapply(files, csv_to_db)\n\n\nufo &lt;- tbl(db, \"ufo\")\n\nufo %&gt;%\n  mutate(length = length(sighting_report)) %&gt;%\n  ggplot(aes(x = length)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\ninitcap &lt;- function(x) {\n  if_else(nchar(x) &gt; 0,\n          paste0(toupper(substr(x, 1, 1)),\n                 tolower(substr(x, 2, nchar(x)))), x)\n}\n\n\nufo &lt;- \n  tbl(db, \"ufo\") %&gt;% \n  mutate(id = row_number())\n\nregex &lt;- paste0(\"Occurred :\\\\s*(.*)\\\\s*\",\n                \"Reported:\\\\s*(.* [AP]M).*?\\\\s*\",\n                \"Posted:\\\\s*(.*)\\\\s*\",\n                \"Location:\\\\s*(.*)\\\\s*\",\n                \"Shape:\\\\s*(.*)\\\\s*\",\n                \"Duration:\\\\s*(.*)\\\\s*\")\n\nregex2 &lt;- paste0(\"^(.*?)\",\n                 \"(?:\\\\s*\\\\(Entered as :\\\\s*(.*)\\\\))?\\\\s*$\")\n\nufo_extracted &lt;- \n  ufo %&gt;%\n  mutate(occurred_plus = regexp_extract(sighting_report, regex, 1L),\n         reported = regexp_extract(sighting_report, regex, 2L),\n         posted = regexp_extract(sighting_report, regex, 3L),\n         location = regexp_extract(sighting_report, regex, 4L),\n         shape = regexp_extract(sighting_report, regex, 5L),\n         duration = regexp_extract(sighting_report, regex, 6L)) %&gt;%\n  select(id, occurred_plus:duration) %&gt;%\n  mutate(occurred_raw = regexp_extract(occurred_plus, regex2, 1L),\n         entered = regexp_extract(occurred_plus, regex2, 1L)) %&gt;%\n  select(-occurred_plus) %&gt;%\n  mutate(location_clean = regexp_replace(location, \n                                         \"(outside of|close to)\", \"near\")) %&gt;%\n  mutate(reported = \n           case_when(reported == '' ~ NA,\n                     nchar(reported) &lt; 8 ~ NA,\n                     regexp_matches(reported, \"[AP]M\") ~\n                       strptime(reported, \"%m/%d/%Y %I:%M:%S %p\"),\n                     TRUE ~ strptime(reported, \"%m/%d/%Y %H:%M:%S\")),\n         occurred = \n           case_when(occurred_raw == '' ~ NA,\n                     nchar(occurred_raw) &lt; 8 ~ NA,\n                     regexp_matches(occurred_raw, \"^[0-9]+/[0-9]+/[0-9]{4}$\") ~\n                       strptime(occurred_raw, \"%m/%d/%Y\"),\n                     regexp_matches(occurred_raw, \"[AP]M\") ~\n                       strptime(occurred_raw, \"%m/%d/%Y %I:%M:%S %p\"),\n                     regexp_matches(occurred_raw, \"[0-9]{2}:[0-9]{2}:[0-9]{2}\") ~\n                       strptime(occurred_raw, \"%m/%d/%Y %H:%M:%S\"),\n                     TRUE ~ strptime(occurred_raw, \"%m/%d/%Y %H:%M\")),\n         posted = if_else(posted == '', NA, \n                          as.Date(strptime(posted, \"%m/%d/%Y\")))) %&gt;%\n  collect() %&gt;%\n  mutate(shape = initcap(shape)) \n\n\nufo_extracted %&gt;%\n  ggplot(aes(y = fct_rev(fct_infreq(shape)))) +\n  geom_bar() +\n  ylab(\"Shape\")\n\n\n\n\n\nufo_extracted %&gt;%\n  filter(!is.na(occurred_raw)) %&gt;%\n  count(occurred_raw) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(occurred_raw = fct_rev(fct_inorder(as.character(occurred_raw)))) %&gt;%\n  ggplot(aes(y = occurred_raw, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nufo_extracted %&gt;%\n  count(duration) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(duration = fct_rev(fct_inorder(duration))) %&gt;%\n  ggplot(aes(y = duration, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nufo_extracted %&gt;%\n  count(location) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(location = fct_rev(fct_inorder(location))) %&gt;%\n  ggplot(aes(y = location, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_6.html#graphing-to-find-anomalies-visually",
    "href": "chapter_6.html#graphing-to-find-anomalies-visually",
    "title": "6  Anomaly Detection",
    "section": "6.1 Graphing to find anomalies visually",
    "text": "6.1 Graphing to find anomalies visually\n\nearthquakes %&gt;%\n  filter(!is.na(mag)) %&gt;%\n  ggplot(aes(x = mag)) +\n  geom_histogram(breaks = seq(-10, 10, 0.1))\n\n\n\n\n\nearthquakes %&gt;%\n  filter(!is.na(mag),\n         between(mag, 7.2, 9.5)) %&gt;%\n  ggplot(aes(x = mag)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\nearthquakes %&gt;%\n  filter(!is.na(mag),\n         between(mag, 7.2, 9.5)) %&gt;%\n  ggplot(aes(x = mag)) +\n  geom_bar() +\n  scale_x_binned(breaks = seq(7.2, 9.5, 0.1))\n\n\n\n\n\nearthquakes %&gt;%\n  filter(!is.na(mag), !is.na(depth)) %&gt;%\n  distinct(mag, depth) %&gt;%\n  ggplot(aes(x = mag, y = depth)) +\n  geom_point(size = 0.1, colour = \"blue\")\n\n\n\n\n\nearthquakes %&gt;%\n  filter(!is.na(mag), !is.na(depth)) %&gt;%\n  filter(between(mag, 4, 7), depth &lt;= 50) %&gt;%\n  ggplot(aes(x = mag, y = depth)) +\n  geom_count(color = \"blue\")\n\n\n\n\n\njapan_quakes &lt;-\n  earthquakes %&gt;%\n  filter(!is.na(mag), !is.na(depth)) %&gt;%\n  filter(grepl(\"Japan\", place)) \n\njapan_quakes %&gt;%\n  ggplot(aes(y = mag)) +\n  geom_boxplot(width = 0.5)\n\n\n\n\n\njapan_quakes %&gt;%\n  summarize(p25 = quantile(mag, probs = 0.25, na.rm = TRUE),\n            p50 = quantile(mag, probs = 0.50, na.rm = TRUE),\n            p75 = quantile(mag, probs = 0.75, na.rm = TRUE)) %&gt;%\n  mutate(iqr = (p75 - p25) * 1.5,\n         lower_whisker = p25 - (p75 - p25) * 1.5,\n         upper_whisker = p75 + (p75 - p25) * 1.5) %&gt;%\n  kable()\n\n\n\n\np25\np50\np75\niqr\nlower_whisker\nupper_whisker\n\n\n\n\n4.3\n4.5\n4.7\n0.6\n3.7\n5.3\n\n\n\n\n\n\njapan_quakes %&gt;%\n  select(mag, time) %&gt;%\n  collect() %&gt;%\n  mutate(year = as.factor(year(time))) %&gt;%\n  ggplot(aes(y = mag, x = year, group = year)) +\n  geom_boxplot()"
  },
  {
    "objectID": "chapter_6.html#forms-of-anomalies",
    "href": "chapter_6.html#forms-of-anomalies",
    "title": "6  Anomaly Detection",
    "section": "6.2 Forms of Anomalies",
    "text": "6.2 Forms of Anomalies\n\n6.2.1 Anomalous Values\n\nearthquakes %&gt;%\n  filter(mag &gt;= 1.08) %&gt;%\n  group_by(mag) %&gt;%\n  summarize(count = n()) %&gt;%\n  arrange(mag) %&gt;%\n  collect(n = 5) %&gt;%\n  kable()\n\n\n\n\nmag\ncount\n\n\n\n\n1.08\n3863\n\n\n1.08\n1\n\n\n1.09\n3712\n\n\n1.10\n39728\n\n\n1.11\n3674\n\n\n\n\n\n\nearthquakes %&gt;%\n  filter(depth &gt; 600) %&gt;%\n  group_by(net) %&gt;%\n  summarize(count = n()) %&gt;%\n  arrange(net) %&gt;%\n  collect(n = 5) %&gt;%\n  kable()\n\n\n\n\nnet\ncount\n\n\n\n\nus\n1215\n\n\n\n\n\n\nearthquakes %&gt;%\n  filter(depth &gt; 600) %&gt;%\n  group_by(place) %&gt;%\n  summarize(count = n()) %&gt;%\n  arrange(place) %&gt;%\n  collect(n = 5) %&gt;%\n  kable()\n\n\n\n\nplace\ncount\n\n\n\n\n100km NW of Ndoi Island, Fiji\n1\n\n\n100km SSW of Ndoi Island, Fiji\n1\n\n\n100km SW of Ndoi Island, Fiji\n1\n\n\n101km ENE of Suva, Fiji\n1\n\n\n101km NNE of Ndoi Island, Fiji\n1\n\n\n\n\n\n\nearthquakes %&gt;%\n  filter(depth &gt; 600) %&gt;%\n  mutate(place_name = case_when(grepl(' of ', place) ~\n                                  split_part(place, ' of ', 2L),\n                                TRUE ~ place)) %&gt;%\n  group_by(place_name) %&gt;%\n  summarize(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\nplace_name\ncount\n\n\n\n\nNdoi Island, Fiji\n487\n\n\nFiji region\n186\n\n\nLambasa, Fiji\n140\n\n\n\n\n\n\nearthquakes %&gt;%\n  summarize(distinct_types = n_distinct(type),\n            distinct_lower = n_distinct(lower(type))) %&gt;%\n  kable()\n\n\n\n\ndistinct_types\ndistinct_lower\n\n\n\n\n25\n24\n\n\n\n\n\n\n\n6.2.2 Anomalous Counts or Frequencies\nWhy use date_trunc('year',time)::date as earthquake_year?\n\nSELECT EXTRACT(year FROM time) AS earthquake_year, \n  COUNT(*) AS earthquakes\nFROM earthquakes\nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nearthquake_year\nearthquakes\n\n\n\n\n2010\n122322\n\n\n2011\n107397\n\n\n2012\n105693\n\n\n2013\n114368\n\n\n2014\n135247\n\n\n2015\n122914\n\n\n2016\n122420\n\n\n2017\n130622\n\n\n2018\n179304\n\n\n2019\n171116\n\n\n\n\n\n\nearthquakes %&gt;%\n  mutate(earthquake_year = as.character(year(time))) %&gt;%\n  group_by(earthquake_year) %&gt;%\n  summarize(earthquakes = n()) %&gt;%\n  ggplot(aes(x = earthquake_year, y = earthquakes)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n2\n\nearthquakes %&gt;%\n  mutate(earthquake_year = as.character(year(time))) %&gt;%\n  select(earthquake_year) %&gt;%\n  ggplot(aes(x = earthquake_year)) +\n  geom_bar()\n\n\nearthquakes %&gt;%\n  mutate(earthquake_month = floor_date(time, \"month\")) %&gt;%\n  group_by(earthquake_month) %&gt;%\n  summarize(earthquakes = n(), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = earthquake_month, y = earthquakes)) +\n  geom_line()\n\n\n\n\nFrom the book, “it turns out that the increase in earthquakes starting in 2017 can be at least partially explained by the status field. The status indicates whether the event has been reviewed by a human (‘reviewed’) or was directly posted by a system without review (‘automatic’).” This can be seen in the following plot.3\n\nearthquakes %&gt;%\n  mutate(earthquake_month = floor_date(time, \"month\")) %&gt;%\n  group_by(earthquake_month, status) %&gt;%\n  summarize(earthquakes = n(), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = earthquake_month, y = earthquakes, color = status)) +\n  geom_line()\n\n\n\n\n\nearthquakes %&gt;%\n  filter(mag &gt;= 6) %&gt;%\n  group_by(place) %&gt;%\n  summarize(earthquakes = n(), .groups = \"drop\") %&gt;%\n  arrange(desc(earthquakes)) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\nplace\nearthquakes\n\n\n\n\nnear the east coast of Honshu, Japan\n52\n\n\noff the east coast of Honshu, Japan\n34\n\n\nVanuatu\n28\n\n\n\n\n\nFor the next query, it seems easy enough to just put the result in a plot.\n\nearthquakes %&gt;%\n  filter(mag &gt;= 6) %&gt;%\n  mutate(place = if_else(grepl(' of ', place),\n                         split_part(place, ' of ', 2L), \n                         place)) %&gt;%\n  count(place, name = \"earthquakes\") %&gt;%\n  arrange(desc(earthquakes)) %&gt;%\n  collect(n = 10) %&gt;%\n  ggplot(aes(y = fct_inorder(place), \n             x = earthquakes)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n6.2.3 Anomalies from the Absence of Data"
  },
  {
    "objectID": "chapter_6.html#handling-anomalies",
    "href": "chapter_6.html#handling-anomalies",
    "title": "6  Anomaly Detection",
    "section": "6.3 Handling Anomalies",
    "text": "6.3 Handling Anomalies\n\n6.3.1 Investigation\n\n\n6.3.2 Removal\n\nearthquakes %&gt;%\n  filter(!mag %in% c(-9,-9.99)) %&gt;%\n  select(time, mag, type) %&gt;%\n  collect(n = 10) %&gt;%\n  kable()\n\n\n\n\ntime\nmag\ntype\n\n\n\n\n2010-01-01 00:10:43\n1.60\nearthquake\n\n\n2010-01-01 00:16:49\n2.20\nearthquake\n\n\n2010-01-01 00:16:54\n1.10\nearthquake\n\n\n2010-01-01 00:29:29\n1.40\nice quake\n\n\n2010-01-01 00:30:16\n0.75\nearthquake\n\n\n2010-01-01 00:31:40\n1.00\nearthquake\n\n\n2010-01-01 00:34:29\n0.46\nearthquake\n\n\n2010-01-01 00:55:05\n2.40\nearthquake\n\n\n2010-01-01 00:57:31\n0.60\nearthquake\n\n\n2010-01-01 01:09:53\n0.20\nearthquake\n\n\n\n\n\n\nearthquakes %&gt;%\n  summarize(avg_mag = avg(mag),\n            avg_mag_adjusted = avg(if_else(mag &gt; -9, mag, NA))) %&gt;%\n  kable()\n\n\n\n\navg_mag\navg_mag_adjusted\n\n\n\n\n1.625101\n1.627323\n\n\n\n\n\n\nearthquakes %&gt;%\n  filter(place == 'Yellowstone National Park, Wyoming') %&gt;%\n  summarize(avg_mag = avg(mag),\n            avg_mag_adjusted = avg(if_else(mag &gt; -9, mag, NA))) %&gt;%\n  kable()\n\n\n\n\navg_mag\navg_mag_adjusted\n\n\n\n\n0.4063935\n0.9233279\n\n\n\n\n\n\n\n6.3.3 Replacement with Alternate Values\n\nearthquakes %&gt;%\n  mutate(event_type = if_else(type == 'earthquake', type, 'Other')) %&gt;%\n  count(event_type) %&gt;%\n  kable()\n\n\n\n\nevent_type\nn\n\n\n\n\nearthquake\n1461750\n\n\nOther\n34176\n\n\n\n\n\n\nextremes &lt;-\n  earthquakes %&gt;%\n  summarize(p95 = quantile(mag, probs = 0.95, na.rm = TRUE),\n            p05 = quantile(mag, probs = 0.05, na.rm = TRUE))\n\nextremes %&gt;% kable()\n\n\n\n\np95\np05\n\n\n\n\n4.5\n0.12\n\n\n\n\n\nNote that this SQL from the book4\n\nCASE \n  WHEN mag &gt; p95 THEN p95\n  WHEN mag &lt; p05 THEN p05\n  ELSE mag\nEND AS mag_winsorized\n\ncan be replaced with a single line:\n\nLEAST(GREATEST(mag, p05), p95) AS mag_winsorized\n\nThe R equivalents of LEAST and GREATEST are pmin and pmax, respectively. And dbplyr will translate pmin and pmax for us, so we can get winsorized data as follows.\n\nearthquakes_wins &lt;-\n  earthquakes %&gt;%\n  mutate(mag = if_else(mag %in% c(-9.99, -9), NA, mag)) %&gt;%\n  filter(!is.na(mag)) %&gt;%\n  cross_join(extremes) %&gt;%\n  mutate(mag_winsorized = pmin(pmax(mag, p05), p95)) %&gt;%\n  select(time, place, mag, mag_winsorized) \n\nearthquakes_wins %&gt;%\n  arrange(desc(mag)) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2011-03-11 05:46:24\n2011 Great Tohoku Earthquake, Japan\n9.1\n4.5\n\n\n2010-02-27 06:34:11\noffshore Bio-Bio, Chile\n8.8\n4.5\n\n\n2012-04-11 08:38:36\noff the west coast of northern Sumatra\n8.6\n4.5\n\n\n\n\nearthquakes_wins %&gt;%\n  filter(mag == mag_winsorized) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2018-04-17 11:41:20\n26km ESE of Lincoln, Montana\n2.50\n2.50\n\n\n2018-04-17 11:41:34\n55km E of Cape Yakataga, Alaska\n1.20\n1.20\n\n\n2018-04-17 11:44:16\n39km N of Seeley Lake, Montana\n0.71\n0.71\n\n\n\n\nearthquakes_wins %&gt;%\n  arrange(mag) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2016-06-20 22:16:25\n20 km SSW of Pa‘auilo, Hawaii\n-5.0\n0.12\n\n\n2012-06-11 01:59:01\nNevada\n-2.6\n0.12\n\n\n2012-06-27 01:14:37\nNevada\n-2.6\n0.12\n\n\n\n\n\n\n\n6.3.4 Rescaling\nIn the book, it says WHERE depth &gt;= 0.05, but I need to use WHERE depth &gt; 0.05 to match the results there.\n\nquake_depths &lt;-\n  earthquakes %&gt;%\n  filter(depth &gt; 0.05) %&gt;%\n  mutate(depth = round(depth, 1)) %&gt;%\n  select(depth)\n\nquake_depths %&gt;%\n  mutate(log_depth = log(depth, base = 10)) %&gt;%\n  count(depth, log_depth) %&gt;%\n  arrange(depth) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\ndepth\nlog_depth\nn\n\n\n\n\n0.1\n-1.0000000\n6994\n\n\n0.2\n-0.6989700\n6876\n\n\n0.3\n-0.5228787\n7269\n\n\n\n\nquake_depths %&gt;%\n  ggplot(aes(x = depth)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\nquake_depths %&gt;%\n  ggplot(aes(x = log(depth, base = 10))) +\n  geom_histogram(binwidth = 0.1)"
  },
  {
    "objectID": "chapter_6.html#footnotes",
    "href": "chapter_6.html#footnotes",
    "title": "6  Anomaly Detection",
    "section": "",
    "text": "The reason for this is not clear and I have filed a feature request to make the handling of ntile() similar to that of other window functions.↩︎\nWithout the select(earthquake_year), the following plot would take 4 seconds to produce—versus 0.4 seconds needed here. This suggests that ggplot is triggering a collect() on fields that it does not need to make the plot.↩︎\nUnlike the plot in the book, I leave observations with “manual” status in the plot, as they are in the SQL query.↩︎\nI simplify the SQL from the book assuming that the table already includes a CROSS JOIN with the SQL equivalent of extremes and I use shorter variable names for the extremes.↩︎"
  },
  {
    "objectID": "chapter_7.html#strengths-and-limits-of-experiment-analysis-with-sql",
    "href": "chapter_7.html#strengths-and-limits-of-experiment-analysis-with-sql",
    "title": "7  Experiment Analysis",
    "section": "7.1 Strengths and Limits of Experiment Analysis with SQL",
    "text": "7.1 Strengths and Limits of Experiment Analysis with SQL\nI would reframe this discussion. We might be using a database because the results of an experiment are stored there.\nAn alternative approach might have the results of an experiment are extracted and exported to a CSV or Excel file and attached to an email and sent to you for analysis.1 But extracted from where? If the data are in a database, it would be better to cut out the middleman and just get the data directly.\nThe limitations of SQL essentially vanish when we access the data using dbplyr. R can do any statistical calculation we can think of, so we have no need of an online statistical calculator (though such calculators can help us frame our analyses and check our results).\nCathy does say that “many databases allow developers to extend SQL functionality with user-defined functions (UDFs) … but they are beyond the scope of this book.” For PostgreSQL there are PL/Python and PL/R, which allow creation of functions in Python and R, respectively. When I first started to use PostgreSQL, these extensions seemed pretty exciting and, because I was maintaining my own databases, I could use them. But over time, I found maintaining UDFs to be more effort than could be justified and I no longer use them. Instead, if I need to do analysis in Python or R, I will extract data from the database, do the analysis, then write data back to the database. While this likely partly reflects the kinds of analysis I do, I think that UDFs are likely to be off limits to most users because of the additional complexity of turning code into UDFs and because many users would lack sufficient database privileges to create these.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\nlibrary(flextable)\nlibrary(janitor)"
  },
  {
    "objectID": "chapter_7.html#the-data-set",
    "href": "chapter_7.html#the-data-set",
    "title": "7  Experiment Analysis",
    "section": "7.2 The Data Set",
    "text": "7.2 The Data Set\nAs discussed in Tanimura (2021), there are four tables used in this chapter. We can get these data from the GitHub site associated with Tanimura (2021).\n\nurl &lt;- paste0(\"https://raw.githubusercontent.com/\",\n              \"cathytanimura/sql_book/master/\",\n              \"Chapter%207%3A%20Experiment%20Analysis/\")\n\ngame_users &lt;- read_csv(paste0(url, \"game_users.csv\"),\n                        col_types = \"iDc\")\n\ngame_actions &lt;- read_csv(paste0(url, \"game_actions.csv\"),\n                         col_types = \"icD\")\n\ngame_purchases &lt;- read_csv(paste0(url, \"game_purchases.csv\"),\n                         col_types = \"iDd\")\n\nexp_assignment &lt;- read_csv(paste0(url, \"exp_assignment.csv\"),\n                           col_types = \"ciDc\")\n\nHaving obtained these four data tables, we replace the tibble version of each with an equivalent stored in an in-memory DuckDB database. Note that all the code below works if you skip the following five lines of code. However, in compiling a version of this chapter, I could do it in about 24 seconds without these lines (i.e., using local data frames) and in about 4 seconds with these lines (i.e., using DuckDB).^][Using PostgreSQL instead of DuckDB yield a time of around 6 seconds; so a substantial improvement over dplyr, but not quite as quick as in-memory DuckDB.]\n\ndb &lt;- dbConnect(duckdb::duckdb())\ngame_users &lt;- copy_to(db, game_users, \"game_users\")\ngame_actions &lt;- copy_to(db, game_actions, \"game_actions\")\ngame_purchases &lt;- copy_to(db, game_purchases, \"game_purchases\")\nexp_assignment &lt;- copy_to(db, exp_assignment, \"exp_assignment\")"
  },
  {
    "objectID": "chapter_7.html#types-of-experiments",
    "href": "chapter_7.html#types-of-experiments",
    "title": "7  Experiment Analysis",
    "section": "7.3 Types of Experiments",
    "text": "7.3 Types of Experiments\nI would reword the first paragraph here to the following for clarity (edits in italics):\n\nThere is a wide range of experiments, If you can change something that a user, customer, constituent, or other entity experiences, you can in theory test the effect of that change on some outcome.\n\n\n7.3.1 Experiments with Binary Outcomes: The Chi-Squared Test\nTo better match the approach of the book, I essentially create the contingency table in the database. An alternative approach would have been to collect() after summarize() and then do the statistical analysis in R. In fact, many of the functions in R are better set-up for this approach. However, this means bring more data into R and doing more calculation in R. If the experiment is very large or you would rather the database server do more of the work, then the approach below may be preferred.\n\ncont_tbl &lt;-\n  exp_assignment %&gt;%\n  left_join(game_actions, by = \"user_id\") %&gt;%\n  group_by(variant, user_id) %&gt;%\n  summarize(completed = \n              coalesce(any(action == \"onboarding complete\", na.rm = TRUE),\n                       FALSE),\n            .groups = \"drop\") %&gt;%\n  count(variant, completed) %&gt;%\n  mutate(completed = if_else(completed, \"Yes\", \"No\")) %&gt;%\n  pivot_wider(names_from = completed, values_from = n) %&gt;%\n  collect()\n\nUsing the packages janitor and flextable, I mimic the nicely formatted output shown in the book:\n\ncont_tbl %&gt;%\n  adorn_totals(c(\"row\", \"col\")) %&gt;%\n  mutate(`% Complete` = prettyNum(Yes/Total * 100, digits = 4)) %&gt;%\n  flextable() %&gt;%\n  add_header_row(values=c(\"\", \"Completed onboarding\", \"\"),\n                 colwidths = c(1, 2, 2))\n\n\nCompleted onboardingvariantYesNoTotal% Completevariant 138,28011,99550,27576.14control36,26813,62949,89772.69Total74,54825,624100,17274.42\n\n\nNow I can do the Chi-squared test. I need to turn variant into the row names of the contingency table so that we want a simple \\(2 \\times 2\\) numeric table as the input to our statistical test and I use column_to_rownames() to this end. I then pipe the result into the base R function chisq.test(). I specified correct = FALSE so that my result matched what I got from the online calculator I found. I then display the Chi-squared statistic and the \\(p\\)-value.\n\nres &lt;- \n  cont_tbl %&gt;%\n  column_to_rownames(var = \"variant\") %&gt;%\n  chisq.test(correct = FALSE)\n\nres$statistic\n\nX-squared \n 157.0758 \n\nres$p.value\n\n[1] 4.926953e-36\n\n\n\n\n7.3.2 Experiments with Continuous Outcomes: The t-Test\n\namounts &lt;- \n  exp_assignment %&gt;%\n  filter(exp_name == 'Onboarding') %&gt;%\n  left_join(game_purchases, by = \"user_id\") %&gt;%\n  group_by(variant, user_id) %&gt;%\n  summarize(amount = sum(coalesce(amount, 0), na.rm = TRUE),\n            .groups = \"drop\")\n\nt_test_stats &lt;-\n  amounts %&gt;%\n  group_by(variant) %&gt;%\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) %&gt;%\n  collect()\n\nt_test_stats %&gt;%\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n50275\n3.688\n19.22\n\n\ncontrol\n49897\n3.781\n18.94\n\n\n\n\n\nWe can make a small function that we can pass a data frame to as df. The calculations assume that df contains two rows (one for each group) and columns named mean, sd, and n for the mean, standard deviation, and number of observations, respectively, in each group.\n\nt_test &lt;- function(df) {\n  mean_diff = abs(df$mean[1] - df$mean[2])\n  se_diff &lt;- sqrt(sum(df$sd^2 / df$n))\n  t_stat &lt;- mean_diff / se_diff\n  p &lt;- pt(t_stat, df = sum(df$n))\n  p_val &lt;- 2 * min(p, 1 - p)\n  return(list(\"statistic\" = t_stat, \"p-value\" = p_val))\n}\n\nt_test(t_test_stats)\n\n$statistic\n[1] 0.7765458\n\n$`p-value`\n[1] 0.4374286\n\n\nThese values line up with those obtained from the online calculator I found.\nAn alternative approach would be to collect() the underlying data and do the \\(t\\)-test in R.\n\nt_test_data &lt;-\n  amounts %&gt;%\n  select(variant, amount) %&gt;%\n  collect()\n\nt.test(formula = amount ~ variant, data = t_test_data)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by variant\nt = 0.77655, df = 100165, p-value = 0.4374\nalternative hypothesis: true difference in means between group control and group variant 1 is not equal to 0\n95 percent confidence interval:\n -0.1426893  0.3299478\nsample estimates:\n  mean in group control mean in group variant 1 \n               3.781218                3.687589 \n\n\n\nt_test_stats_2 &lt;-\n  amounts %&gt;%\n  inner_join(game_actions, by = \"user_id\") %&gt;%\n  filter(action == \"onboarding complete\") %&gt;%\n  group_by(variant) %&gt;%\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) %&gt;%\n  collect()\n\nt_test_stats_2 %&gt;%\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n38280\n4.843\n21.899\n\n\ncontrol\n36268\n5.202\n22.049\n\n\n\n\n\n\nt_test(t_test_stats_2)\n\n$statistic\n[1] 2.229649\n\n$`p-value`\n[1] 0.02577371"
  },
  {
    "objectID": "chapter_7.html#challenges-with-experiments-and-options-for-rescuing-flawed-experiments",
    "href": "chapter_7.html#challenges-with-experiments-and-options-for-rescuing-flawed-experiments",
    "title": "7  Experiment Analysis",
    "section": "7.4 Challenges with Experiments and Options for Rescuing Flawed Experiments",
    "text": "7.4 Challenges with Experiments and Options for Rescuing Flawed Experiments\n\n7.4.1 Variant Assignment\n\n\n7.4.2 Outliers\n\nexp_assignment %&gt;%\n  left_join(game_purchases, by = \"user_id\", keep = TRUE,\n            suffix = c(\"\", \".y\")) %&gt;%\n  inner_join(game_actions, by = \"user_id\") %&gt;%\n  filter(action == \"onboarding complete\",\n         exp_name == 'Onboarding') %&gt;%\n  group_by(variant) %&gt;%\n  summarize(total_cohorted = n_distinct(user_id),\n            purchasers = n_distinct(user_id.y),\n            .groups = \"drop\") %&gt;%\n  mutate(pct_purchased = purchasers * 100.0 / total_cohorted) %&gt;%\n  kable(digits = 2)\n\n\n\n\nvariant\ntotal_cohorted\npurchasers\npct_purchased\n\n\n\n\nvariant 1\n38280\n4981\n13.01\n\n\ncontrol\n36268\n4988\n13.75\n\n\n\n\n\n\n\n7.4.3 Time Boxing\n\namounts_boxed &lt;- \n  exp_assignment %&gt;%\n  filter(exp_name == 'Onboarding') %&gt;%\n  mutate(exp_end = exp_date + days(7)) %&gt;%\n  left_join(game_purchases, \n            by = join_by(user_id, exp_end &gt;= purch_date)) %&gt;%\n  group_by(variant, user_id) %&gt;%\n  summarize(amount = sum(coalesce(amount, 0), na.rm = TRUE),\n            .groups = \"drop\")\n\n\nt_test_stats_boxed &lt;-\n  amounts_boxed %&gt;%\n  group_by(variant) %&gt;%\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) %&gt;%\n  collect()\n\nt_test_stats_boxed %&gt;%\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n50275\n1.352\n5.613\n\n\ncontrol\n49897\n1.369\n5.766\n\n\n\n\n\n\nt_test(t_test_stats_boxed)\n\n$statistic\n[1] 0.4920715\n\n$`p-value`\n[1] 0.6226699\n\n\n\n\n7.4.4 Pre-Post Analysis\nThe original SQL query is something like this, but we make some tweaks to get this to work more naturally using dbplyr.\n\nSELECT \n  CASE WHEN a.created BETWEEN '2020-01-13' AND '2020-01-26' THEN 'pre'\n     WHEN a.created BETWEEN '2020-01-27' AND '2020-02-09' THEN 'post'\n     END AS variant,\n  count(distinct a.user_id) AS cohorted,\n  count(distinct b.user_id) AS opted_in,\n  count(distinct b.user_id) * 1.0 / count(DISTINCT a.user_id) AS pct_optin,\n  count(distinct a.created) AS days\nFROM game_users a\nLEFT JOIN game_actions b ON a.user_id = b.user_id \n  AND b.action = 'email_optin'\nWHERE a.created BETWEEN '2020-01-13' AND '2020-02-09'\nGROUP BY 1\n;\n\nThe first tweak is to translate the b.action = 'email_optin' part of the join into a filter, which we embed in the left_join() portion of our code (mirroring how it works in the SQL). The second tweak is to move the pct_optin out of the grouped aggregation portion of the query, as in the SQL it is referring to what we will later have as opted_in and cohorted. (In fact, I don’t bother with `pct_optin at all, as it’s easy to calculate when we use it in making a table.)\nNote that COUNT(DISTINCT col) becomes n_distinct(col). We use keep = TRUE and suffix = c(\"\", \".y\") to store what is b.user_id in the SQL as user_id.y.\nWe calculate not_opted_in so that our table is better prepared for the chisq.test() we will pass it to later on.\n\nopt_in_df &lt;-\n  game_users %&gt;%\n  filter(between(created, \n                 as.Date('2020-01-13'), \n                 as.Date('2020-02-09'))) %&gt;%\n  left_join(game_actions %&gt;% \n              filter(action == 'email_optin'), by = \"user_id\",\n            keep = TRUE,\n            suffix = c(\"\", \".y\")) %&gt;%\n  mutate(variant = if_else(created &lt;= '2020-01-26', 'pre', 'post')) %&gt;%\n  group_by(variant) %&gt;%\n  summarize(cohorted = n_distinct(user_id),\n            opted_in = n_distinct(user_id.y),\n            days = n_distinct(created),\n            .groups = \"drop\") %&gt;%\n  mutate(not_opted_in = cohorted - opted_in) %&gt;%\n  collect()\n\n\nopt_in_df %&gt;%\n  select(variant, days, opted_in, not_opted_in, cohorted) %&gt;%\n  adorn_totals(c(\"row\")) %&gt;%\n  mutate(`% opted in` = prettyNum(100.0 * opted_in/cohorted, digits = 4)) %&gt;%\n  rename(Yes = opted_in, No = not_opted_in, Total = cohorted) %&gt;%\n  flextable() %&gt;%\n  add_header_row(values=c(\"\", \"Opted in\", \"\"),\n                 colwidths = c(2, 2, 2))\n\n\nOpted invariantdaysYesNoTotal% opted inpost1411,22016,39727,61740.63pre1414,48910,17324,66258.75Total2825,70926,57052,27949.18\n\n\n\nres &lt;- \n  opt_in_df %&gt;% \n  select(-cohorted, -days) %&gt;%\n  column_to_rownames(var = \"variant\") %&gt;%\n  chisq.test(correct = FALSE)\n\nres$statistic\n\nX-squared \n 1712.075 \n\nres$p.value\n\n[1] 0\n\n\n\n\n7.4.5 Natural experiment analysis\n\nby_country &lt;- \n  game_users %&gt;%\n  filter(country %in% c('United States', 'Canada')) %&gt;%\n  left_join(game_purchases, by = \"user_id\",\n            keep = TRUE,\n            suffix = c(\"\", \".y\")) %&gt;%\n  group_by(country) %&gt;%\n  summarize(cohorted = n_distinct(user_id),\n            opted_in = n_distinct(user_id.y),\n            .groups = \"drop\") %&gt;%\n  mutate(pct_purchased = 1.0 * opted_in/cohorted) %&gt;%\n  collect()\n\nby_country %&gt;% kable()\n\n\n\n\ncountry\ncohorted\nopted_in\npct_purchased\n\n\n\n\nUnited States\n45012\n4958\n0.1101484\n\n\nCanada\n20179\n5011\n0.2483275\n\n\n\n\n\n\nres &lt;- \n  by_country %&gt;% \n  select(-pct_purchased) %&gt;%\n  column_to_rownames(var = \"country\") %&gt;%\n  chisq.test(correct = FALSE)\n\nres$statistic\n\nX-squared \n 1447.273 \n\nres$p.value\n\n[1] 1.122178e-316\n\n\nTo avoid a warning, we disconnect from the database once we’re done with it.\n\ndbDisconnect(db, shutdown = TRUE)\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_7.html#footnotes",
    "href": "chapter_7.html#footnotes",
    "title": "7  Experiment Analysis",
    "section": "",
    "text": "Ew!↩︎"
  },
  {
    "objectID": "chapter_8.html#when-to-use-sql-for-complex-data-sets",
    "href": "chapter_8.html#when-to-use-sql-for-complex-data-sets",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.1 When to Use SQL for Complex Data Sets",
    "text": "8.1 When to Use SQL for Complex Data Sets\n\n8.1.1 Advantages of Using SQL\n\n\n8.1.2 When to Build into ETL Instead\n\n\n8.1.3 When to Put Logic in Other Tools\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\n\n\npg &lt;- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)"
  },
  {
    "objectID": "chapter_8.html#code-organization",
    "href": "chapter_8.html#code-organization",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.2 Code Organization",
    "text": "8.2 Code Organization\n\nearthquakes &lt;- tbl(pg, \"earthquakes\")\nlegislators_terms &lt;- tbl(pg, \"legislators_terms\")\nvideogame_sales &lt;- tbl(pg, \"videogame_sales\")\n\n\nearthquakes %&gt;%\n  filter(date_part('year', time) &gt;= 2019,\n         between(mag, 0, 1)) %&gt;%\n  mutate(place = case_when(grepl('CA', place) ~ 'California',\n                           grepl('AK', place) ~ 'Alaska',\n                           TRUE ~ trim(split_part(place, ',', 2L)))) %&gt;%\n  count(place, type, mag) %&gt;%\n  arrange(desc(n)) %&gt;%\n  collect(n = 10) %&gt;%\n  kable()\n\n\n\n\nplace\ntype\nmag\nn\n\n\n\n\nAlaska\nearthquake\n1.00\n4824\n\n\nAlaska\nearthquake\n0.90\n4153\n\n\nAlaska\nearthquake\n0.80\n3219\n\n\nCalifornia\nearthquake\n0.56\n2631\n\n\nAlaska\nearthquake\n0.70\n2061\n\n\nNevada\nearthquake\n1.00\n1688\n\n\nNevada\nearthquake\n0.80\n1681\n\n\nNevada\nearthquake\n0.90\n1669\n\n\nAlaska\nearthquake\n0.60\n1464\n\n\nCalifornia\nearthquake\n0.55\n1444\n\n\n\n\n\n\n8.2.1 Commenting\n\n\n8.2.2 Capitalization, Indentation, Parentheses, and Other Formatting Tricks\n\n\n8.2.3 Storing Code"
  },
  {
    "objectID": "chapter_8.html#organizing-computations",
    "href": "chapter_8.html#organizing-computations",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.3 Organizing Computations",
    "text": "8.3 Organizing Computations\n\n8.3.1 Understanding Order of SQL Clause Evaluation\nIn general, I think the order of evaluation is more intuitive when writing SQL using dbplyr. It is perhaps more confusing to someone who has written a lot of SQL without thinking about the order things are evaluated (e.g., WHERE comes early in evaluation, but relatively late in the SQL).\nIn dplyr the meaning of filter() is usually clear by where it is placed. Some times it is translated into WHERE and some times it is HAVING. In the example below, a WHERE-like filter would be placed just after legislators_terms (much as it is evaluated by the SQL query engine), while in this case we have filter being translated into HAVING because it is using the result of GROUP BY query. One hardly need give this much thought.\n\nterms_by_states &lt;-\n  legislators_terms %&gt;%\n  group_by(state) %&gt;%\n  summarize(terms = n()) %&gt;%\n  filter(terms &gt;= 1000) %&gt;%\n  arrange(desc(terms))\n\nterms_by_states %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT \"state\", COUNT(*) AS \"terms\"\nFROM \"legislators_terms\"\nGROUP BY \"state\"\nHAVING (COUNT(*) &gt;= 1000.0)\nORDER BY \"terms\" DESC\n\nterms_by_states %&gt;%\n  collect(n = 10) %&gt;%\n  kable()\n\n\n\n\nstate\nterms\n\n\n\n\nNY\n4159\n\n\nPA\n3252\n\n\nOH\n2239\n\n\nCA\n2121\n\n\nIL\n2011\n\n\nTX\n1692\n\n\nMA\n1667\n\n\nVA\n1648\n\n\nNC\n1351\n\n\nMI\n1284\n\n\n\n\n\nThe way dplyr code is written makes it easy to look at the output each step in the series of pipes. In the query below, we can easily highlight the code up to the end of any pipe and evaluate it to see if it is doing what we want and expect it be doing. In this specific case, I find the dplyr code to be more intuitive than the SQL provided in the book, which uses an `a\n\nlegislators_terms %&gt;%\n  group_by(state) %&gt;%\n  summarize(terms = n(), .groups = \"drop\") %&gt;%\n  mutate(avg_terms = mean(terms, na.rm = TRUE)) %&gt;%\n  collect(n = 10) %&gt;%\n  kable(digits = 2)\n\n\n\n\nstate\nterms\navg_terms\n\n\n\n\nDK\n16\n746.83\n\n\nND\n170\n746.83\n\n\nNV\n177\n746.83\n\n\nOH\n2239\n746.83\n\n\nGU\n24\n746.83\n\n\nNY\n4159\n746.83\n\n\nHI\n122\n746.83\n\n\nIN\n1177\n746.83\n\n\nNE\n379\n746.83\n\n\nWV\n428\n746.83\n\n\n\n\n\n\nlegislators_terms %&gt;%\n  group_by(state) %&gt;%\n  summarize(terms = n(), .groups = \"drop\") %&gt;%\n  window_order(desc(terms)) %&gt;%\n  mutate(rank = row_number()) %&gt;%\n  arrange(rank) %&gt;%\n  collect(n = 10) %&gt;%\n  kable()\n\n\n\n\nstate\nterms\nrank\n\n\n\n\nNY\n4159\n1\n\n\nPA\n3252\n2\n\n\nOH\n2239\n3\n\n\nCA\n2121\n4\n\n\nIL\n2011\n5\n\n\nTX\n1692\n6\n\n\nMA\n1667\n7\n\n\nVA\n1648\n8\n\n\nNC\n1351\n9\n\n\nMI\n1284\n10\n\n\n\n\n\n\n\n8.3.2 Subqueries\nIn writing dbplyr code, it is more natural to think in terms of CTEs, even though the code you write will generally be translated into SQL using subqueries.\nThe query in the book written with LATERAL seems much more confusing to me than the following. (Also, adding EXPLAIN to each query suggests that LATERAL is more complicated for PostgreSQL.) I rewrote the LATERAL query using CTEs and got the following, which seems closer to the second SQL query included in the book.\n\nWITH \n\ncurrent_legislators AS (\n  SELECT distinct id_bioguide, party\n  FROM legislators_terms\n  WHERE term_end &gt; '2020-06-01'),\n  \nparty_changers AS (\n  SELECT b.id_bioguide, min(term_start) as first_term\n  FROM legislators_terms b\n  INNER JOIN current_legislators AS a\n  ON b.id_bioguide = a.id_bioguide AND b.party &lt;&gt; a.party\n  GROUP BY 1)\n\nSELECT date_part('year', first_term) as first_year, party,\n  count(id_bioguide) as legislators\nFROM current_legislators\nINNER JOIN party_changers\nUSING (id_bioguide)\nGROUP BY 1, 2;\n\n\n3 records\n\n\nfirst_year\nparty\nlegislators\n\n\n\n\n1979\nRepublican\n1\n\n\n2011\nLibertarian\n1\n\n\n2015\nDemocrat\n1\n\n\n\n\n\nTranslating the CTE version into dbplyr is a piece of cake.\n\ncurrent_legislators &lt;-\n  legislators_terms %&gt;%\n  filter(term_end &gt; '2020-06-01') %&gt;%\n  distinct(id_bioguide, party)\n\nparty_changers &lt;-\n  legislators_terms %&gt;%\n  inner_join(current_legislators, \n             join_by(id_bioguide)) %&gt;%\n  filter(party.x != party.y) %&gt;%\n  group_by(id_bioguide) %&gt;%\n  summarize(first_term = min(term_start, na.rm = TRUE), .groups = \"drop\")\n\ncurrent_legislators %&gt;%\n  inner_join(party_changers, by = \"id_bioguide\") %&gt;%\n  mutate(first_year = date_part('year', first_term)) %&gt;%\n  group_by(first_year, party) %&gt;%\n  summarize(legislators = n(), .groups = \"drop\") %&gt;%\n  collect() %&gt;%\n  kable()\n\n\n\n\nfirst_year\nparty\nlegislators\n\n\n\n\n1979\nRepublican\n1\n\n\n2011\nLibertarian\n1\n\n\n2015\nDemocrat\n1\n\n\n\n\n\n\n\n8.3.3 Temporary Tables\nCreating temporary tables with dbplyr is easy: simply append compute() at the end of the table definition. Generally, dbplyr will take care of details that likely do not matter much, such as choosing a name for the table (we don’t care because we can refer to the table below as current_legislators regardless of the name chosen for it).\n\ncurrent_legislators %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT DISTINCT \"id_bioguide\", \"party\"\nFROM \"legislators_terms\"\nWHERE (\"term_end\" &gt; '2020-06-01')\n\ncurrent_legislators &lt;-\n  legislators_terms %&gt;%\n  filter(term_end &gt; '2020-06-01') %&gt;%\n  distinct(id_bioguide, party) %&gt;%\n  compute()\n\ncurrent_legislators %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT *\nFROM \"dbplyr_001\"\n\n\nCreating temporary tables can lead to significant performance gains in some situations, as the query optimizer has a simpler object to work with. Note that dbplyr allows the creating of an index with temporary tables, which can improve performance even further.\nNot that some database administrators do not allow users to create temporary tables. In such cases, you can often use collect() followed by copy_inline() effectively. (This is also useful when you have data outside the database—so collect() is not relevant—but want to merge it with data in the database.1)\n\n\n8.3.4 Common Table Expressions\nWe have been using them throughout the book already. For the sake of completeness, I rewrite the query given in the book here.\n\nfirst_term &lt;- \n  legislators_terms %&gt;%\n  group_by(id_bioguide) %&gt;%\n  summarize(first_term = min(term_start, na.rm = TRUE),\n            .groups = \"drop\")\n    \nfirst_term %&gt;%\n  inner_join(legislators_terms, by = \"id_bioguide\") %&gt;%\n  mutate(periods = date_part('year', age(term_start, first_term))) %&gt;%\n  group_by(periods) %&gt;%\n  summarize(cohort_retained = n_distinct(id_bioguide)) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\nperiods\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n3600\n\n\n2\n3619\n\n\n\n\n\n\n\n8.3.5 grouping sets\nI don’t think there is a “pure” dbplyr way of doing these. However, not all is lost for the dedicated dbplyr user, as the following examples demonstrate.\n\nglobal_sales &lt;-\n  tbl(pg, sql(\"\n    SELECT platform, genre, publisher,\n      sum(global_sales) as global_sales\n    FROM videogame_sales\n    GROUP BY grouping sets (platform, genre, publisher)\"))\n\nglobal_sales %&gt;%\n  arrange(desc(global_sales)) %&gt;%\n  collect(n = 10) %&gt;%\n  kable()\n\n\n\n\nplatform\ngenre\npublisher\nglobal_sales\n\n\n\n\nNA\nNA\nNintendo\n1786.56\n\n\nNA\nAction\nNA\n1751.18\n\n\nNA\nSports\nNA\n1330.93\n\n\nPS2\nNA\nNA\n1255.64\n\n\nNA\nNA\nElectronic Arts\n1110.32\n\n\nNA\nShooter\nNA\n1037.37\n\n\nX360\nNA\nNA\n979.96\n\n\nPS3\nNA\nNA\n957.84\n\n\nNA\nRole-Playing\nNA\n927.37\n\n\nWii\nNA\nNA\n926.71\n\n\n\n\n\n\nglobal_sales_cube &lt;-\n  tbl(pg, sql(\"\n    SELECT coalesce(platform, 'All') as platform,\n      coalesce(genre,'All') AS genre,\n      coalesce(publisher,'All') AS publisher,\n      sum(global_sales) AS global_sales\n    FROM videogame_sales\n    GROUP BY cube (platform, genre, publisher)\"))\n\nglobal_sales_cube %&gt;%\n  arrange(platform, genre, publisher) %&gt;%\n  collect(n = 10) %&gt;%\n  kable()\n\n\n\n\nplatform\ngenre\npublisher\nglobal_sales\n\n\n\n\n2600\nAction\n20th Century Fox Video Games\n1.72\n\n\n2600\nAction\nActivision\n4.64\n\n\n2600\nAction\nAll\n29.34\n\n\n2600\nAction\nAnswer Software\n0.50\n\n\n2600\nAction\nAtari\n7.68\n\n\n2600\nAction\nAvalon Interactive\n0.17\n\n\n2600\nAction\nBomb\n0.22\n\n\n2600\nAction\nCBS Electronics\n0.31\n\n\n2600\nAction\nColeco\n1.26\n\n\n2600\nAction\nCPG Products\n0.54"
  },
  {
    "objectID": "chapter_8.html#managing-data-set-size-and-privacy-concerns",
    "href": "chapter_8.html#managing-data-set-size-and-privacy-concerns",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.4 Managing Data Set Size and Privacy Concerns",
    "text": "8.4 Managing Data Set Size and Privacy Concerns\n\n8.4.1 Sampling with %, mod\nOne can also use random() to similar effect.\n\n\n8.4.2 Reducing Dimensionality\n\nlegislators_terms %&gt;%\n  mutate(state_group = \n           case_when(state %in% c('CA', 'TX', 'FL', 'NY', 'PA') ~ state, \n                     TRUE ~ 'Other')) %&gt;%\n  group_by(state_group) %&gt;%\n  summarize(terms = n()) %&gt;%\n  arrange(desc(terms)) %&gt;%\n  collect(n = 6) %&gt;%\n  kable()\n\n\n\n\nstate_group\nterms\n\n\n\n\nOther\n31980\n\n\nNY\n4159\n\n\nPA\n3252\n\n\nCA\n2121\n\n\nTX\n1692\n\n\nFL\n859\n\n\n\n\n\n\ntop_states &lt;-\n  legislators_terms %&gt;%\n  group_by(state) %&gt;%\n  summarize(n_reps = n_distinct(id_bioguide), .groups = \"drop\") %&gt;%\n  window_order(desc(n_reps)) %&gt;%\n  mutate(rank = row_number())\n\nlegislators_terms %&gt;%\n  inner_join(top_states, by = \"state\") %&gt;%\n  mutate(state_group = case_when(rank &lt;= 5 ~ state,\n                                 TRUE ~ 'Other')) %&gt;%\n  group_by(state_group) %&gt;%\n  summarize(terms = n_distinct(id_bioguide)) %&gt;%\n  arrange(desc(terms)) %&gt;%\n  collect(n = 6) %&gt;%\n  kable()\n\n\n\n\nstate_group\nterms\n\n\n\n\nOther\n8317\n\n\nNY\n1494\n\n\nPA\n1075\n\n\nOH\n694\n\n\nIL\n509\n\n\nVA\n451\n\n\n\n\n\nNote that the CASE WHEN in the SQL in the book can be significantly simplified.\n\nWITH num_terms AS (\n  SELECT id_bioguide, count(term_id) as terms\n    FROM legislators_terms\n    GROUP BY 1)\n    \nSELECT terms &gt;= 2 AS two_terms_flag,\n  count(*) as legislators\nFROM num_terms\nGROUP BY 1;\n\n\n2 records\n\n\ntwo_terms_flag\nlegislators\n\n\n\n\nFALSE\n4139\n\n\nTRUE\n8379\n\n\n\n\n\n\nnum_terms &lt;- \n  legislators_terms %&gt;%\n  group_by(id_bioguide) %&gt;%\n  summarize(terms = n(), .groups = \"drop\")\n\nnum_terms %&gt;%\n  mutate(two_terms_flag = terms &gt;= 2) %&gt;%\n  count(two_terms_flag) %&gt;%\n  collect() %&gt;%\n  kable()\n\n\n\n\ntwo_terms_flag\nn\n\n\n\n\nFALSE\n4139\n\n\nTRUE\n8379\n\n\n\n\n\nNow we can reuse the num_terms lazy table we created above.\n\nnum_terms %&gt;%\n  mutate(terms_level = case_when(terms &gt;= 10 ~ '10+',\n                                 terms &gt;= 2 ~ '2 - 9',\n                                 TRUE ~ '1')) %&gt;%\n  count(terms_level) %&gt;%\n  collect() %&gt;%\n  kable()\n\n\n\n\nterms_level\nn\n\n\n\n\n1\n4139\n\n\n2 - 9\n7496\n\n\n10+\n883\n\n\n\n\n\n\n\n8.4.3 PII and Data Privacy"
  },
  {
    "objectID": "chapter_8.html#conclusion",
    "href": "chapter_8.html#conclusion",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.5 Conclusion",
    "text": "8.5 Conclusion"
  },
  {
    "objectID": "chapter_8.html#footnotes",
    "href": "chapter_8.html#footnotes",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "",
    "text": "See here for examples.↩︎"
  },
  {
    "objectID": "chapter_9.html#funnel-analysis",
    "href": "chapter_9.html#funnel-analysis",
    "title": "9  Conclusion",
    "section": "9.1 Funnel Analysis",
    "text": "9.1 Funnel Analysis\nThe issue described in this section of Tanimura (2021) is more difficult to get to using dbplyr. This seems the natural translation of the ideas represented in the SQL there.\n\nusers %&gt;%\n  left_join(step_one, by = \"user_id\",\n            keep = TRUE, suffix = c(\"\", \"_1\")) %&gt;%\n  left_join(step_two, by = \"user_id\",\n            keep = TRUE, suffix = c(\"\", \"_2\")) %&gt;%\n  summarize(all_users = n_distinct(user_id),\n            step_one_users = n_distinct(user_id_1),\n            step_two_users = n_distinct(user_id_2)) %&gt;%\n  mutate(pct_step_one = step_one_users/all_users,\n         pct_one_to_two = step_two_users / step_one_users)\n\nThis yields the second of the two options provided, which seems to be the implicitly preferred one if we want to capture all step_two_users—even if some customers go directly to step_two—though it does make pct_one_to_two a bit more difficult to interpret in such cases."
  },
  {
    "objectID": "chapter_9.html#churn-lapse-and-other-definitions-of-departure",
    "href": "chapter_9.html#churn-lapse-and-other-definitions-of-departure",
    "title": "9  Conclusion",
    "section": "9.2 Churn, Lapse, and Other Definitions of Departure",
    "text": "9.2 Churn, Lapse, and Other Definitions of Departure\nThis query seems easier to follow with a separate window specification (WINDOW w AS) and using a CTE.\n\nWITH gap_intervals AS (\n  SELECT id_bioguide, term_start,\n    lag(term_start) OVER w AS prev,\n    age(term_start, lag(term_start) OVER w) as gap_interval\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  WINDOW w AS (partition BY id_bioguide \n               ORDER BY term_start))\n\nSELECT avg(gap_interval) AS avg_gap\nFROM gap_intervals\nWHERE gap_interval IS NOT NULL;\n\n\n1 records\n\n\navg_gap\n\n\n\n\n2 years 2 mons 17 days 15:41:54.83805\n\n\n\n\n\nTranslating this query into dplyr is straightforward, with again most of the work being done in creation of gap_intervals. Note that we can refer to prev created in an earlier part of the mutate call in creating gap_interval. It seems that dbplyr detects this kind of situation and cleverly puts the calculation of prev into a subquery.\n\ngap_intervals &lt;-\n  legislators_terms %&gt;%\n  filter(term_type == 'rep') %&gt;%\n  group_by(id_bioguide) %&gt;%\n  window_order(term_start) %&gt;%\n  mutate(prev = lag(term_start),\n         gap_interval = age(term_start, prev)) %&gt;%\n  ungroup() %&gt;%\n  select(id_bioguide, term_start, prev, gap_interval)\n\nThis makes it effectively a single line of code to transform the data into the average shown above.\n\ngap_intervals %&gt;%\n  summarize(avg_gap = mean(gap_interval, na.rm = TRUE)) %&gt;%\n  kable()\n\n\n\n\navg_gap\n\n\n\n\n2 years 2 mons 17 days 15:41:54.83805\n\n\n\n\n\nWith gap_intervals in our pocket, the following query is much simpler than the SQL shown in the book.\n\ngap_months &lt;-\n  gap_intervals %&gt;%\n  mutate(gap_months = year(gap_interval) * 12 + month(gap_interval)) %&gt;%\n  count(gap_months, name = \"instances\") %&gt;%\n  arrange(gap_months) %&gt;%\n  ungroup()\n\ngap_months %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\ngap_months\ninstances\n\n\n\n\n1\n25\n\n\n2\n4\n\n\n3\n2\n\n\n\n\n\nThe following plot better matches what is shown in the book, where it seems the caption is incorrect.\n\ngap_months %&gt;%\n  filter(between(gap_months, 14, 49)) %&gt;%\n  ggplot(aes(x = gap_months, y = instances)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nlatest_date &lt;- \n  legislators_terms %&gt;% \n  summarize(max = max(term_start)) %&gt;%\n  pull()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\nintervals &lt;-\n  legislators_terms %&gt;%\n  filter(term_type == 'rep') %&gt;%\n  group_by(id_bioguide) %&gt;%\n  summarize(max_date = max(term_start), .groups = \"drop\") %&gt;%\n  mutate(interval_since_last = age(latest_date, max_date))\n\nintervals %&gt;%\n  mutate(years_since_last = year(interval_since_last)) %&gt;%\n  count(years_since_last) %&gt;%\n  arrange(years_since_last) %&gt;%\n  collect(n = 3) %&gt;%\n  kable()\n\n\n\n\nyears_since_last\nn\n\n\n\n\n0\n6\n\n\n1\n440\n\n\n2\n1\n\n\n\n\n\n\nintervals %&gt;%\n  mutate(months_since_last = year(interval_since_last) * 12 +\n           month(interval_since_last)) %&gt;%\n  count(months_since_last, name = \"reps\") %&gt;%\n  mutate(status = case_when(months_since_last &lt;= 23 ~ 'Current',\n                            months_since_last &lt;= 48 ~ 'Lapsed',\n                            TRUE ~ 'Churned')) %&gt;%\n  group_by(status) %&gt;%\n  summarize(total_reps = sum(reps, na.rm = TRUE)) %&gt;%\n  kable()\n\n\n\n\nstatus\ntotal_reps\n\n\n\n\nChurned\n10685\n\n\nCurrent\n446\n\n\nLapsed\n105"
  },
  {
    "objectID": "chapter_9.html#basket-analysis",
    "href": "chapter_9.html#basket-analysis",
    "title": "9  Conclusion",
    "section": "9.3 Basket Analysis",
    "text": "9.3 Basket Analysis\nThis seems like a case where the programmability of R and the SQL-generation capability of dbplyr to address this more comprehensively. To make this tangible, let’s make some data and put it in the database:\n\npurchases &lt;- tribble(\n  ~customer_id, ~product,\n  1, \"bananas\",\n  2, \"apples\",\n  2, \"oranges\",\n  3, \"apples\",\n  3, \"oranges\",\n  3, \"bananas\",\n  4, \"bananas\",\n  4, \"apples\",\n  4, \"oranges\",\n  4, \"passionfruit\") %&gt;%\n  mutate(customer_id = as.integer(customer_id)) %&gt;%\n  copy_inline(pg, .)\n\nNow, let’s make a function that performs as much of the analysis in the database as possible. The only data we bring into R relates to the distinct number of products purchased by customers. So let’s say some customers purchase one product, others two, three, or four, while some customers purchase six products. In this case n_prods = c(1, 2, 3, 4, 6) and that is the only data we need in R from the database. If p = 2 (i.e., we’re interested in product pairs), we then create all the combinations of 2 products for each value in n_prods. For a customer with one product, there is no pair. For a customer with two products, there is one pair. For a customer with three products, there are two pairs: {1, 2} and {2, 3}. These combinations are created in R and the resulting table (combos) is passed back to the database.\nFrom there, everything is processed in the database.\nIn creating cust_prods, each unique product purchased by each customer is given a within-customer ID (prod_num) using the row_number() window function, and note is made of the number of unique products purchased by the customer (n_products). We then join cust_prods with combos using (customer_id, n_products) to create base, which contains customer IDs, as well as the “baskets” of products represented in the form of prod_num values stored in p_1, p_2, etc.\nNext, we want to add the p actual products in each basket from cust_prods to the data stored in base. For example, we want to turn p_1 of 1 into product_1 of \"apples\". The get_prod() function does this for one product at a time and we use Reduce from base R to repeatedly apply this idea p times.\nFinally, we collapse the product_i fields into a single basket field analogous to what is done in Tanimura (2021).\n\nget_baskets &lt;- function(df, p) {\n  \n  get_combos &lt;- function(n, p) {\n    if (n &lt; p) return(NULL)\n    combos &lt;- t(matrix(combn(sort(n), p), nrow = p))\n    colnames(combos) &lt;- paste0(\"p_\", 1:p)\n    combos &lt;- as_tibble(combos)\n    combos$row_num &lt;- 1:nrow(combos)\n    cross_join(tibble(n_products = n), combos)\n  }\n\n  n_prods &lt;- \n    df %&gt;%\n    distinct(customer_id, product) %&gt;%\n    group_by(customer_id) %&gt;%\n    summarize(n_products = n()) %&gt;%\n    select(n_products) %&gt;%\n    distinct() %&gt;%\n    pull()\n\n  combos &lt;-\n    n_prods %&gt;% \n    lapply(get_combos, p = p) %&gt;%\n    bind_rows() %&gt;%\n    copy_inline(pg, .)\n\n  cust_prods &lt;-\n    df %&gt;% \n    group_by(customer_id) %&gt;% \n    window_order(product) %&gt;% \n    mutate(prod_num = row_number()) %&gt;%\n    ungroup() %&gt;%\n    group_by(customer_id) %&gt;% \n    mutate(n_products = n()) %&gt;%\n    ungroup()\n\n  base &lt;- \n    cust_prods %&gt;%\n    select(customer_id, n_products) %&gt;%\n    distinct() %&gt;%\n    inner_join(combos, by = join_by(n_products)) %&gt;%\n    select(-n_products)\n\n  get_prod &lt;- function(i) {\n    cust_prods %&gt;%\n      rename_with(function(x) paste0(\"p_\", i), \"prod_num\") %&gt;%\n      inner_join(base) %&gt;%\n      rename_with(function(x) paste0(\"product_\", i), \"product\") %&gt;%\n      select(customer_id, row_num, starts_with(\"product_\"))\n  }\n\n  Reduce(inner_join, lapply(1:p, get_prod)) %&gt;%\n    arrange(customer_id, row_num) %&gt;%\n    mutate(basket = \n             sql(paste0(\"concat(\", paste0(\"product_\", 1:p, collapse = \", ', ', \"), \")\"))) %&gt;%\n    compute() %&gt;%\n    select(customer_id, basket)\n}\n\nNow we can apply our function to our data to get two-item baskets.\n\npairs &lt;- get_baskets(purchases, 2) \n\npairs %&gt;% kable()\n\n\n\n\ncustomer_id\nbasket\n\n\n\n\n2\napples, oranges\n\n\n3\napples, bananas\n\n\n3\napples, oranges\n\n\n3\nbananas, oranges\n\n\n4\napples, bananas\n\n\n4\napples, oranges\n\n\n4\napples, passionfruit\n\n\n4\nbananas, oranges\n\n\n4\nbananas, passionfruit\n\n\n4\noranges, passionfruit\n\n\n\n\n\nAnd we can identify the most popular baskets easily:\n\npairs %&gt;%\n  count(basket) %&gt;%\n  arrange(desc(n)) %&gt;%\n  kable()\n\n\n\n\nbasket\nn\n\n\n\n\napples, oranges\n3\n\n\nbananas, oranges\n2\n\n\napples, bananas\n2\n\n\napples, passionfruit\n1\n\n\noranges, passionfruit\n1\n\n\nbananas, passionfruit\n1\n\n\n\n\n\nBut the function also works for three-item baskets …\n\nget_baskets(purchases, 3) %&gt;% kable()\n\n\n\n\ncustomer_id\nbasket\n\n\n\n\n3\napples, bananas, oranges\n\n\n4\napples, bananas, oranges\n\n\n4\napples, bananas, passionfruit\n\n\n4\napples, oranges, passionfruit\n\n\n4\nbananas, oranges, passionfruit\n\n\n\n\n\n… and four-item baskets too.\n\nget_baskets(purchases, 4) %&gt;% kable()\n\n\n\n\ncustomer_id\nbasket\n\n\n\n\n4\napples, bananas, oranges, passionfruit"
  },
  {
    "objectID": "chapter_9.html#resources",
    "href": "chapter_9.html#resources",
    "title": "9  Conclusion",
    "section": "9.4 Resources",
    "text": "9.4 Resources"
  },
  {
    "objectID": "chapter_9.html#final-thoughts",
    "href": "chapter_9.html#final-thoughts",
    "title": "9  Conclusion",
    "section": "9.5 Final Thoughts",
    "text": "9.5 Final Thoughts\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Tanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  }
]