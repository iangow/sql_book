[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SQL for Data Analysis using R",
    "section": "",
    "text": "Preface\nThis “book” is a collection of notes made as I work through Tanimura (2021). An alternative title might have been “Data Analysis with Data Stored in Databases”; while SQL is the language used in Tanimura (2021) to do the analysis, this is in some ways a minor detail. My view is that one can easy do “SQL” analysis using the dplyr package in R (this uses dbplyr behind the scenes … mostly). The dplyr package will quietly translate R code into SQL for the user.\nAn advantage of using dplyr rather than SQL directly is that one doesn’t need to learn as much SQL. In principle, one could use dplyr without knowing any SQL. Given that dplyr and R can be used to analyse data from other data sources, this reduces the amount that is needed to be learnt to do analysis. Additionally, one could write code to analyse data from a database and then easily reuse the code for data from a CSV file.\nNotwithstanding the discussion in the previous paragraph, I recommend that people who find themselves using SQL-driven databases a lot learn some SQL. This view is implicitly endorsed by Hadley Wickham with the inclusion of a significant amount of material intended to teach SQL to readers of “R for Data Science”. While we include a brief primer on SQL for dplyr users here, clearly an excellent source for learning SQL is Tanimura (2021) itself.\nAnother benefit of using R is that we can do more with R. Tanimura (2021) includes many plots of data produced using SQL, but the code to make the plots is not included. In contrast, here I will include code to produce the data as well as code to make plots (where applicable).\nMy view is that the material here might be useful to someone who is using Tanimura (2021) and wants to see how the ideas there can be implemented using R, while at the same time being useful to someone who knows (or is looking to learn) R and is looking for realistic data sets to work on."
  },
  {
    "objectID": "index.html#sql-and-me",
    "href": "index.html#sql-and-me",
    "title": "SQL for Data Analysis using R",
    "section": "SQL and me",
    "text": "SQL and me\nComputer Information Systems I was a required subject in my undergraduate degree program.1 While I do not recall many of the details of the subject, I do recall a lecturer whose English was difficult to understand (I am not sure how many lectures I attended, as I thought at the time that lectures are a very inefficient way to learn … as I still do) and a group project.\nThe group project required the construction of an SQL database for a hypothetical business. My group comprised three people—myself, a good friend, and someone who was working full-time and studying a degree specializing in information systems. I was a self-supporting full-time student, so I could not afford the software we used to build the database (my memory tells me it was about $60, but I know enough not to trust memories), but my friend had a copy. I recall that some parts of the project were completed over the phone (this was before the internet), perhaps not the best way to write SQL. In the end, I guess we got the project done and submitted.2\nIn any case, after that subject I continued on with the rest of my degree, but I don’t think I used SQL or any data analysis tools except for Excel for the remainder of my Bachelor of Commerce (the Bachelor of Laws was free of quantitative analysis as far as I recall).\nWhen I graduated I joined a “strategy consulting” firm. My first project involved trying to explain the factors affecting branch profitability for a regional firm offering banking and insurance products. I don’t remember all the details, but there were many data analyses and some involved making requests for SQL queries to be run to supply the data that I used.\nMy second significant project involved analyses of product and customer profitability. Again the details are hazy, but I recall that analyses required joining multiple tables involving data on mapping of products to cost centres, products to orders, orders to customers, and so on. I would guess that I used Microsoft Access on a laptop that was underpowered (for its time).3\nA later project had a reputation that preceded it. Several people had joined this project before deciding to leave the firm. My recollection is that a partner had been hired from another firm with the understanding that he would lead an “insurance benchmarking study”. The assigned junior person on the project would be waiting until the very busy partner had managed to extract complete surveys from various insurance firms. Faced with this prospect, I tried to engineer my way off this project by creating a Visual Basic program that would take completed surveys and, with assistance of a Microsoft Access database, create a Powerpoint slide automatically. My program worked OK, but I ended up needing to resign to get off the project myself.\nA later freelance consulting project had me working with the financial planning team of a major Australian bank. Their existing planning process involved sending out numerous Excel spreadsheet templates to various units (branches, project managers) and then carefully stitching back the completed spreadsheets into a single large spreadsheet. This process would take weeks because the returned spreadsheets had been quite mangled. I helped the team reengineer the process to use a Microsoft Access database fed by locked templates. The resulting process took hours instead of weeks.\nMaking the fateful—and much regretted—decision to leave business for a “career” in business academia, my exposure to SQL has not gone down. For the first several years after entering the PhD program at Stanford, PROC SQL in SAS was a mainstay of my analysis pipeline. In 2011, I decided to migrate to an alternative database, as I found SAS restrictive. After trying a number of alternatives (SQLite and MySQL), I settled on PostgreSQL as the backend of my data analysis workflow.\nInitially, I was writing a lot of SQL and using other programs (e.g., Stata and R) to analyse data. When dplyr and dbplyr emerged, I immediately found this to be very intuitive and facile for analysis. Nowadays, I rarely write SQL directly and rely on dbplyr to do that for me. Of course, it’s still very helpful to know SQL well and I suspect I still “think in SQL” even though I type in dbplyr commands. Today, I essentially never use “data files”; all my data go through a PostgreSQL database.\nThe point of boring any reader with the autobiographical details above is to illustrate that one can go a lot of places in data analysis and not get very far from SQL.\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_2.html#missing-data",
    "href": "chapter_2.html#missing-data",
    "title": "2  Preparing Data for Analysis",
    "section": "2.1 Missing data",
    "text": "2.1 Missing data\n\nlibrary(tidyverse)\nlibrary(DBI)\n\n\npg <- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)\n\n\ndates_sql <-\n   \"SELECT * \n    FROM generate_series('2000-01-01'::timestamp,\n                         '2030-12-31', '1 day')\"\n\ndates <- \n  tbl(pg, sql(dates_sql)) %>%\n  rename(date = generate_series)\n\ndates_processed <-\n  dates %>%\n  mutate(date_key = as.integer(to_char(date, 'yyyymmdd')),\n         day_of_month = as.integer(date_part('day',date)),\n         day_of_year = as.integer(date_part('doy', date)),\n         day_of_week = as.integer(date_part('dow', date)),\n         day_name  = trim(to_char(date, 'Day')),\n         day_short_name = trim(to_char(date, 'Dy')),\n         week_number = as.integer(date_part('week', date)),\n         week_of_month = as.integer(to_char(date,'W')),\n         week = as.Date(date_trunc('week', date)),\n         month_number = as.integer(date_part('month',date)),\n         month_name = trim(to_char(date, 'Month')),\n         month_short_name = trim(to_char(date, 'Mon')),\n         first_day_of_month = as.Date(date_trunc('month', date)),\n         last_day_of_month = as.Date(date_trunc('month', date) +\n                                       sql(\"interval '1 month' -\n                                            interval '1 day'\")),\n         quarter_number = as.integer(date_part('quarter', date)),\n         quarter_name = trim('Q' %||% as.integer(date_part('quarter', date))),\n         first_day_of_quarter = as.Date(date_trunc('quarter', date)),\n         last_day_of_quarter = as.Date(date_trunc('quarter', date) + \n                                         sql(\"interval '3 months' -\n                                              interval '1 day'\")),\n         year = as.integer(date_part('year', date)),\n         decade = as.integer(date_part('decade', date)) * 10,\n         century = as.integer(date_part('century', date)))\n\ndates_processed %>%\n  collect(n = 10)\n\n# A tibble: 10 × 22\n   date                date_key day_of_month day_of_year day_of_week day_name \n   <dttm>                 <int>        <int>       <int>       <int> <chr>    \n 1 2000-01-01 00:00:00 20000101            1           1           6 Saturday \n 2 2000-01-02 00:00:00 20000102            2           2           0 Sunday   \n 3 2000-01-03 00:00:00 20000103            3           3           1 Monday   \n 4 2000-01-04 00:00:00 20000104            4           4           2 Tuesday  \n 5 2000-01-05 00:00:00 20000105            5           5           3 Wednesday\n 6 2000-01-06 00:00:00 20000106            6           6           4 Thursday \n 7 2000-01-07 00:00:00 20000107            7           7           5 Friday   \n 8 2000-01-08 00:00:00 20000108            8           8           6 Saturday \n 9 2000-01-09 00:00:00 20000109            9           9           0 Sunday   \n10 2000-01-10 00:00:00 20000110           10          10           1 Monday   \n# ℹ 16 more variables: day_short_name <chr>, week_number <int>,\n#   week_of_month <int>, week <date>, month_number <int>, month_name <chr>,\n#   month_short_name <chr>, first_day_of_month <date>,\n#   last_day_of_month <date>, quarter_number <int>, quarter_name <chr>,\n#   first_day_of_quarter <date>, last_day_of_quarter <date>, year <int>,\n#   decade <dbl>, century <int>\n\ndates_processed %>%\n  show_query()\n\n<SQL>\nSELECT\n  *,\n  CAST(to_char(\"date\", 'yyyymmdd') AS INTEGER) AS \"date_key\",\n  CAST(date_part('day', \"date\") AS INTEGER) AS \"day_of_month\",\n  CAST(date_part('doy', \"date\") AS INTEGER) AS \"day_of_year\",\n  CAST(date_part('dow', \"date\") AS INTEGER) AS \"day_of_week\",\n  trim(to_char(\"date\", 'Day')) AS \"day_name\",\n  trim(to_char(\"date\", 'Dy')) AS \"day_short_name\",\n  CAST(date_part('week', \"date\") AS INTEGER) AS \"week_number\",\n  CAST(to_char(\"date\", 'W') AS INTEGER) AS \"week_of_month\",\n  CAST(date_trunc('week', \"date\") AS DATE) AS \"week\",\n  CAST(date_part('month', \"date\") AS INTEGER) AS \"month_number\",\n  trim(to_char(\"date\", 'Month')) AS \"month_name\",\n  trim(to_char(\"date\", 'Mon')) AS \"month_short_name\",\n  CAST(date_trunc('month', \"date\") AS DATE) AS \"first_day_of_month\",\n  CAST(date_trunc('month', \"date\") + interval '1 month' -\n                                            interval '1 day' AS DATE) AS \"last_day_of_month\",\n  CAST(date_part('quarter', \"date\") AS INTEGER) AS \"quarter_number\",\n  trim('Q' || CAST(date_part('quarter', \"date\") AS INTEGER)) AS \"quarter_name\",\n  CAST(date_trunc('quarter', \"date\") AS DATE) AS \"first_day_of_quarter\",\n  CAST(date_trunc('quarter', \"date\") + interval '3 months' -\n                                              interval '1 day' AS DATE) AS \"last_day_of_quarter\",\n  CAST(date_part('year', \"date\") AS INTEGER) AS \"year\",\n  CAST(date_part('decade', \"date\") AS INTEGER) * 10.0 AS \"decade\",\n  CAST(date_part('century', \"date\") AS INTEGER) AS \"century\"\nFROM (\n  SELECT \"generate_series\" AS \"date\"\n  FROM (\nSELECT * \n    FROM generate_series('2000-01-01'::timestamp,\n                         '2030-12-31', '1 day')\n  ) \"q01\"\n) \"q02\"\n\n\n\nctry_pops <-\n  tribble(\n  ~country, ~year_1980,  ~year_1990, ~year_2000, ~year_2010,\n  \"Canada\", 24593, 27791, 31100, 34207,\n  \"Mexico\", 68347, 84634, 99775, 114061,\n  \"United States\", 227225, 249623, 282162, 309326\n)\n\nctry_pops %>%\n  pivot_longer(cols = -country, \n               names_to = \"year\",\n               names_prefix = \"year_\",\n               values_ptypes = integer(),\n               values_to = \"population\")\n\n# A tibble: 12 × 3\n   country       year  population\n   <chr>         <chr>      <int>\n 1 Canada        1980       24593\n 2 Canada        1990       27791\n 3 Canada        2000       31100\n 4 Canada        2010       34207\n 5 Mexico        1980       68347\n 6 Mexico        1990       84634\n 7 Mexico        2000       99775\n 8 Mexico        2010      114061\n 9 United States 1980      227225\n10 United States 1990      249623\n11 United States 2000      282162\n12 United States 2010      309326\n\n\n\nctry_pops_db <- copy_to(pg, ctry_pops)\n\n\nctry_pops_db %>%\n  pivot_longer(cols = -country, \n               names_to = \"year\",\n               names_prefix = \"year_\",\n               values_to = \"population\") %>%\n  show_query()\n\n<SQL>\n(\n  (\n    (\n      SELECT \"country\", '1980' AS \"year\", \"year_1980\" AS \"population\"\n      FROM \"ctry_pops\"\n    )\n    UNION ALL\n    (\n      SELECT \"country\", '1990' AS \"year\", \"year_1990\" AS \"population\"\n      FROM \"ctry_pops\"\n    )\n  )\n  UNION ALL\n  (\n    SELECT \"country\", '2000' AS \"year\", \"year_2000\" AS \"population\"\n    FROM \"ctry_pops\"\n  )\n)\nUNION ALL\n(\n  SELECT \"country\", '2010' AS \"year\", \"year_2010\" AS \"population\"\n  FROM \"ctry_pops\"\n)\n\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_3.html#date-datetime-and-time-manipulations",
    "href": "chapter_3.html#date-datetime-and-time-manipulations",
    "title": "3  Time Series Analysis",
    "section": "3.1 Date, Datetime, and Time Manipulations",
    "text": "3.1 Date, Datetime, and Time Manipulations"
  },
  {
    "objectID": "chapter_3.html#time-zone-conversions",
    "href": "chapter_3.html#time-zone-conversions",
    "title": "3  Time Series Analysis",
    "section": "3.2 Time Zone Conversions",
    "text": "3.2 Time Zone Conversions\nTanimura (2021) points out that often “timestamps in the database are not encoded with the time zone, and you will need to consult with the source or developer to figure out how your data was stored.” When pushing data to a PostgreSQL database, I use the timestamp with time zone type as much as possible.\nTanimura (2021) provides the following example, which is interesting because the west coast of the United States would not be on the PST time zone at that time of year. Instead, it would be on PDT.\n\nSELECT '2020-09-01 00:00:00' AT TIME ZONE 'pst';\n\n\n1 records\n\n\ntimezone\n\n\n\n\n2020-08-31 16:00:00\n\n\n\n\n\n\nSELECT '2020-09-01 00:00:00' AT TIME ZONE 'pdt';\n\n\n1 records\n\n\ntimezone\n\n\n\n\n2020-08-31 17:00:00\n\n\n\n\n\nI think most people barely know the difference between PST and PDT and even fewer would know the exact dates that one switches from one to the other. A better approach is to use a time zone that encodes information about when PDT is used and when PST is used. In PostgreSQL, the table pg_timezone_names has information that we need.\n\nSELECT * \nFROM pg_timezone_names\nWHERE name ~ '^US/';\n\n\nDisplaying records 1 - 10\n\n\nname\nabbrev\nutc_offset\nis_dst\n\n\n\n\nUS/Alaska\nAKDT\n-08:00:00\nTRUE\n\n\nUS/Pacific\nPDT\n-07:00:00\nTRUE\n\n\nUS/Eastern\nEDT\n-04:00:00\nTRUE\n\n\nUS/Michigan\nEDT\n-04:00:00\nTRUE\n\n\nUS/Arizona\nMST\n-07:00:00\nFALSE\n\n\nUS/Indiana-Starke\nCDT\n-05:00:00\nTRUE\n\n\nUS/Aleutian\nHDT\n-09:00:00\nTRUE\n\n\nUS/Hawaii\nHST\n-10:00:00\nFALSE\n\n\nUS/East-Indiana\nEDT\n-04:00:00\nTRUE\n\n\nUS/Central\nCDT\n-05:00:00\nTRUE\n\n\n\n\n\n\nSELECT * \nFROM pg_timezone_names\nWHERE abbrev IN ('PDT', 'PST') \nORDER BY name DESC\nLIMIT 5;\n\n\n5 records\n\n\nname\nabbrev\nutc_offset\nis_dst\n\n\n\n\nUS/Pacific\nPDT\n-07:00:00\nTRUE\n\n\nPST8PDT\nPDT\n-07:00:00\nTRUE\n\n\nMexico/BajaNorte\nPDT\n-07:00:00\nTRUE\n\n\nCanada/Pacific\nPDT\n-07:00:00\nTRUE\n\n\nAsia/Manila\nPST\n08:00:00\nFALSE\n\n\n\n\n\nThe following two queries show that US/Pacific is sometimes PDT and sometimes PST.\n\nSELECT \n    '2020-09-01 17:00:01 US/Pacific'::timestamptz AS t1,\n    '2020-09-01 17:00:01 PDT'::timestamptz  AS t2;\n\n\n1 records\n\n\nt1\nt2\n\n\n\n\n2020-09-02 00:00:01\n2020-09-02 00:00:01\n\n\n\n\n\n\nSELECT \n    '2020-12-01 16:00:01 US/Pacific'::timestamptz AS t1,\n    '2020-12-01 16:00:01 PST'::timestamptz  AS t2;\n\n\n1 records\n\n\nt1\nt2\n\n\n\n\n2020-12-02 00:00:01\n2020-12-02 00:00:01\n\n\n\n\n\n\nsql <-\n  \"SELECT     \n    '2020-12-01 16:00:01 US/Pacific'::timestamptz AS t1,\n    '2020-12-01 16:00:01 PST'::timestamptz AS t2\"\n\ntwo_times <- tbl(pg, sql(sql))\ntwo_times\n\n# Source:   SQL [1 x 2]\n# Database: postgres  [iangow@/tmp:5432/iangow]\n  t1                  t2                 \n  <dttm>              <dttm>             \n1 2020-12-02 00:00:01 2020-12-02 00:00:01\n\ntwo_times %>%\n  collect()\n\n# A tibble: 1 × 2\n  t1                  t2                 \n  <dttm>              <dttm>             \n1 2020-12-02 00:00:01 2020-12-02 00:00:01\n\none_time <-\n  two_times %>%\n  select(t1) %>%\n  pull()\n\nprint(one_time, tz = \"UTC\")\n\n[1] \"2020-12-02 00:00:01 UTC\"\n\nprint(one_time, tz = \"US/Pacific\")\n\n[1] \"2020-12-01 16:00:01 PST\"\n\nSys.timezone()\n\n[1] \"Australia/Melbourne\"\n\nprint(one_time, tz = Sys.timezone())\n\n[1] \"2020-12-02 11:00:01 AEDT\"\n\n\nThe above examples illustrate a few key ideas.\nFirst, while we supply the literal form '2020-09-01 17:00:01 US/Pacific'::timestamptz, it seems that once a variable has been encoded as TIMESTAMP WITH TIME ZONE, it behaves as though it is actually being stored as a timestamp in the UTC time zone, just with the displayed time perhaps being different.\nSecond, columns of type TIMESTAMP WITH TIME ZONE come into R with the associated time-zone information, which is what we want (especially if we will later put timestamp data back into PostgreSQL).\nThird, we can see that we can choose to display information in a different time zone without changing the underlying data.\nSome care is needed with timestamp data. I think the AT TIME ZONE queries provided in Tanimura (2021) are actually pretty dangerous, as can be seen in the following query. While we supply 2020-09-01 00:00:00 as UTC and then render it AT TIME ZONE 'PDT', it turns out that the returned value is interpreted as a TIMESTAMP WITHOUT TIME ZONE and subsequent queries lead to confusing behaviour. In the query below, the second application of AT TIME ZONE interprets the TIMESTAMP WITHOUT TIME ZONE as though it came from the stated time zone and the results seem to have AT TIME ZONE doing the opposite of what it did when given a TIMESTAMP WITH TIME ZONE (as in the initial literal '2020-09-01 00:00:01 -0').\n\nWITH q1 AS\n (SELECT '2020-09-01 00:00:01 -0' AT TIME ZONE 'PDT' AS t1)\n \nSELECT t1 AT TIME ZONE 'UTC' AS t2,\n  to_char(t1, 'YYYY-MM-DD HH24:MI:SS TZ'),\n  t1 AT TIME ZONE 'PDT' AS t3,\n  t1 AT TIME ZONE 'Australia/Melbourne' AS t4,\n  pg_typeof(t1)\nFROM q1\n\n\n1 records\n\n\n\n\n\n\n\n\n\nt2\nto_char\nt3\nt4\npg_typeof\n\n\n\n\n2020-08-31 17:00:01\n2020-08-31 17:00:01\n2020-09-01 00:00:01\n2020-08-31 07:00:01\ntimestamp without time zone\n\n\n\n\n\nIt seems that TIMESTAMP WITHOUT TIME ZONE values should be converted to a time zone as quickly as possible to avoid confusion and that great care is needed with AT TIME ZONE given that it does very different things according to the supplied data type.\n\nWITH q1 AS\n (SELECT '2020-09-01 00:00:01 -0'::timestamptz AS t1)\n \nSELECT t1,\n  to_char(t1, 'YYYY-MM-DD HH24:MI:SS TZ'),\n  pg_typeof(t1)\nFROM q1\n\n\n1 records\n\n\n\n\n\n\n\nt1\nto_char\npg_typeof\n\n\n\n\n2020-09-01 00:00:01\n2020-09-01 00:00:01 UTC\ntimestamp with time zone\n\n\n\n\n\nStrange behaviour can result from values stored as TIMESTAMP WITHOUT TIME ZONE. Below we see that t1 is printed as UTC no matter what, while the behaviour of t2 seems easier to understand.\n\nsql <-\n  \"SELECT     \n    '2020-12-01 00:00:01-00' AS t1,\n    '2020-12-01 00:00:01-00'::timestamptz AS t2\"\n\ntwo_times_notz <- tbl(pg, sql(sql))\ntwo_times_notz\n\n# Source:   SQL [1 x 2]\n# Database: postgres  [iangow@/tmp:5432/iangow]\n  t1                     t2                 \n  <chr>                  <dttm>             \n1 2020-12-01 00:00:01-00 2020-12-01 00:00:01\n\n\n\ntwo_times_notz_r <-\n  collect(two_times_notz)\n\n\nprint(two_times_notz_r$t1)\n\n[1] \"2020-12-01 00:00:01-00\"\n\nSys.timezone()\n\n[1] \"Australia/Melbourne\"\n\nprint(two_times_notz_r$t1, tz = Sys.timezone())\n\n[1] \"2020-12-01 00:00:01-00\"\n\nprint(two_times_notz_r$t2)\n\n[1] \"2020-12-01 00:00:01 UTC\"\n\nSys.timezone()\n\n[1] \"Australia/Melbourne\"\n\nprint(two_times_notz_r$t2, tz = Sys.timezone())\n\n[1] \"2020-12-01 11:00:01 AEDT\"\n\n\nAs pointed out by Tanimura (2021), one drawback to storing information as UTC is that localtime information may be lost. But it seems it would be more prudent to store information as TIMESTAMP WITH TIME ZONE and keep local time zone information as a separate column to avoid confusion. For example, if the orders table is stored as TIMESTAMP WITHOUT TIME ZONE based on the local time of the customer, which might be Australia/Melbourne and the shipping table uses TIMESTAMP WITH TIME ZONE, then an analyst of time-to-ship data would be confused by orders apparently being shipped before they are made. If shipping table uses TIMESTAMP WITH TIME ZONE using timestamps in the time zone of the East Bay warehouse (so US/Pacific), things would be even worse.\nI think that fully fleshing out the issues here would require a separate chapter. In fact, nothing in the core part of Chapter 3 of Tanimura (2021) (which focuses on the retail_sales table) really uses timestamp information, so we can put these issues aside for now."
  },
  {
    "objectID": "chapter_3.html#date-and-timestamp-format-conversions",
    "href": "chapter_3.html#date-and-timestamp-format-conversions",
    "title": "3  Time Series Analysis",
    "section": "3.3 Date and Timestamp Format Conversions",
    "text": "3.3 Date and Timestamp Format Conversions\nAs discussed in Tanimura (2021), PostgreSQL has a rich array of functions for converting dates and times and extracting such information as months and days of the week.\n\nSELECT date_trunc('month','2020-10-04 12:33:35 -00'::timestamptz);\n\n\n1 records\n\n\ndate_trunc\n\n\n\n\n2020-10-01\n\n\n\n\n\nOne such function\n\na_time_df <- tbl(pg, sql(\"SELECT '2020-10-04 12:33:35'::timestamp AS a_time\"))\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time))\n\n# Source:   SQL [1 x 2]\n# Database: postgres  [iangow@/tmp:5432/iangow]\n  a_time              a_trunced_time     \n  <dttm>              <dttm>             \n1 2020-10-04 12:33:35 2020-10-01 00:00:00\n\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time)) %>%\n  show_query()\n\n<SQL>\nSELECT *, date_trunc('month', \"a_time\") AS \"a_trunced_time\"\nFROM (SELECT '2020-10-04 12:33:35'::timestamp AS a_time) \"q01\"\n\na_time_df %>%\n  collect()\n\n# A tibble: 1 × 1\n  a_time             \n  <dttm>             \n1 2020-10-04 12:33:35\n\n\n\na_time_df <- tbl(pg, sql(\"SELECT '2020-10-04 12:33:35 US/Pacific'::timestamp with time zone AS a_time\"))\n\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time)) \n\n# Source:   SQL [1 x 2]\n# Database: postgres  [iangow@/tmp:5432/iangow]\n  a_time              a_trunced_time     \n  <dttm>              <dttm>             \n1 2020-10-04 19:33:35 2020-10-01 00:00:00\n\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time)) %>%\n  show_query()\n\n<SQL>\nSELECT *, date_trunc('month', \"a_time\") AS \"a_trunced_time\"\nFROM (SELECT '2020-10-04 12:33:35 US/Pacific'::timestamp with time zone AS a_time) \"q01\"\n\na_time_df %>%\n  collect()\n\n# A tibble: 1 × 1\n  a_time             \n  <dttm>             \n1 2020-10-04 19:33:35\n\n\n\na_time_df %>%\n  mutate(new_time = a_time + sql(\"interval '3 hours'\")) %>%\n  collect()\n\n# A tibble: 1 × 2\n  a_time              new_time           \n  <dttm>              <dttm>             \n1 2020-10-04 19:33:35 2020-10-04 22:33:35"
  },
  {
    "objectID": "chapter_3.html#the-retail-sales-data-set",
    "href": "chapter_3.html#the-retail-sales-data-set",
    "title": "3  Time Series Analysis",
    "section": "3.4 The Retail Sales Data Set",
    "text": "3.4 The Retail Sales Data Set\n\nSELECT sales_month, sales\nFROM retail_sales\nWHERE kind_of_business = 'Retail and food services sales, total'\nORDER BY 1\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nsales\n\n\n\n\n1992-01-01\n146376\n\n\n1992-02-01\n147079\n\n\n1992-03-01\n159336\n\n\n1992-04-01\n163669\n\n\n1992-05-01\n170068\n\n\n1992-06-01\n168663\n\n\n1992-07-01\n169890\n\n\n1992-08-01\n170364\n\n\n1992-09-01\n164617\n\n\n1992-10-01\n173655\n\n\n\n\n\n\nretail_sales <- tbl(pg, \"retail_sales\")\nretail_sales %>%\n  filter(kind_of_business == 'Retail and food services sales, total') %>%\n  select(sales_month, sales) %>%\n  arrange(sales_month) %>%\n  ggplot(aes(x = sales_month, y = sales)) +\n  geom_line()\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year,\n    sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business = 'Retail and food services sales, total'\nGROUP BY 1\n;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nsales\n\n\n\n\n2007\n4439733\n\n\n2005\n4085746\n\n\n1992\n2014102\n\n\n2011\n4598302\n\n\n2014\n5215656\n\n\n2006\n4294359\n\n\n2010\n4284968\n\n\n2001\n3378906\n\n\n2019\n6218002\n\n\n2018\n6001623\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business == 'Retail and food services sales, total') %>%\n  mutate(sales_year = date_part('year', sales_month)) %>%\n  group_by(sales_year) %>%\n  summarize(sales = sum(sales, na.rm = TRUE)) %>%\n  arrange(sales_year) %>%\n  ggplot(aes(x = sales_year, y = sales)) +\n  geom_line()\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year, \n  kind_of_business, sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business IN \n          ('Book stores',\n           'Sporting goods stores',\n           'Hobby, toy, and game stores')\nGROUP BY 1,2\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nkind_of_business\nsales\n\n\n\n\n1992\nSporting goods stores\n15583\n\n\n1992\nHobby, toy, and game stores\n11251\n\n\n1992\nBook stores\n8327\n\n\n1993\nHobby, toy, and game stores\n11651\n\n\n1993\nSporting goods stores\n16791\n\n\n1993\nBook stores\n9108\n\n\n1994\nSporting goods stores\n18825\n\n\n1994\nBook stores\n10107\n\n\n1994\nHobby, toy, and game stores\n12850\n\n\n1995\nHobby, toy, and game stores\n13714\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c('Book stores',\n             'Sporting goods stores',\n             'Hobby, toy, and game stores')) %>%\n  mutate(sales_year = date_part('year', sales_month)) %>%\n  group_by(sales_year, kind_of_business) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  arrange(sales_year) %>%\n  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\nWHERE kind_of_business IN ('Men''s clothing stores','Women''s clothing stores')\nORDER BY 1,2;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nkind_of_business\nsales\n\n\n\n\n1992-01-01\nMen’s clothing stores\n701\n\n\n1992-01-01\nWomen’s clothing stores\n1873\n\n\n1992-02-01\nMen’s clothing stores\n658\n\n\n1992-02-01\nWomen’s clothing stores\n1991\n\n\n1992-03-01\nMen’s clothing stores\n731\n\n\n1992-03-01\nWomen’s clothing stores\n2403\n\n\n1992-04-01\nMen’s clothing stores\n816\n\n\n1992-04-01\nWomen’s clothing stores\n2665\n\n\n1992-05-01\nMen’s clothing stores\n856\n\n\n1992-05-01\nWomen’s clothing stores\n2752\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% c(\"Men's clothing stores\",\n                                 \"Women's clothing stores\")) %>%\n  select(sales_month, kind_of_business, sales) %>%\n  arrange(sales_month) %>%\n  ggplot(aes(x = sales_month, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year,\n  kind_of_business, sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business IN \n        ('Men''s clothing stores',\n        'Women''s clothing stores')\nGROUP BY 1, 2\nORDER BY 1, 2;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nkind_of_business\nsales\n\n\n\n\n1992\nMen’s clothing stores\n10179\n\n\n1992\nWomen’s clothing stores\n31815\n\n\n1993\nMen’s clothing stores\n9962\n\n\n1993\nWomen’s clothing stores\n32350\n\n\n1994\nMen’s clothing stores\n10032\n\n\n1994\nWomen’s clothing stores\n30585\n\n\n1995\nMen’s clothing stores\n9315\n\n\n1995\nWomen’s clothing stores\n28696\n\n\n1996\nMen’s clothing stores\n9546\n\n\n1996\nWomen’s clothing stores\n28238\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  mutate(sales_year = date_part('year', sales_month)) %>%\n  group_by(sales_year, kind_of_business) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  arrange(sales_year) %>%\n  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT date_part('year', sales_month) AS sales_year,\n  sum(CASE WHEN kind_of_business = 'Women''s clothing stores' \n          then sales \n          END) AS womens_sales,\n  sum(CASE WHEN kind_of_business = 'Men''s clothing stores' \n          then sales \n          END) AS mens_sales\nFROM retail_sales\nWHERE kind_of_business IN \n   ('Men''s clothing stores',\n    'Women''s clothing stores')\nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nwomens_sales\nmens_sales\n\n\n\n\n1992\n31815\n10179\n\n\n1993\n32350\n9962\n\n\n1994\n30585\n10032\n\n\n1995\n28696\n9315\n\n\n1996\n28238\n9546\n\n\n1997\n27822\n10069\n\n\n1998\n28332\n10196\n\n\n1999\n29549\n9667\n\n\n2000\n31447\n9507\n\n\n2001\n31453\n8625\n\n\n\n\n\n\npivoted_sales <-\n  retail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  mutate(kind_of_business = if_else(kind_of_business == \"Women's clothing stores\",\n                                    \"womens\", \"mens\"),\n         sales_year = date_part('year', sales_month)) %>%\n  group_by(sales_year, kind_of_business) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  pivot_wider(id_cols = \"sales_year\",\n              names_from = \"kind_of_business\",\n              names_glue = \"{kind_of_business}_{.value}\",\n              values_from = \"sales\")  \n\npivoted_sales %>%\n  show_query()\n\n<SQL>\nSELECT\n  \"sales_year\",\n  MAX(CASE WHEN (\"kind_of_business\" = 'mens') THEN \"sales\" END) AS \"mens_sales\",\n  MAX(CASE WHEN (\"kind_of_business\" = 'womens') THEN \"sales\" END) AS \"womens_sales\"\nFROM (\n  SELECT \"sales_year\", \"kind_of_business\", SUM(\"sales\") AS \"sales\"\n  FROM (\n    SELECT\n      \"sales_month\",\n      \"naics_code\",\n      CASE WHEN (\"kind_of_business\" = 'Women''s clothing stores') THEN 'womens' WHEN NOT (\"kind_of_business\" = 'Women''s clothing stores') THEN 'mens' END AS \"kind_of_business\",\n      \"reason_for_null\",\n      \"sales\",\n      date_part('year', \"sales_month\") AS \"sales_year\"\n    FROM \"retail_sales\"\n    WHERE (\"kind_of_business\" IN ('Men''s clothing stores', 'Women''s clothing stores'))\n  ) \"q01\"\n  GROUP BY \"sales_year\", \"kind_of_business\"\n) \"q02\"\nGROUP BY \"sales_year\"\n\npivoted_sales %>%\n  arrange(sales_year) %>%\n  collect(n = 10) %>%\n  knitr::kable()\n\n\n\n\nsales_year\nmens_sales\nwomens_sales\n\n\n\n\n1992\n10179\n31815\n\n\n1993\n9962\n32350\n\n\n1994\n10032\n30585\n\n\n1995\n9315\n28696\n\n\n1996\n9546\n28238\n\n\n1997\n10069\n27822\n\n\n1998\n10196\n28332\n\n\n1999\n9667\n29549\n\n\n2000\n9507\n31447\n\n\n2001\n8625\n31453\n\n\n\n\n\n\npivoted_sales %>%\n  filter(sales_year <= 2019) %>%\n  group_by(sales_year) %>%\n  mutate(womens_minus_mens = womens_sales - mens_sales,\n         mens_minus_womens = mens_sales - womens_sales) %>%\n  select(sales_year, womens_minus_mens, mens_minus_womens) %>%\n  arrange(sales_year) %>%\n  ggplot(aes(y = womens_minus_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\npivoted_sales %>%\n  filter(sales_year <= 2019) %>%\n  group_by(sales_year) %>%\n  mutate(womens_times_of_mens = womens_sales / mens_sales) %>%\n  arrange(sales_year) %>%\n  ggplot(aes(y = womens_times_of_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\npivoted_sales %>%\n  filter(sales_year <= 2019) %>%\n  group_by(sales_year) %>%\n  mutate(womens_pct_of_mens = (womens_sales / mens_sales - 1) * 100) %>%\n  arrange(sales_year) %>%\n  ggplot(aes(y = womens_pct_of_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  group_by(sales_month) %>%\n  mutate(total_sales = sum(sales)) %>%\n  ungroup() %>%\n  mutate(pct_total_sales = sales * 100 / total_sales) %>%\n  select(sales_month, kind_of_business, pct_total_sales) %>%\n  collect(n = 3)\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n# A tibble: 3 × 3\n  sales_month kind_of_business        pct_total_sales\n  <date>      <chr>                             <dbl>\n1 1992-01-01  Women's clothing stores            72.8\n2 1992-01-01  Men's clothing stores              27.2\n3 1992-02-01  Men's clothing stores              24.8\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  group_by(sales_month) %>%\n  mutate(total_sales = sum(sales)) %>%\n  ungroup() %>%\n  mutate(pct_total_sales = sales * 100 / total_sales) %>%\n  show_query()\n\n<SQL>\nSELECT *, (\"sales\" * 100.0) / \"total_sales\" AS \"pct_total_sales\"\nFROM (\n  SELECT *, SUM(\"sales\") OVER (PARTITION BY \"sales_month\") AS \"total_sales\"\n  FROM \"retail_sales\"\n  WHERE (\"kind_of_business\" IN ('Men''s clothing stores', 'Women''s clothing stores'))\n) \"q01\"\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  group_by(sales_month) %>%\n  mutate(total_sales = sum(sales)) %>%\n  ungroup() %>%\n  mutate(pct_total_sales = sales * 100 / total_sales) %>%\n  ggplot(aes(y = pct_total_sales, x = sales_month, color = kind_of_business)) +\n  geom_line()\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business == \"Women's clothing stores\") %>%\n  mutate(sales_year = date_part('year',sales_month)) %>%\n  group_by(sales_year) %>%\n  summarize(sales = sum(sales, na.rm = TRUE)) %>%\n  ungroup() %>%\n  window_order(sales_year) %>%\n  mutate(index_sales = first(sales),\n         pct_from_index = (sales/index_sales - 1) * 100)\n\n# Source:     SQL [?? x 4]\n# Database:   postgres  [iangow@/tmp:5432/iangow]\n# Ordered by: sales_year\n   sales_year sales index_sales pct_from_index\n        <dbl> <dbl>       <dbl>          <dbl>\n 1       1992 31815       31815           0   \n 2       1993 32350       31815           1.68\n 3       1994 30585       31815          -3.87\n 4       1995 28696       31815          -9.80\n 5       1996 28238       31815         -11.2 \n 6       1997 27822       31815         -12.6 \n 7       1998 28332       31815         -10.9 \n 8       1999 29549       31815          -7.12\n 9       2000 31447       31815          -1.16\n10       2001 31453       31815          -1.14\n# ℹ more rows\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% c(\"Women's clothing stores\",\n                                 \"Men's clothing stores\"),\n         sales_month <= '2019-12-31') %>%\n  mutate(sales_year = date_part('year',sales_month)) %>%\n  group_by(kind_of_business, sales_year) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  group_by(kind_of_business) %>%\n  window_order(sales_year) %>%\n  mutate(index_sales = first(sales),\n         pct_from_index = (sales/index_sales - 1) * 100) %>%\n  ungroup() %>%\n  ggplot(aes(y = pct_from_index, x = sales_year, color = kind_of_business)) +\n  geom_line()\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business == \"Women's clothing stores\") %>%\n  window_order(sales_month) %>%\n  window_frame(-11, 0) %>%\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n()) %>%\n  select(sales_month, moving_avg, records_count) %>%\n  collect(n = 10)\n\n# A tibble: 10 × 3\n   sales_month moving_avg records_count\n   <date>           <dbl>         <int>\n 1 1992-01-01       1873              1\n 2 1992-02-01       1932              2\n 3 1992-03-01       2089              3\n 4 1992-04-01       2233              4\n 5 1992-05-01       2337.             5\n 6 1992-06-01       2351.             6\n 7 1992-07-01       2354.             7\n 8 1992-08-01       2392.             8\n 9 1992-09-01       2411.             9\n10 1992-10-01       2445.            10\n\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_4.html#cohorts-a-useful-analysis-framework",
    "href": "chapter_4.html#cohorts-a-useful-analysis-framework",
    "title": "4  Cohorts",
    "section": "4.1 Cohorts: A Useful Analysis Framework",
    "text": "4.1 Cohorts: A Useful Analysis Framework\nChapter 4 examines the fascinating topic of cohorts, where a cohort is a group of observations (often people) who acquire a shared characteristic at (approximately) the same time. For example, children entering kindergarten in New Zealand in 2017 or the Harvard MBA Class of 2002.\nWhile cohort analysis has some attractive features, I guess that data do not often come in a format that facilitates such analysis. Instead, as is the case with the legislators data set studied in Chapter 4, the data analyst needs to rearrange the data to support cohort analysis.\nI found Chapter 4 a little confusing on a first pass through it.1 The chapter launches into some SQL code intended to create cohorts, but it’s a little unclear why we’re doing what we’re doing, and we quickly see that our cohort analysis does not make sense (e.g., we have more of our original cohort in period 5 than we had in period 4) and we must have done something wrong. I think I see the idea Cathy is going for here: one needs to think carefully about how to arrange the data to avoid subtle mistakes. The challenge I see is that it’s not obvious that everyone would make the same mistake and one is too deep in the weeds of the code to really see the forest for the trees.\nSo before I launch into the code, I will spend a little time thinking about things conceptually. I will start with a different example from that used in Chapter 4, but one that I think brings out some of the issues.\nFor some reason I associate cohort analysis with life expectancy. The people who are born today form a cohort and one might ask: How long do we expect members of this cohort to live? One often hears life expectancy statistics quoted as something like: “In Australia, a boy born in 2018–2020 can expect to live to the age of 81.2 years and a girl would be expected to live to 85.3 years compared to 51.1 for boys and 54.8 years for girls born in in 1891–1900.”2\nThe people who construct the life expectancies for the children must be veritable polymaths. They need to anticipate future developments in medical care and technology. Skin cancer is a significant cause of death in Australia, due to a mismatch between the median complexion and the intensity of the sun. But the analysts calculating life expectancy need to think about how medical technology is likely to affect rates of death from carcinoma in the future. I can imagine whole-body scanners a bit like the scanners in US airports that detect skin cancers before they become problematic. These analysts also need to understand how road safety will evolve. Will children today all be in driverless vehicles in fifty years time and will accidents then be a rarity? And what about war? The data analyst needs to be able to forecast the possibility of World War III breaking out and shortening life spans. Who are these people?\nOf course it seems unlikely these über-analysts exist. Rather they surely do something more prosaic. Here is my guess as to how life expectancies are constructed.3 I guess that the data analyst gathers data on cohorts notionally formed at some point in the past and then looks at survival rates for that cohort over some period, then aggregates those data into a life expectancy.\nFor example, the data analyst might gather data on people who turned 21 in 2018 and then data on whether those people surived to their 22nd birthday. The proportion of such people who make their 22nd birthday could be interpreted as a survival probability \\(p_{21}\\). Repeat that for each age-based cohort to get probabilities \\(\\left\\{p_i: i = 0, 1, \\dots, 119, 120 \\right\\}\\). Now to find the median life expectancy, we could calculate something like this:4\n\\[ \\left\\{j: \\arg \\min_{i} \\left(\\prod_{0}^{i} p_i\\right) \\leq \\frac{1}{2} \\right\\} \\] So we have a (fairly) well-defined procedure here. There are obviously some details to be worked out. For example, do we focus on one year (2018 in this case)? Or collect data over multiple years? Does it make sense to form cohorts by years? Or would grouping into larger cohorts (e.g., 20–25) make more sense? Do we identify people by birthdays? Or just use some kind of census date? (People who are 21 on 1 July might have just turned 21, or might be about to turn 22.)\nBut what exactly have we calculated? In a sense it’s a nonsensical number. Why would survival rates for 88-year-olds in 2018 be relevant for the life expectancy of newborns today, who will face a very different world when they turn 88 in 2111. First, perhaps the analysts really don’t calculate it in this way (though I’m doubtful they are polymaths). Second, even though it’s a “meaningless” number, it probably still has attractive properties, such as the ability to represent in a one or two numbers a lot about the quality of life in Australia.\nA final note is that it is not clear to me where the “51.1 for boys and 54.8 years for girls born in in 1891–1900” values come from. Are these the equivalent life expectancies calculated using data available around 1900? Or are these the observed lifespans of people born in 1891–1900? If the latter, how accurate were the former as estimates of these values?"
  },
  {
    "objectID": "chapter_4.html#the-legislators-data-set",
    "href": "chapter_4.html#the-legislators-data-set",
    "title": "4  Cohorts",
    "section": "4.2 The Legislators Data Set",
    "text": "4.2 The Legislators Data Set\nNow that we understand cohorts, let’s move onto the legislators data set.\n\nlibrary(DBI)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(dbplyr)\n\n\nAttaching package: 'dbplyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\nlibrary(ggplot2)\nlibrary(knitr)\n\n\npg <- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)\n\n\nlegislators_terms <- tbl(pg, \"legislators_terms\")\n\n\nSELECT id_bioguide ,min(term_start) AS first_term\nFROM legislators_terms \nGROUP BY 1\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\n\n\n\n\nA000118\n1975-01-14\n\n\nP000281\n1933-03-09\n\n\nK000039\n1933-03-09\n\n\nA000306\n1907-12-02\n\n\nO000095\n1949-01-03\n\n\nB000937\n1913-04-07\n\n\nS000038\n1912-01-01\n\n\nS000657\n1901-12-02\n\n\nP000383\n1873-12-01\n\n\nL000058\n1887-12-05\n\n\n\n\n\n\nWITH first_terms AS (\n      SELECT id_bioguide, min(term_start) AS first_term\n      FROM legislators_terms \n      GROUP BY 1) \nSELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n    count(distinct a.id_bioguide) as cohort_retained\nFROM first_terms a\nJOIN legislators_terms b on a.id_bioguide = b.id_bioguide \nGROUP BY 1\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n3600\n\n\n2\n3619\n\n\n3\n1831\n\n\n4\n3210\n\n\n5\n1744\n\n\n6\n2385\n\n\n7\n1360\n\n\n8\n1607\n\n\n9\n1028\n\n\n\n\n\n\nfirst_terms <- \n  legislators_terms %>%\n  group_by(id_bioguide) %>%\n  summarize(first_term = min(term_start, na.rm = TRUE))\n\ncohorts <-\n  legislators_terms %>%\n  inner_join(first_terms, by = \"id_bioguide\") %>%\n  mutate(period = date_part('year', age(term_start, first_term))) %>%\n  group_by(period) %>%\n  summarize(cohort_retained = sql(\"count(distinct id_bioguide)\")) \n\ncohorts %>%\n  collect(n = 10)\n\n# A tibble: 10 × 2\n   period cohort_retained\n    <dbl>           <int>\n 1      0           12518\n 2      1            3600\n 3      2            3619\n 4      3            1831\n 5      4            3210\n 6      5            1744\n 7      6            2385\n 8      7            1360\n 9      8            1607\n10      9            1028\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts AS (\n  SELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n      count(distinct a.id_bioguide) as cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  GROUP BY 1)\n  \nSELECT period,\n  first_value(cohort_retained) OVER (ORDER BY period) AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / first_value(cohort_retained) OVER (ORDER BY period) AS pct_retained\nFROM cohorts;\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n3600\n0.2875859\n\n\n2\n12518\n3619\n0.2891037\n\n\n3\n12518\n1831\n0.1462694\n\n\n4\n12518\n3210\n0.2564307\n\n\n5\n12518\n1744\n0.1393194\n\n\n6\n12518\n2385\n0.1905256\n\n\n7\n12518\n1360\n0.1086436\n\n\n8\n12518\n1607\n0.1283751\n\n\n9\n12518\n1028\n0.0821217\n\n\n\n\n\n\nretained_data <-\n  cohorts %>%\n  window_order(period) %>%\n  mutate(cohort_size = first(cohort_retained)) %>%\n  mutate(pct_retained = cohort_retained * 1.0/cohort_size) %>%\n  select(period, cohort_size, cohort_retained, pct_retained) \n\nretained_data %>%\n  collect(n = 10)\n\n# A tibble: 10 × 4\n   period cohort_size cohort_retained pct_retained\n    <dbl>       <int>           <int>        <dbl>\n 1      0       12518           12518       1     \n 2      1       12518            3600       0.288 \n 3      2       12518            3619       0.289 \n 4      3       12518            1831       0.146 \n 5      4       12518            3210       0.256 \n 6      5       12518            1744       0.139 \n 7      6       12518            2385       0.191 \n 8      7       12518            1360       0.109 \n 9      8       12518            1607       0.128 \n10      9       12518            1028       0.0821\n\n\n\nretained_data %>%\n  ggplot(aes(x = period, y = pct_retained)) +\n  geom_line()\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts AS (\n  SELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n      count(distinct a.id_bioguide) as cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  GROUP BY 1),\n  \nretained_data AS (\n  SELECT period,\n    first_value(cohort_retained) OVER (ORDER BY period) AS cohort_size,\n    cohort_retained,\n    cohort_retained * 1.0 / first_value(cohort_retained) OVER (ORDER BY period) AS pct_retained\n  FROM cohorts)\n\nSELECT cohort_size,\n  max(case when period = 0 then pct_retained end) as yr0,\n  max(case when period = 1 then pct_retained end) as yr1,\n  max(case when period = 2 then pct_retained end) as yr2,\n  max(case when period = 3 then pct_retained end) as yr3,\n  max(case when period = 4 then pct_retained end) as yr4\nFROM retained_data\nGROUP BY 1;\n\n\n1 records\n\n\ncohort_size\nyr0\nyr1\nyr2\nyr3\nyr4\n\n\n\n\n12518\n1\n0.2875859\n0.2891037\n0.1462694\n0.2564307\n\n\n\n\n\n\nretained_data %>%\n  select(period, pct_retained) %>%\n  filter(period <= 4) %>%\n  collect() %>%\n  arrange(period) %>%\n  pivot_wider(names_from = period, \n              names_prefix = \"yr\",\n              values_from = pct_retained) %>%\n  collect() %>%\n  kable()\n\n\n\n\nyr0\nyr1\nyr2\nyr3\nyr4\n\n\n\n\n1\n0.2875859\n0.2891037\n0.1462694\n0.2564307"
  },
  {
    "objectID": "chapter_4.html#adjusting-time-series-to-increase-retention-accuracy",
    "href": "chapter_4.html#adjusting-time-series-to-increase-retention-accuracy",
    "title": "4  Cohorts",
    "section": "4.3 Adjusting Time Series to Increase Retention Accuracy",
    "text": "4.3 Adjusting Time Series to Increase Retention Accuracy\n\nWITH\n\nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT a.id_bioguide, a.first_term, b.term_start, b.term_end,\n  c.date,\n  date_part('year', age(c.date, a.first_term)) AS period\nFROM first_terms a\nJOIN legislators_terms b USING (id_bioguide)\nLEFT JOIN date_dim c ON c.date BETWEEN b.term_start and b.term_end \nand c.month_name = 'December' and c.day_of_month = 31;\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\ndate\nperiod\n\n\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-03\n1993-12-31\n0\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-03\n1994-12-31\n1\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-03\n1993-12-31\n0\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-03\n1994-12-31\n1\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-03\n1987-12-31\n0\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-03\n1988-12-31\n1\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n1983-12-31\n0\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n1984-12-31\n1\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-03\n2007-12-31\n0\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-03\n2008-12-31\n1\n\n\n\n\n\n\nyear_ends <-\n  tbl(pg, sql(\"\n    SELECT generate_series::date AS date\n    FROM generate_series('1770-12-31', '2030-12-31', interval '1 year')\"))\n\ncohorts <-\n  first_terms %>%\n  inner_join(legislators_terms, by = join_by(id_bioguide)) %>%\n  left_join(year_ends, \n            by = join_by(between(y$date, x$term_start, x$term_end))) %>%\n  mutate(period = date_part('year', age(date, first_term))) %>%\n  select(id_bioguide, first_term, term_start, term_end, date, period) \n\ncohorts %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\ndate\nperiod\n\n\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-03\n1993-12-31\n0\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-03\n1994-12-31\n1\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-03\n1993-12-31\n0\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-03\n1994-12-31\n1\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-03\n1987-12-31\n0\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-03\n1988-12-31\n1\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n1983-12-31\n0\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n1984-12-31\n1\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-03\n2007-12-31\n0\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-03\n2008-12-31\n1\n\n\n\n\n\n\nyear_ends <- \n  tibble(date = seq(as.Date(\"1770-12-31\"), \n                    as.Date(\"2030-12-31\"), \n                    by = \"year\")) %>%\n  copy_inline(pg, .)\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n  \nyear_ends AS (\n  SELECT generate_series::date as date\n  FROM generate_series('1770-12-31', '2030-12-31', interval '1 year')\n)\n  \nSELECT \n  coalesce(date_part('year', age(c.date, a.first_term)), 0) AS period,\n  count(DISTINCT a.id_bioguide) AS cohort_retained\nFROM first_terms a\nJOIN legislators_terms b ON a.id_bioguide = b.id_bioguide \nLEFT JOIN year_ends c ON c.date BETWEEN b.term_start AND b.term_end \nGROUP BY 1;\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n12328\n\n\n2\n8166\n\n\n3\n8069\n\n\n4\n5862\n\n\n5\n5795\n\n\n6\n4361\n\n\n7\n4339\n\n\n8\n3521\n\n\n9\n3485\n\n\n\n\n\n\nWITH\n\nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) as first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts_retained AS (        \n    SELECT coalesce(date_part('year', age(c.date,a.first_term)),0) AS period,\n    count(distinct a.id_bioguide) as cohort_retained\n    FROM first_terms a\n    JOIN legislators_terms b USING (id_bioguide)\n    LEFT JOIN date_dim c ON c.date BETWEEN b.term_start AND b.term_end \n      AND c.month_name = 'December' AND c.day_of_month = 31\n    GROUP BY 1)\n    \nSELECT period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0/first_value(cohort_retained) over w as pct_retained\nFROM cohorts_retained\nWINDOW w AS (ORDER BY period);\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n12328\n0.9848219\n\n\n2\n12518\n8166\n0.6523406\n\n\n3\n12518\n8069\n0.6445918\n\n\n4\n12518\n5862\n0.4682857\n\n\n5\n12518\n5795\n0.4629334\n\n\n6\n12518\n4361\n0.3483783\n\n\n7\n12518\n4339\n0.3466209\n\n\n8\n12518\n3521\n0.2812750\n\n\n9\n12518\n3485\n0.2783991\n\n\n\n\n\n\ncohorts_retained <-\n  cohorts %>%\n  mutate(period = coalesce(date_part('year', age(date, first_term)), 0)) %>%\n  select(period, id_bioguide) %>%\n  distinct() %>%\n  group_by(period) %>%\n  summarize(cohort_retained = n()) \n\npct_retained <-\n  cohorts_retained %>%\n  window_order(period) %>%\n  mutate(cohort_size = first(cohort_retained),\n         cohort_retained = as.double(cohort_retained),\n         pct_retained = cohort_retained/cohort_size) \n\npct_retained %>%\n  arrange(period) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12328\n12518\n0.9848219\n\n\n2\n8166\n12518\n0.6523406\n\n\n3\n8069\n12518\n0.6445918\n\n\n4\n5862\n12518\n0.4682857\n\n\n5\n5795\n12518\n0.4629334\n\n\n6\n4361\n12518\n0.3483783\n\n\n7\n4339\n12518\n0.3466209\n\n\n8\n3521\n12518\n0.2812750\n\n\n9\n3485\n12518\n0.2783991\n\n\n\n\npct_retained %>%\n  ggplot(aes(x = period, y = pct_retained)) + \n  geom_line()\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT id_bioguide, a.first_term, b.term_start,\n  CASE WHEN b.term_type = 'rep' THEN b.term_start + interval '2 years'\n       WHEN b.term_type = 'sen' THEN b.term_start + interval '6 years'\n  END AS term_end\nFROM first_terms a\nJOIN legislators_terms b USING (id_bioguide);\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\n\n\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-06\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-04\n\n\nF000062\n1992-11-10\n1992-11-10\n1998-11-10\n\n\nF000469\n2019-01-03\n2019-01-03\n2021-01-03\n\n\nK000367\n2007-01-04\n2007-01-04\n2013-01-04\n\n\nM000639\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nS000033\n1991-01-03\n1991-01-03\n1993-01-03\n\n\n\n\n\n\nfirst_terms %>%\n  inner_join(legislators_terms, by = join_by(id_bioguide)) %>%\n  mutate(term_end = \n           case_when(term_type == 'rep' ~ term_start + years(2),\n                     term_type == 'sen' ~ term_start + years(6))) %>%\n  select(id_bioguide, first_term, term_start, term_end)  %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\n\n\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-06\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-04\n\n\nF000062\n1992-11-10\n1992-11-10\n1998-11-10\n\n\nF000469\n2019-01-03\n2019-01-03\n2021-01-03\n\n\nK000367\n2007-01-04\n2007-01-04\n2013-01-04\n\n\nM000639\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nS000033\n1991-01-03\n1991-01-03\n1993-01-03\n\n\n\n\n\n\nSELECT first_century, period\n,first_value(cohort_retained) over (partition by first_century order by period) as cohort_size\n,cohort_retained\n,cohort_retained * 1.0 / \n first_value(cohort_retained) over (partition by first_century order by period) as pct_retained\nFROM\n(\n        SELECT date_part('century',a.first_term) as first_century\n        ,coalesce(date_part('year',age(c.date,a.first_term)),0) as period\n        ,count(distinct a.id_bioguide) as cohort_retained\n        FROM\n        (\n                SELECT id_bioguide, min(term_start) as first_term\n                FROM legislators_terms \n                GROUP BY 1\n        ) a\n        JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n        LEFT JOIN date_dim c on c.date between b.term_start and b.term_end \n        and c.month_name = 'December' and c.day_of_month = 31\n        GROUP BY 1,2\n) aa\nORDER BY 1,2\n;\n\n\nDisplaying records 1 - 10\n\n\nfirst_century\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n18\n0\n368\n368\n1.0000000\n\n\n18\n1\n368\n360\n0.9782609\n\n\n18\n2\n368\n242\n0.6576087\n\n\n18\n3\n368\n233\n0.6331522\n\n\n18\n4\n368\n149\n0.4048913\n\n\n18\n5\n368\n144\n0.3913043\n\n\n18\n6\n368\n99\n0.2690217\n\n\n18\n7\n368\n101\n0.2744565\n\n\n18\n8\n368\n73\n0.1983696\n\n\n18\n9\n368\n70\n0.1902174"
  },
  {
    "objectID": "chapter_5.html#why-text-analysis-with-sql",
    "href": "chapter_5.html#why-text-analysis-with-sql",
    "title": "5  Text Analysis",
    "section": "5.1 Why Text Analysis with SQL",
    "text": "5.1 Why Text Analysis with SQL\nI would actually reframe this question as “why store text data in a database?” and offer different reasons from those offered in Tanimura (2021). To structure my answer I will use a representative textual analysis problem (really set of problems) I’ve managed in the past.\nPublic companies routinely communicate with investors or their representatives through conference calls. Most public companies hold conference calls when they announce their earnings results for a quarter or year. The typical earnings conference call starts with a presentation of results by management, typically the CEO or CEO, followed by the “Q&A” portion of the call during which call participants can ask questions of management Apart from representatives of the company, the typical participant in a conference call is an equity analyst. Equity analysts typically cover a relatively small numbers of companies, typically in a single industry, and provide insights and investment recommendations and related to their covered companies and industries.\nAnalyst recommendations usually come from valuation analyses that draw on projections of future financial performance. An analyst’s valuation model is usually constructed using a spreadsheet and to some extent an analyst’s questions on a conference call will seek information that can be used for model inputs.\nTranscripts of conference calls are collected by a number of data providers, who presumably supply them to various users, including investors and academic researchers. I have used transcripts of conference calls in my own research. The data provider in my case provided a continuous stream of transcripts in XML files. Each call is contained in its own XML file with a file name that indicates the unique identifier of the call. Some elements of the call are contained in structured XML, but the bulk of the data in a call are in a single unstructured XML field.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\n\n\npg <- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)\n\n\nufo <- tbl(pg, \"ufo\")\n\nufo %>%\n  mutate(length = length(sighting_report)) %>%\n  ggplot(aes(x = length)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\nufo <- \n  tbl(pg, \"ufo\") %>% \n  mutate(id = row_number())\n\nregex <- paste0(\"Occurred :\\\\s*(.*)\\\\s*\",\n                \"Reported:\\\\s*(.* [AP]M).*?\\\\s*\",\n                \"Posted:\\\\s*(.*)\\\\s*\",\n                \"Location:\\\\s*(.*)\\\\s*\",\n                \"Shape:\\\\s*(.*)\\\\s*\",\n                \"Duration:\\\\s*(.*)\\\\s*\")\n\nregex2 <- paste0(\"^(.*?)\",\n                 \"(?:\\\\s*\\\\(Entered as :\\\\s*(.*)\\\\))?\\\\s*$\")\n\nufo_extracted <- \n  ufo %>%\n  mutate(matches = regexp_matches(sighting_report, regex)) %>%\n  mutate(occurred_plus = matches[[1]],\n         reported = matches[[2]],\n         posted = matches[[3]],\n         location = matches[[4]],\n         shape = matches[[5]],\n         duration = matches[[6]]) %>%\n  select(id, occurred_plus:duration) %>%\n  mutate(occurred_plus = regexp_matches(occurred_plus, regex2)) %>%\n  mutate(occurred_raw = occurred_plus[[1]],\n         entered = occurred_plus[[2]]) %>%\n  select(-occurred_plus) %>%\n  mutate(location_clean = regexp_replace(location, \n                                         \"(outside of|close to)\", \"near\")) %>%\n  mutate(occurred = case_when(occurred_raw == '' ~ NA,\n                              length(occurred_raw) < 8 ~ NA,\n                              TRUE ~ as.POSIXct(occurred_raw)),\n         reported = case_when(reported == '' ~ NA,\n                              length(reported) < 8 ~ NA,\n                              TRUE ~ as.POSIXct(reported)),\n         posted = if_else(posted == '', NA, as.Date(posted)),\n         shape = initcap(shape)) %>%\n  collect()\n\n\nufo_extracted %>%\n  ggplot(aes(y = fct_rev(fct_infreq(shape)))) +\n  geom_bar() +\n  ylab(\"Shape\")\n\n\n\n\n\nufo_extracted %>%\n  filter(!is.na(occurred_raw)) %>%\n  count(occurred_raw) %>%\n  arrange(desc(n)) %>%\n  slice_head(n = 10) %>%\n  mutate(occurred_raw = fct_rev(fct_inorder(as.character(occurred_raw)))) %>%\n  ggplot(aes(y = occurred_raw, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nufo_extracted %>%\n  count(duration) %>%\n  arrange(desc(n)) %>%\n  slice_head(n = 10) %>%\n  mutate(duration = fct_rev(fct_inorder(duration))) %>%\n  ggplot(aes(y = duration, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nufo_extracted %>%\n  count(location) %>%\n  arrange(desc(n)) %>%\n  slice_head(n = 10) %>%\n  mutate(location = fct_rev(fct_inorder(location))) %>%\n  ggplot(aes(y = location, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_6.html#graphing-to-find-anomalies-visually",
    "href": "chapter_6.html#graphing-to-find-anomalies-visually",
    "title": "6  Anomaly Detection",
    "section": "6.1 Graphing to find anomalies visually",
    "text": "6.1 Graphing to find anomalies visually\n\nearthquakes %>%\n  filter(!is.na(mag)) %>%\n  ggplot(aes(x = mag)) +\n  geom_histogram(breaks = seq(-10, 10, 0.1))\n\n\n\n\n\nearthquakes %>%\n  filter(!is.na(mag),\n         between(mag, 7.2, 9.5)) %>%\n  ggplot(aes(x = mag)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\nearthquakes %>%\n  filter(!is.na(mag),\n         between(mag, 7.2, 9.5)) %>%\n  ggplot(aes(x = mag)) +\n  geom_bar() +\n  scale_x_binned(breaks = seq(7.2, 9.5, 0.1))\n\n\n\n\n\nearthquakes %>%\n  filter(!is.na(mag), !is.na(depth)) %>%\n  distinct(mag, depth) %>%\n  ggplot(aes(x = mag, y = depth)) +\n  geom_point(size = 0.1, colour = \"blue\")\n\n\n\n\n\nearthquakes %>%\n  filter(!is.na(mag), !is.na(depth)) %>%\n  filter(between(mag, 4, 7), depth <= 50) %>%\n  ggplot(aes(x = mag, y = depth)) +\n  geom_count(color = \"blue\")\n\n\n\n\n\njapan_quakes <-\n  earthquakes %>%\n  filter(!is.na(mag), !is.na(depth)) %>%\n  filter(grepl(\"Japan\", place)) \n\njapan_quakes %>%\n  ggplot(aes(y = mag)) +\n  geom_boxplot(width = 0.5)\n\n\n\n\n\njapan_quakes %>%\n  summarize(p25 = quantile(mag, probs = 0.25, na.rm = TRUE),\n            p50 = quantile(mag, probs = 0.50, na.rm = TRUE),\n            p75 = quantile(mag, probs = 0.75, na.rm = TRUE)) %>%\n  mutate(iqr = (p75 - p25) * 1.5,\n         lower_whisker = p25 - (p75 - p25) * 1.5,\n         upper_whisker = p75 + (p75 - p25) * 1.5) %>%\n  kable()\n\n\n\n\np25\np50\np75\niqr\nlower_whisker\nupper_whisker\n\n\n\n\n4.3\n4.5\n4.7\n0.6\n3.7\n5.3\n\n\n\n\n\n\njapan_quakes %>%\n  select(mag, time) %>%\n  collect() %>%\n  mutate(year = as.factor(year(time))) %>%\n  ggplot(aes(y = mag, x = year, group = year)) +\n  geom_boxplot()"
  },
  {
    "objectID": "chapter_6.html#forms-of-anomalies",
    "href": "chapter_6.html#forms-of-anomalies",
    "title": "6  Anomaly Detection",
    "section": "6.2 Forms of Anomalies",
    "text": "6.2 Forms of Anomalies\n\n6.2.1 Anomalous Values\n\nearthquakes %>%\n  filter(mag >= 1.08) %>%\n  group_by(mag) %>%\n  summarize(count = n()) %>%\n  arrange(mag) %>%\n  collect(n = 5) %>%\n  kable()\n\n\n\n\nmag\ncount\n\n\n\n\n1.08\n3863\n\n\n1.08\n1\n\n\n1.09\n3712\n\n\n1.10\n39728\n\n\n1.11\n3674\n\n\n\n\n\n\nearthquakes %>%\n  filter(depth > 600) %>%\n  group_by(net) %>%\n  summarize(count = n()) %>%\n  arrange(net) %>%\n  collect(n = 5) %>%\n  kable()\n\n\n\n\nnet\ncount\n\n\n\n\nus\n1215\n\n\n\n\n\n\nearthquakes %>%\n  filter(depth > 600) %>%\n  group_by(place) %>%\n  summarize(count = n()) %>%\n  arrange(place) %>%\n  collect(n = 5) %>%\n  kable()\n\n\n\n\nplace\ncount\n\n\n\n\n100km NW of Ndoi Island, Fiji\n1\n\n\n100km SSW of Ndoi Island, Fiji\n1\n\n\n100km SW of Ndoi Island, Fiji\n1\n\n\n101km ENE of Suva, Fiji\n1\n\n\n101km NNE of Ndoi Island, Fiji\n1\n\n\n\n\n\n\nearthquakes %>%\n  filter(depth > 600) %>%\n  mutate(place_name = case_when(grepl(' of ', place) ~\n                                  split_part(place, ' of ', 2L),\n                                TRUE ~ place)) %>%\n  group_by(place_name) %>%\n  summarize(count = n()) %>%\n  arrange(desc(count)) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nplace_name\ncount\n\n\n\n\nNdoi Island, Fiji\n487\n\n\nFiji region\n186\n\n\nLambasa, Fiji\n140\n\n\n\n\n\n\nearthquakes %>%\n  summarize(distinct_types = n_distinct(type),\n            distinct_lower = n_distinct(lower(type))) %>%\n  kable()\n\n\n\n\ndistinct_types\ndistinct_lower\n\n\n\n\n25\n24\n\n\n\n\n\n\n\n6.2.2 Anomalous Counts or Frequencies\nWhy use date_trunc('year',time)::date as earthquake_year?\n\nSELECT EXTRACT(year FROM time) AS earthquake_year, \n  COUNT(*) AS earthquakes\nFROM earthquakes\nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nearthquake_year\nearthquakes\n\n\n\n\n2010\n122322\n\n\n2011\n107397\n\n\n2012\n105693\n\n\n2013\n114368\n\n\n2014\n135247\n\n\n2015\n122914\n\n\n2016\n122420\n\n\n2017\n130622\n\n\n2018\n179304\n\n\n2019\n171116\n\n\n\n\n\n\nearthquakes %>%\n  mutate(earthquake_year = as.character(year(time))) %>%\n  group_by(earthquake_year) %>%\n  summarize(earthquakes = n()) %>%\n  ggplot(aes(x = earthquake_year, y = earthquakes)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n2\n\nearthquakes %>%\n  mutate(earthquake_year = as.character(year(time))) %>%\n  select(earthquake_year) %>%\n  ggplot(aes(x = earthquake_year)) +\n  geom_bar()\n\n\nearthquakes %>%\n  mutate(earthquake_month = floor_date(time, \"month\")) %>%\n  group_by(earthquake_month) %>%\n  summarize(earthquakes = n(), .groups = \"drop\") %>%\n  ggplot(aes(x = earthquake_month, y = earthquakes)) +\n  geom_line()\n\n\n\n\nFrom the book, “it turns out that the increase in earthquakes starting in 2017 can be at least partially explained by the status field. The status indicates whether the event has been reviewed by a human (‘reviewed’) or was directly posted by a system without review (‘automatic’).” This can be seen in the following plot.3\n\nearthquakes %>%\n  mutate(earthquake_month = floor_date(time, \"month\")) %>%\n  group_by(earthquake_month, status) %>%\n  summarize(earthquakes = n(), .groups = \"drop\") %>%\n  ggplot(aes(x = earthquake_month, y = earthquakes, color = status)) +\n  geom_line()\n\n\n\n\n\nearthquakes %>%\n  filter(mag >= 6) %>%\n  group_by(place) %>%\n  summarize(earthquakes = n(), .groups = \"drop\") %>%\n  arrange(desc(earthquakes)) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nplace\nearthquakes\n\n\n\n\nnear the east coast of Honshu, Japan\n52\n\n\noff the east coast of Honshu, Japan\n34\n\n\nVanuatu\n28\n\n\n\n\n\nFor the next query, it seems easy enough to just put the result in a plot.\n\nearthquakes %>%\n  filter(mag >= 6) %>%\n  mutate(place = if_else(grepl(' of ', place),\n                         split_part(place, ' of ', 2L), \n                         place)) %>%\n  count(place, name = \"earthquakes\") %>%\n  arrange(desc(earthquakes)) %>%\n  collect(n = 10) %>%\n  ggplot(aes(y = fct_inorder(place), \n             x = earthquakes)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n6.2.3 Anomalies from the Absence of Data"
  },
  {
    "objectID": "chapter_6.html#handling-anomalies",
    "href": "chapter_6.html#handling-anomalies",
    "title": "6  Anomaly Detection",
    "section": "6.3 Handling Anomalies",
    "text": "6.3 Handling Anomalies\n\n6.3.1 Investigation\n\n\n6.3.2 Removal\n\nearthquakes %>%\n  filter(!mag %in% c(-9,-9.99)) %>%\n  select(time, mag, type) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\ntime\nmag\ntype\n\n\n\n\n2010-01-01 00:10:43\n1.60\nearthquake\n\n\n2010-01-01 00:16:49\n2.20\nearthquake\n\n\n2010-01-01 00:16:54\n1.10\nearthquake\n\n\n2010-01-01 00:29:29\n1.40\nice quake\n\n\n2010-01-01 00:30:16\n0.75\nearthquake\n\n\n2010-01-01 00:31:40\n1.00\nearthquake\n\n\n2010-01-01 00:34:29\n0.46\nearthquake\n\n\n2010-01-01 00:55:05\n2.40\nearthquake\n\n\n2010-01-01 00:57:31\n0.60\nearthquake\n\n\n2010-01-01 01:09:53\n0.20\nearthquake\n\n\n\n\n\n\nearthquakes %>%\n  summarize(avg_mag = avg(mag),\n            avg_mag_adjusted = avg(if_else(mag > -9, mag, NA))) %>%\n  kable()\n\n\n\n\navg_mag\navg_mag_adjusted\n\n\n\n\n1.625101\n1.627323\n\n\n\n\n\n\nearthquakes %>%\n  filter(place == 'Yellowstone National Park, Wyoming') %>%\n  summarize(avg_mag = avg(mag),\n            avg_mag_adjusted = avg(if_else(mag > -9, mag, NA))) %>%\n  kable()\n\n\n\n\navg_mag\navg_mag_adjusted\n\n\n\n\n0.4063935\n0.9233279\n\n\n\n\n\n\n\n6.3.3 Replacement with Alternate Values\n\nearthquakes %>%\n  mutate(event_type = if_else(type == 'earthquake', type, 'Other')) %>%\n  count(event_type) %>%\n  kable()\n\n\n\n\nevent_type\nn\n\n\n\n\nearthquake\n1461750\n\n\nOther\n34176\n\n\n\n\n\n\nextremes <-\n  earthquakes %>%\n  summarize(p95 = quantile(mag, probs = 0.95, na.rm = TRUE),\n            p05 = quantile(mag, probs = 0.05, na.rm = TRUE))\n\nextremes %>% kable()\n\n\n\n\np95\np05\n\n\n\n\n4.5\n0.12\n\n\n\n\n\nNote that this SQL from the book4\n\nCASE \n  WHEN mag > p95 THEN p95\n  WHEN mag < p05 THEN p05\n  ELSE mag\nEND AS mag_winsorized\n\ncan be replaced with a single line:\n\nLEAST(GREATEST(mag, p05), p95) AS mag_winsorized\n\nThe R equivalents of LEAST and GREATEST are pmin and pmax, respectively. And dbplyr will translate pmin and pmax for us, so we can get winsorized data as follows.\n\nearthquakes_wins <-\n  earthquakes %>%\n  filter(!is.na(mag)) %>%\n  cross_join(extremes) %>%\n  mutate(mag_winsorized = pmin(pmax(mag, p05), p95)) %>%\n  select(time, place, mag, mag_winsorized) \n\nearthquakes_wins %>%\n  arrange(desc(mag)) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2011-03-11 05:46:24\n2011 Great Tohoku Earthquake, Japan\n9.1\n4.5\n\n\n2010-02-27 06:34:11\noffshore Bio-Bio, Chile\n8.8\n4.5\n\n\n2012-04-11 08:38:36\noff the west coast of northern Sumatra\n8.6\n4.5\n\n\n\n\nearthquakes_wins %>%\n  filter(mag == mag_winsorized) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2010-01-01 00:10:43\nSouthern Alaska\n1.6\n1.6\n\n\n2010-01-01 00:16:49\nWashington\n2.2\n2.2\n\n\n2010-01-01 00:16:54\n15km E of Coso Junction, CA\n1.1\n1.1\n\n\n\n\nearthquakes_wins %>%\n  arrange(mag) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2010-01-17 23:06:41\nYellowstone National Park, Wyoming\n-9.99\n0.12\n\n\n2010-01-17 21:55:54\nYellowstone National Park, Wyoming\n-9.99\n0.12\n\n\n2010-01-18 14:12:08\nYellowstone National Park, Wyoming\n-9.99\n0.12\n\n\n\n\n\n\n\n6.3.4 Rescaling\nIn the book, it says WHERE depth >= 0.05, but I need to use WHERE depth > 0.05 to match the results there.\n\nquake_depths <-\n  earthquakes %>%\n  filter(depth > 0.05) %>%\n  mutate(depth = sql(\"round(depth, 1)\")) %>%\n  select(depth)\n\nquake_depths %>%\n  count(depth)\n\n# Source:   SQL [?? x 2]\n# Database: postgres  [iangow@/tmp:5432/iangow]\n   depth     n\n   <dbl> <int>\n 1   0.1  6994\n 2   0.2  6876\n 3   0.3  7269\n 4   0.4  7684\n 5   0.5  7488\n 6   0.6  7729\n 7   0.7  8067\n 8   0.8  8360\n 9   0.9  8515\n10   1   10220\n# ℹ more rows\n\nquake_depths %>%\n  ggplot(aes(x = depth)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\nquake_depths %>%\n  ggplot(aes(x = log(depth, base = 10))) +\n  geom_histogram(binwidth = 0.1)"
  },
  {
    "objectID": "chapter_7.html#strengths-and-limits-of-experiment-analysis-with-sql",
    "href": "chapter_7.html#strengths-and-limits-of-experiment-analysis-with-sql",
    "title": "7  Experiment Analysis",
    "section": "7.1 Strengths and Limits of Experiment Analysis with SQL",
    "text": "7.1 Strengths and Limits of Experiment Analysis with SQL\nI would reframe this discussion. We might be using a database because the results of an experiment are stored there.\nAn alternative approach might have the results of an experiment are extracted and exported to a CSV or Excel file and attached to an email and sent to you for analysis.1 But extracted from where? If the data are in a database, it would be better to cut out the middleman and just get the data directly.\nThe limitations of SQL essentially vanish when we access the data using dbplyr. R can do any statistical calculation we can think of, so we have no need of an online statistical calculator (though such calculators can help us frame our analyses and check our results).\nCathy does say that “many databases allow developers to extend SQL functionality with user-defined functions (UDFs) … but they are beyond the scope of this book.” For PostgreSQL there are PL/Python and PL/R, which allow creation of functions in Python and R, respectively. When I first started to use PostgreSQL, these extensions seemed pretty exciting and, because I was maintaining my own databases, I could use them. But over time, I found maintaining UDFs to be more effort than could be justified and I no longer use them. Instead, if I need to do analysis in Python or R, I will extract data from the database, do the analysis, then write data back to the database. While this likely partly reflects the kinds of analysis I do, I think that UDFs are likely to be off limits to most users because of the additional complexity of turning code into UDFs and because many users would lack sufficient database privileges to create these."
  },
  {
    "objectID": "chapter_7.html#the-data-set",
    "href": "chapter_7.html#the-data-set",
    "title": "7  Experiment Analysis",
    "section": "7.2 The Data Set",
    "text": "7.2 The Data Set\nAs discussed in Tanimura (2021), there are four tables used in this chapter. Here we set up references to each of these tables.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\nlibrary(flextable)\nlibrary(janitor)\n\n\npg <- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)\ndbExecute(pg, \"SET search_path TO sql_book\")\n\n\ngame_users <- tbl(pg, \"game_users\")\ngame_actions <- tbl(pg, \"game_actions\")\ngame_purchases <- tbl(pg, \"game_purchases\")\nexp_assignment <- tbl(pg, \"exp_assignment\")"
  },
  {
    "objectID": "chapter_7.html#types-of-experiments",
    "href": "chapter_7.html#types-of-experiments",
    "title": "7  Experiment Analysis",
    "section": "7.3 Types of Experiments",
    "text": "7.3 Types of Experiments\nI would reword the first paragraph here to the following for clarity (edits in italics):\n\nThere is a wide range of experiments, If you can change something that a user, customer, constituent, or other entity experiences, you can in theory test the effect of that change on some outcome.\n\n\n7.3.1 Experiments wtih Binary Outcomes: The Chi-Squared Test\nTo better match the approach of the book, I essentially create the contingency table in the database. An alternative approach would have been to collect() after summarize() and then do the statistical analysis in R. In fact, many of the functions in R are better set-up for this approach. However, this means bring more data into R and doing more calculation in R. If the experiment is very large or you would rather the database server do more of the work, then the approach below may be preferred.\n\ncont_tbl <-\n  exp_assignment %>%\n  left_join(game_actions, by = \"user_id\") %>%\n  group_by(variant, user_id) %>%\n  summarize(completed = \n              coalesce(any(action == \"onboarding complete\", na.rm = TRUE),\n                       FALSE),\n            .groups = \"drop\") %>%\n  count(variant, completed) %>%\n  mutate(completed = if_else(completed, \"Yes\", \"No\")) %>%\n  pivot_wider(names_from = completed, values_from = n) %>%\n  collect()\n\nUsing the packages janitor and flextable, I mimic the nicely formatted output shown in the book:\n\ncont_tbl %>%\n  adorn_totals(c(\"row\", \"col\")) %>%\n  mutate(`% Complete` = prettyNum(Yes/Total * 100, digits = 4)) %>%\n  flextable() %>%\n  add_header_row(values=c(\"\", \"Completed onboarding\", \"\"),\n                 colwidths = c(1, 2, 2))\n\n\nCompleted onboardingvariantNoYesTotal% Completecontrol13,62936,26849,89772.69variant 111,99538,28050,27576.14Total25,62474,548100,17274.42\n\n\nNow I can do the Chi-squared test. I need to turn variant into the row names of the contingency table so that we want a simple \\(2 \\times 2\\) numeric table as the input to our statistical test and I use column_to_rownames() to this end. I then pipe the result into the base R function chisq.test(). I specified correct = FALSE so that my result matched what I got from the online calculator I found. I then display the Chi-squared statistic and the \\(p\\)-value.\n\nres <- \n  cont_tbl %>%\n  column_to_rownames(var = \"variant\") %>%\n  chisq.test(correct = FALSE)\n\nres$statistic\n\nX-squared \n 157.0758 \n\nres$p.value\n\n[1] 4.926953e-36\n\n\n\n\n7.3.2 Experiments with Continuous Outcomes: The t-Test\n\namounts <- \n  exp_assignment %>%\n  filter(exp_name == 'Onboarding') %>%\n  left_join(game_purchases, by = \"user_id\") %>%\n  group_by(variant, user_id) %>%\n  summarize(amount = sum(coalesce(amount, 0), na.rm = TRUE),\n            .groups = \"drop\")\n\nt_test_stats <-\n  amounts %>%\n  group_by(variant) %>%\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) %>%\n  collect()\n\nt_test_stats %>%\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n50275\n3.688\n19.22\n\n\ncontrol\n49897\n3.781\n18.94\n\n\n\n\n\nWe can make a small function that we can pass a data frame to as df. The calculations assume that df contains two rows (one for each group) and columns named mean, sd, and n for the mean, standard deviation, and number of observations, respectively, in each group.\n\nt_test <- function(df) {\n  mean_diff = abs(df$mean[1] - df$mean[2])\n  se_diff <- sqrt(sum(df$sd^2 / df$n))\n  t_stat <- mean_diff / se_diff\n  p <- pt(t_stat, df = sum(df$n))\n  p_val <- 2 * min(p, 1 - p)\n  return(list(\"statistic\" = t_stat, \"p-value\" = p_val))\n}\n\nt_test(t_test_stats)\n\n$statistic\n[1] 0.7765458\n\n$`p-value`\n[1] 0.4374286\n\n\nThese values line up with those obtained from the online calculator I found.\nAn alternative approach would be to collect() the underlying data and do the \\(t\\)-test in R.\n\nt_test_data <-\n  amounts %>%\n  select(variant, amount) %>%\n  collect()\n\nt.test(formula = amount ~ variant, data = t_test_data)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by variant\nt = 0.77655, df = 100165, p-value = 0.4374\nalternative hypothesis: true difference in means between group control and group variant 1 is not equal to 0\n95 percent confidence interval:\n -0.1426893  0.3299478\nsample estimates:\n  mean in group control mean in group variant 1 \n               3.781218                3.687589 \n\n\n\nt_test_stats_2 <-\n  amounts %>%\n  inner_join(game_actions, by = \"user_id\") %>%\n  filter(action == \"onboarding complete\") %>%\n  group_by(variant) %>%\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) %>%\n  collect()\n\nt_test_stats_2 %>%\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n38280\n4.843\n21.899\n\n\ncontrol\n36268\n5.202\n22.049\n\n\n\n\n\n\nt_test(t_test_stats_2)\n\n$statistic\n[1] 2.229649\n\n$`p-value`\n[1] 0.02577371"
  },
  {
    "objectID": "chapter_7.html#challenges-with-experiments-and-options-for-rescuing-flawed-experiments",
    "href": "chapter_7.html#challenges-with-experiments-and-options-for-rescuing-flawed-experiments",
    "title": "7  Experiment Analysis",
    "section": "7.4 Challenges with Experiments and Options for Rescuing Flawed Experiments",
    "text": "7.4 Challenges with Experiments and Options for Rescuing Flawed Experiments\n\n7.4.1 Variant Assignment\n\n\n7.4.2 Outliers\n\nexp_assignment %>%\n  left_join(game_purchases, by = \"user_id\", keep = TRUE,\n            suffix = c(\"\", \".y\")) %>%\n  inner_join(game_actions, by = \"user_id\") %>%\n  filter(action == \"onboarding complete\",\n         exp_name == 'Onboarding') %>%\n  group_by(variant) %>%\n  summarize(total_cohorted = n_distinct(user_id),\n            purchasers = n_distinct(user_id.y),\n            .groups = \"drop\") %>%\n  mutate(pct_purchased = purchasers * 100.0 / total_cohorted) %>%\n  kable(digits = 2)\n\n\n\n\nvariant\ntotal_cohorted\npurchasers\npct_purchased\n\n\n\n\ncontrol\n36268\n4988\n13.75\n\n\nvariant 1\n38280\n4981\n13.01\n\n\n\n\n\n\n\n7.4.3 Time Boxing\n\namounts_boxed <- \n  exp_assignment %>%\n  filter(exp_name == 'Onboarding') %>%\n  mutate(exp_end = exp_date + days(7)) %>%\n  left_join(game_purchases, \n            by = join_by(user_id, exp_end >= purch_date)) %>%\n  group_by(variant, user_id) %>%\n  summarize(amount = sum(coalesce(amount, 0), na.rm = TRUE),\n            .groups = \"drop\")\n\n\nt_test_stats_boxed <-\n  amounts_boxed %>%\n  group_by(variant) %>%\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) %>%\n  collect()\n\nt_test_stats_boxed %>%\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n50275\n1.352\n5.613\n\n\ncontrol\n49897\n1.369\n5.766\n\n\n\n\n\n\nt_test(t_test_stats_boxed)\n\n$statistic\n[1] 0.4920715\n\n$`p-value`\n[1] 0.6226699\n\n\n\n\n7.4.4 Pre-Post Analysis\n\nSELECT \n  CASE WHEN a.created BETWEEN '2020-01-13' AND '2020-01-26' THEN 'pre'\n     when a.created BETWEEN '2020-01-27' AND '2020-02-09' THEN 'post'\n     end as variant,\n  count(distinct a.user_id) AS cohorted,\n  count(distinct b.user_id) AS opted_in,\n  count(distinct b.user_id) * 1.0 / count(DISTINCT a.user_id) AS pct_optin,\n  count(distinct a.created) as days\nFROM game_users a\nLEFT JOIN game_actions b on a.user_id = b.user_id \n  AND b.action = 'email_optin'\nWHERE a.created BETWEEN '2020-01-13' AND '2020-02-09'\nGROUP BY 1\n;\n\n\n2 records\n\n\nvariant\ncohorted\nopted_in\npct_optin\ndays\n\n\n\n\npost\n27617\n11220\n0.4062715\n14\n\n\npre\n24662\n14489\n0.5875030\n14\n\n\n\n\n\n\nopt_in_df <-\n  game_users %>%\n  filter(between(created, '2020-01-13', '2020-02-09')) %>%\n  left_join(game_actions %>% \n              filter(action == 'email_optin'), by = \"user_id\",\n            keep = TRUE,\n            suffix = c(\"\", \".y\")) %>%\n  mutate(variant = if_else(created <= '2020-01-26', 'pre', 'post')) %>%\n  group_by(variant) %>%\n  summarize(cohorted = n_distinct(user_id),\n            opted_in = n_distinct(user_id.y),\n            days = n_distinct(created),\n            .groups = \"drop\") %>%\n  mutate(not_opted_in = cohorted - opted_in) %>%\n  collect()\n\n\nopt_in_df %>%\n  select(variant, days, opted_in, not_opted_in, cohorted) %>%\n  adorn_totals(c(\"row\")) %>%\n  mutate(`% opted in` = prettyNum(100.0 * opted_in/cohorted, digits = 4)) %>%\n  rename(Yes = opted_in, No = not_opted_in, Total = cohorted) %>%\n  flextable() %>%\n  add_header_row(values=c(\"\", \"Opted in\", \"\"),\n                 colwidths = c(2, 2, 2))\n\n\nOpted invariantdaysYesNoTotal% opted inpost1411,22016,39727,61740.63pre1414,48910,17324,66258.75Total2825,70926,57052,27949.18\n\n\n\nres <- \n  opt_in_df %>% \n  select(-cohorted, -days) %>%\n  column_to_rownames(var = \"variant\") %>%\n  chisq.test(correct = FALSE)\n\nres$statistic\n\nX-squared \n 1712.075 \n\nres$p.value\n\n[1] 0\n\n\n\n\n7.4.5 Natural experiment analysis\n\nby_country <- \n  game_users %>%\n  filter(country %in% c('United States', 'Canada')) %>%\n  left_join(game_purchases, by = \"user_id\",\n            keep = TRUE,\n            suffix = c(\"\", \".y\")) %>%\n  group_by(country) %>%\n  summarize(cohorted = n_distinct(user_id),\n            opted_in = n_distinct(user_id.y),\n            .groups = \"drop\") %>%\n  mutate(pct_purchased = 1.0 * opted_in/cohorted) %>%\n  collect()\n\nby_country %>% kable()\n\n\n\n\ncountry\ncohorted\nopted_in\npct_purchased\n\n\n\n\nCanada\n20179\n5011\n0.2483275\n\n\nUnited States\n45012\n4958\n0.1101484\n\n\n\n\n\n\nres <- \n  by_country %>% \n  select(-pct_purchased) %>%\n  column_to_rownames(var = \"country\") %>%\n  chisq.test(correct = FALSE)\n\nres$statistic\n\nX-squared \n 1447.273 \n\nres$p.value\n\n[1] 1.122178e-316\n\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_8.html#when-to-use-sql-for-complex-data-sets",
    "href": "chapter_8.html#when-to-use-sql-for-complex-data-sets",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.1 When to Use SQL for Complex Data Sets",
    "text": "8.1 When to Use SQL for Complex Data Sets\n\n8.1.1 Advantages of Using SQL\n\n\n8.1.2 When to Build into ETL Instead\n\n\n8.1.3 When to Put Logic in Other Tools\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\n\n\npg <- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)"
  },
  {
    "objectID": "chapter_8.html#code-organization",
    "href": "chapter_8.html#code-organization",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.2 Code Organization",
    "text": "8.2 Code Organization\n\nearthquakes <- tbl(pg, \"earthquakes\")\nlegislators_terms <- tbl(pg, \"legislators_terms\")\nvideogame_sales <- tbl(pg, \"videogame_sales\")\n\n\nearthquakes %>%\n  filter(date_part('year', time) >= 2019,\n         between(mag, 0, 1)) %>%\n  mutate(place = case_when(grepl('CA', place) ~ 'California',\n                           grepl('AK', place) ~ 'Alaska',\n                           TRUE ~ trim(split_part(place, ',', 2L)))) %>%\n  count(place, type, mag) %>%\n  arrange(desc(n)) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nplace\ntype\nmag\nn\n\n\n\n\nAlaska\nearthquake\n1.00\n4824\n\n\nAlaska\nearthquake\n0.90\n4153\n\n\nAlaska\nearthquake\n0.80\n3219\n\n\nCalifornia\nearthquake\n0.56\n2631\n\n\nAlaska\nearthquake\n0.70\n2061\n\n\nNevada\nearthquake\n1.00\n1688\n\n\nNevada\nearthquake\n0.80\n1681\n\n\nNevada\nearthquake\n0.90\n1669\n\n\nAlaska\nearthquake\n0.60\n1464\n\n\nCalifornia\nearthquake\n0.55\n1444\n\n\n\n\n\n\n8.2.1 Commenting\n\n\n8.2.2 Capitalization, Indentation, Parentheses, and Other Formatting Tricks\n\n\n8.2.3 Storing Code"
  },
  {
    "objectID": "chapter_8.html#organizing-computations",
    "href": "chapter_8.html#organizing-computations",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.3 Organizing Computations",
    "text": "8.3 Organizing Computations\n\n8.3.1 Understanding Order of SQL Clause Evaluation\nIn general, I think the order of evaluation is more intuitive when writing SQL using dbplyr. It is perhaps more confusing to someone who has written a lot of SQL without thinking about the order things are evaluated (e.g., WHERE comes early in evaluation, but relatively late in the SQL).\nIn dplyr the meaning of filter() is usually clear by where it is placed. Some times it is translated into WHERE and some times it is HAVING. In the example below, a WHERE-like filter would be placed just after legislators_terms (much as it is evaluated by the SQL query engine), while in this case we have filter being translated into HAVING because it is using the result of GROUP BY query. One hardly need give this much thought.\n\nterms_by_states <-\n  legislators_terms %>%\n  group_by(state) %>%\n  summarize(terms = n()) %>%\n  filter(terms >= 1000) %>%\n  arrange(desc(terms))\n\nterms_by_states %>%\n  show_query()\n\n<SQL>\nSELECT \"state\", COUNT(*) AS \"terms\"\nFROM \"legislators_terms\"\nGROUP BY \"state\"\nHAVING (COUNT(*) >= 1000.0)\nORDER BY \"terms\" DESC\n\nterms_by_states %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nstate\nterms\n\n\n\n\nNY\n4159\n\n\nPA\n3252\n\n\nOH\n2239\n\n\nCA\n2121\n\n\nIL\n2011\n\n\nTX\n1692\n\n\nMA\n1667\n\n\nVA\n1648\n\n\nNC\n1351\n\n\nMI\n1284\n\n\n\n\n\nThe way dplyr code is written makes it easy to look at the output each step in the series of pipes. In the query below, we can easily highlight the code up to the end of any pipe and evaluate it to see if it is doing what we want and expect it be doing. In this specific case, I find the dplyr code to be more intuitive than the SQL provided in the book, which uses an `a\n\nlegislators_terms %>%\n  group_by(state) %>%\n  summarize(terms = n(), .groups = \"drop\") %>%\n  mutate(avg_terms = mean(terms, na.rm = TRUE)) %>%\n  collect(n = 10) %>%\n  kable(digits = 2)\n\n\n\n\nstate\nterms\navg_terms\n\n\n\n\nDK\n16\n746.83\n\n\nND\n170\n746.83\n\n\nNV\n177\n746.83\n\n\nOH\n2239\n746.83\n\n\nGU\n24\n746.83\n\n\nNY\n4159\n746.83\n\n\nHI\n122\n746.83\n\n\nIN\n1177\n746.83\n\n\nNE\n379\n746.83\n\n\nWV\n428\n746.83\n\n\n\n\n\n\nlegislators_terms %>%\n  group_by(state) %>%\n  summarize(terms = n(), .groups = \"drop\") %>%\n  window_order(desc(terms)) %>%\n  mutate(rank = row_number()) %>%\n  arrange(rank) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nstate\nterms\nrank\n\n\n\n\nNY\n4159\n1\n\n\nPA\n3252\n2\n\n\nOH\n2239\n3\n\n\nCA\n2121\n4\n\n\nIL\n2011\n5\n\n\nTX\n1692\n6\n\n\nMA\n1667\n7\n\n\nVA\n1648\n8\n\n\nNC\n1351\n9\n\n\nMI\n1284\n10\n\n\n\n\n\n\n\n8.3.2 Subqueries\nIn writing dbplyr code, it is more natural to think in terms of CTEs, even though the code you write will generally be translated into SQL using subqueries.\nThe query in the book written with LATERAL seems much more confusing to me than the following. (Also, adding EXPLAIN to each query suggests that LATERAL is more complicated for PostgreSQL.) I rewrote the LATERAL query using CTEs and got the following, which seems closer to the second SQL query included in the book.\n\nWITH \n\ncurrent_legislators AS (\n  SELECT distinct id_bioguide, party\n  FROM legislators_terms\n  WHERE term_end > '2020-06-01'),\n  \nparty_changers AS (\n  SELECT b.id_bioguide, min(term_start) as first_term\n  FROM legislators_terms b\n  INNER JOIN current_legislators AS a\n  ON b.id_bioguide = a.id_bioguide AND b.party <> a.party\n  GROUP BY 1)\n\nSELECT date_part('year', first_term) as first_year, party,\n  count(id_bioguide) as legislators\nFROM current_legislators\nINNER JOIN party_changers\nUSING (id_bioguide)\nGROUP BY 1, 2;\n\n\n3 records\n\n\nfirst_year\nparty\nlegislators\n\n\n\n\n1979\nRepublican\n1\n\n\n2011\nLibertarian\n1\n\n\n2015\nDemocrat\n1\n\n\n\n\n\nTranslating the CTE version into dbplyr is a piece of cake.\n\ncurrent_legislators <-\n  legislators_terms %>%\n  filter(term_end > '2020-06-01') %>%\n  distinct(id_bioguide, party)\n\nparty_changers <-\n  legislators_terms %>%\n  inner_join(current_legislators, \n             join_by(id_bioguide)) %>%\n  filter(party.x != party.y) %>%\n  group_by(id_bioguide) %>%\n  summarize(first_term = min(term_start, na.rm = TRUE), .groups = \"drop\")\n\ncurrent_legislators %>%\n  inner_join(party_changers, by = \"id_bioguide\") %>%\n  mutate(first_year = date_part('year', first_term)) %>%\n  group_by(first_year, party) %>%\n  summarize(legislators = n(), .groups = \"drop\") %>%\n  collect() %>%\n  kable()\n\n\n\n\nfirst_year\nparty\nlegislators\n\n\n\n\n1979\nRepublican\n1\n\n\n2011\nLibertarian\n1\n\n\n2015\nDemocrat\n1\n\n\n\n\n\n\n\n8.3.3 Temporary Tables\nCreating temporary tables with dbplyr is easy: simply append compute() at the end of the table definition. Generally, dbplyr will take care of details that likely do not matter much, such as choosing a name for the table (we don’t care because we can refer to the table below as current_legislators regardless of the name chosen for it).\n\ncurrent_legislators %>%\n  show_query()\n\n<SQL>\nSELECT DISTINCT \"id_bioguide\", \"party\"\nFROM \"legislators_terms\"\nWHERE (\"term_end\" > '2020-06-01')\n\ncurrent_legislators <-\n  legislators_terms %>%\n  filter(term_end > '2020-06-01') %>%\n  distinct(id_bioguide, party) %>%\n  compute()\n\ncurrent_legislators %>%\n  show_query()\n\n<SQL>\nSELECT *\nFROM \"dbplyr_001\"\n\n\nCreating temporary tables can lead to significant performance gains in some situations, as the query optimizer has a simpler object to work with. Note that dbplyr allows the creating of an index with temporary tables, which can improve performance even further.\nNot that some database administrators do not allow users to create temporary tables. In such cases, you can often use collect() followed by copy_inline() effectively. (This is also useful when you have data outside the database—so collect() is not relevant—but want to merge it with data in the database.1)\n\n\n8.3.4 Common Table Expressions\nWe have been using them throughout the book already. For the sake of completeness, I rewrite the query given in the book here.\n\nfirst_term <- \n  legislators_terms %>%\n  group_by(id_bioguide) %>%\n  summarize(first_term = min(term_start, na.rm = TRUE),\n            .groups = \"drop\")\n    \nfirst_term %>%\n  inner_join(legislators_terms, by = \"id_bioguide\") %>%\n  mutate(periods = date_part('year', age(term_start, first_term))) %>%\n  group_by(periods) %>%\n  summarize(cohort_retained = n_distinct(id_bioguide)) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nperiods\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n3600\n\n\n2\n3619\n\n\n\n\n\n\n\n8.3.5 grouping sets\nI don’t think there is a “pure” dbplyr way of doing these. However, not all is lost for the dedicated dbplyr user, as the following examples demonstrate.\n\nglobal_sales <-\n  tbl(pg, sql(\"\n    SELECT platform, genre, publisher,\n      sum(global_sales) as global_sales\n    FROM videogame_sales\n    GROUP BY grouping sets (platform, genre, publisher)\"))\n\nglobal_sales %>%\n  arrange(desc(global_sales)) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nplatform\ngenre\npublisher\nglobal_sales\n\n\n\n\nNA\nNA\nNintendo\n1786.56\n\n\nNA\nAction\nNA\n1751.18\n\n\nNA\nSports\nNA\n1330.93\n\n\nPS2\nNA\nNA\n1255.64\n\n\nNA\nNA\nElectronic Arts\n1110.32\n\n\nNA\nShooter\nNA\n1037.37\n\n\nX360\nNA\nNA\n979.96\n\n\nPS3\nNA\nNA\n957.84\n\n\nNA\nRole-Playing\nNA\n927.37\n\n\nWii\nNA\nNA\n926.71\n\n\n\n\n\n\nglobal_sales_cube <-\n  tbl(pg, sql(\"\n    SELECT coalesce(platform, 'All') as platform,\n      coalesce(genre,'All') AS genre,\n      coalesce(publisher,'All') AS publisher,\n      sum(global_sales) AS global_sales\n    FROM videogame_sales\n    GROUP BY cube (platform, genre, publisher)\"))\n\nglobal_sales_cube %>%\n  arrange(platform, genre, publisher) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nplatform\ngenre\npublisher\nglobal_sales\n\n\n\n\n2600\nAction\n20th Century Fox Video Games\n1.72\n\n\n2600\nAction\nActivision\n4.64\n\n\n2600\nAction\nAll\n29.34\n\n\n2600\nAction\nAnswer Software\n0.50\n\n\n2600\nAction\nAtari\n7.68\n\n\n2600\nAction\nAvalon Interactive\n0.17\n\n\n2600\nAction\nBomb\n0.22\n\n\n2600\nAction\nCBS Electronics\n0.31\n\n\n2600\nAction\nColeco\n1.26\n\n\n2600\nAction\nCPG Products\n0.54"
  },
  {
    "objectID": "chapter_8.html#managing-data-set-size-and-privacy-concerns",
    "href": "chapter_8.html#managing-data-set-size-and-privacy-concerns",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.4 Managing Data Set Size and Privacy Concerns",
    "text": "8.4 Managing Data Set Size and Privacy Concerns\n\n8.4.1 Sampling with %, mod\nOne can also use random() to similar effect.\n\n\n8.4.2 Reducing Dimensionality\n\nlegislators_terms %>%\n  mutate(state_group = \n           case_when(state %in% c('CA', 'TX', 'FL', 'NY', 'PA') ~ state, \n                     TRUE ~ 'Other')) %>%\n  group_by(state_group) %>%\n  summarize(terms = n()) %>%\n  arrange(desc(terms)) %>%\n  collect(n = 6) %>%\n  kable()\n\n\n\n\nstate_group\nterms\n\n\n\n\nOther\n31980\n\n\nNY\n4159\n\n\nPA\n3252\n\n\nCA\n2121\n\n\nTX\n1692\n\n\nFL\n859\n\n\n\n\n\n\ntop_states <-\n  legislators_terms %>%\n  group_by(state) %>%\n  summarize(n_reps = n_distinct(id_bioguide), .groups = \"drop\") %>%\n  window_order(desc(n_reps)) %>%\n  mutate(rank = row_number())\n\nlegislators_terms %>%\n  inner_join(top_states, by = \"state\") %>%\n  mutate(state_group = case_when(rank <= 5 ~ state,\n                                 TRUE ~ 'Other')) %>%\n  group_by(state_group) %>%\n  summarize(terms = n_distinct(id_bioguide)) %>%\n  arrange(desc(terms)) %>%\n  collect(n = 6) %>%\n  kable()\n\n\n\n\nstate_group\nterms\n\n\n\n\nOther\n8317\n\n\nNY\n1494\n\n\nPA\n1075\n\n\nOH\n694\n\n\nIL\n509\n\n\nVA\n451\n\n\n\n\n\nNote that the CASE WHEN in the SQL in the book can be significantly simplified.\n\nWITH num_terms AS (\n  SELECT id_bioguide, count(term_id) as terms\n    FROM legislators_terms\n    GROUP BY 1)\n    \nSELECT terms >= 2 AS two_terms_flag,\n  count(*) as legislators\nFROM num_terms\nGROUP BY 1;\n\n\n2 records\n\n\ntwo_terms_flag\nlegislators\n\n\n\n\nFALSE\n4139\n\n\nTRUE\n8379\n\n\n\n\n\n\nnum_terms <- \n  legislators_terms %>%\n  group_by(id_bioguide) %>%\n  summarize(terms = n(), .groups = \"drop\")\n\nnum_terms %>%\n  mutate(two_terms_flag = terms >= 2) %>%\n  count(two_terms_flag) %>%\n  collect() %>%\n  kable()\n\n\n\n\ntwo_terms_flag\nn\n\n\n\n\nFALSE\n4139\n\n\nTRUE\n8379\n\n\n\n\n\nNow we can reuse the num_terms lazy table we created above.\n\nnum_terms %>%\n  mutate(terms_level = case_when(terms >= 10 ~ '10+',\n                                 terms >= 2 ~ '2 - 9',\n                                 TRUE ~ '1')) %>%\n  count(terms_level) %>%\n  collect() %>%\n  kable()\n\n\n\n\nterms_level\nn\n\n\n\n\n1\n4139\n\n\n2 - 9\n7496\n\n\n10+\n883\n\n\n\n\n\n\n\n8.4.3 PII and Data Privacy"
  },
  {
    "objectID": "chapter_8.html#conclusion",
    "href": "chapter_8.html#conclusion",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.5 Conclusion",
    "text": "8.5 Conclusion"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Tanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  }
]