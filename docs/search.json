[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SQL for Data Analysis using R",
    "section": "",
    "text": "This “book” is a collection of notes made as I work through Tanimura (2021). An alternative title might have been “Data Analysis with Data Stored in Databases”; while SQL is the language used in Tanimura (2021) to do the analysis, this is in some ways a minor detail. My view is that one can easy do “SQL” analysis using the dplyr package in R (this uses dbplyr behind the scenes … mostly). The dplyr package will quietly translate R code into SQL for the user.\nAn advantage of using dplyr rather than SQL directly is that one doesn’t need to learn as much SQL. In principle, one could use dplyr without knowing any SQL. Given that dplyr and R can be used to analyse data from other data sources, this reduces the amount that is needed to be learnt to do analysis. Additionally, one could write code to analyse data from a database and then easily reuse the code for data from a CSV file.\nNotwithstanding the discussion in the previous paragraph, I recommend that people who find themselves using SQL-driven databases a lot learn some SQL. This view is implicitly endorsed by Hadley Wickham with the inclusion of a significant amount of material intended to teach SQL to readers of “R for Data Science”. While we include a brief primer on SQL for dplyr users here, clearly an excellent source for learning SQL is Tanimura (2021) itself.\nAnother benefit of using R is that we can do more with R. Tanimura (2021) includes many plots of data produced using SQL, but the code to make the plots is not included. In contrast, here I will include code to produce the data as well as code to make plots (where applicable).\nMy view is that the material here might be useful to someone who is using Tanimura (2021) and wants to see how the ideas there can be implemented using R, while at the same time being useful to someone who knows (or is looking to learn) R and is looking for realistic data sets to work on.\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_2.html",
    "href": "chapter_2.html",
    "title": "2  Preparing Data for Analysis",
    "section": "",
    "text": "Chapter 2 of Tanimura (2021) provides a good foundation discussion of issues related to preparing data for analysis. While the discussion is couched in terms of SQL, in reality the issues are not specific to SQL or databases. For this reason, I recommend that you read the chapter.\nWhile Chapter 2 of Tanimura (2021) contains many code snippets, few of these seem to be intended for users to run (in part because they assume a database set-up that users would not have). For this reason, I do not attempt to provide dplyr equivalents to the code there except for a couple of exceptions that I discuss below."
  },
  {
    "objectID": "chapter_2.html#missing-data",
    "href": "chapter_2.html#missing-data",
    "title": "2  Preparing Data for Analysis",
    "section": "2.1 Missing data",
    "text": "2.1 Missing data\n\nlibrary(tidyverse)\nlibrary(DBI)\n\n\npg <- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\ndbExecute(pg, \"SET search_path TO sql_book\")\n\n[1] 0\n\n\n\ndates_sql <-\n   \"SELECT * \n    FROM generate_series('2000-01-01'::timestamp,\n                         '2030-12-31', '1 day')\"\n\ndates <- \n  tbl(pg, sql(dates_sql)) %>%\n  rename(date = generate_series)\n\ndates_processed <-\n  dates %>%\n  mutate(date_key = as.integer(to_char(date, 'yyyymmdd')),\n         day_of_month = as.integer(date_part('day',date)),\n         day_of_year = as.integer(date_part('doy', date)),\n         day_of_week = as.integer(date_part('dow', date)),\n         day_name  = trim(to_char(date, 'Day')),\n         day_short_name = trim(to_char(date, 'Dy')),\n         week_number = as.integer(date_part('week', date)),\n         week_of_month = as.integer(to_char(date,'W')),\n         week = as.Date(date_trunc('week', date)),\n         month_number = as.integer(date_part('month',date)),\n         month_name = trim(to_char(date, 'Month')),\n         month_short_name = trim(to_char(date, 'Mon')),\n         first_day_of_month = as.Date(date_trunc('month', date)),\n         last_day_of_month = as.Date(date_trunc('month', date) +\n                                       sql(\"interval '1 month' -\n                                            interval '1 day'\")),\n         quarter_number = as.integer(date_part('quarter', date)),\n         quarter_name = trim('Q' %||% as.integer(date_part('quarter', date))),\n         first_day_of_quarter = as.Date(date_trunc('quarter', date)),\n         last_day_of_quarter = as.Date(date_trunc('quarter', date) + \n                                         sql(\"interval '3 months' -\n                                              interval '1 day'\")),\n         year = as.integer(date_part('year', date)),\n         decade = as.integer(date_part('decade', date)) * 10,\n         century = as.integer(date_part('century', date)))\n\ndates_processed %>%\n  collect(n = 10)\n\n# A tibble: 10 × 22\n   date                date_key day_of_month day_of_year day_of_week day_name \n   <dttm>                 <int>        <int>       <int>       <int> <chr>    \n 1 2000-01-01 00:00:00 20000101            1           1           6 Saturday \n 2 2000-01-02 00:00:00 20000102            2           2           0 Sunday   \n 3 2000-01-03 00:00:00 20000103            3           3           1 Monday   \n 4 2000-01-04 00:00:00 20000104            4           4           2 Tuesday  \n 5 2000-01-05 00:00:00 20000105            5           5           3 Wednesday\n 6 2000-01-06 00:00:00 20000106            6           6           4 Thursday \n 7 2000-01-07 00:00:00 20000107            7           7           5 Friday   \n 8 2000-01-08 00:00:00 20000108            8           8           6 Saturday \n 9 2000-01-09 00:00:00 20000109            9           9           0 Sunday   \n10 2000-01-10 00:00:00 20000110           10          10           1 Monday   \n# ℹ 16 more variables: day_short_name <chr>, week_number <int>,\n#   week_of_month <int>, week <date>, month_number <int>, month_name <chr>,\n#   month_short_name <chr>, first_day_of_month <date>,\n#   last_day_of_month <date>, quarter_number <int>, quarter_name <chr>,\n#   first_day_of_quarter <date>, last_day_of_quarter <date>, year <int>,\n#   decade <dbl>, century <int>\n\ndates_processed %>%\n  show_query()\n\n<SQL>\nSELECT\n  *,\n  CAST(to_char(\"date\", 'yyyymmdd') AS INTEGER) AS \"date_key\",\n  CAST(date_part('day', \"date\") AS INTEGER) AS \"day_of_month\",\n  CAST(date_part('doy', \"date\") AS INTEGER) AS \"day_of_year\",\n  CAST(date_part('dow', \"date\") AS INTEGER) AS \"day_of_week\",\n  trim(to_char(\"date\", 'Day')) AS \"day_name\",\n  trim(to_char(\"date\", 'Dy')) AS \"day_short_name\",\n  CAST(date_part('week', \"date\") AS INTEGER) AS \"week_number\",\n  CAST(to_char(\"date\", 'W') AS INTEGER) AS \"week_of_month\",\n  CAST(date_trunc('week', \"date\") AS DATE) AS \"week\",\n  CAST(date_part('month', \"date\") AS INTEGER) AS \"month_number\",\n  trim(to_char(\"date\", 'Month')) AS \"month_name\",\n  trim(to_char(\"date\", 'Mon')) AS \"month_short_name\",\n  CAST(date_trunc('month', \"date\") AS DATE) AS \"first_day_of_month\",\n  CAST(date_trunc('month', \"date\") + interval '1 month' -\n                                            interval '1 day' AS DATE) AS \"last_day_of_month\",\n  CAST(date_part('quarter', \"date\") AS INTEGER) AS \"quarter_number\",\n  trim('Q' || CAST(date_part('quarter', \"date\") AS INTEGER)) AS \"quarter_name\",\n  CAST(date_trunc('quarter', \"date\") AS DATE) AS \"first_day_of_quarter\",\n  CAST(date_trunc('quarter', \"date\") + interval '3 months' -\n                                              interval '1 day' AS DATE) AS \"last_day_of_quarter\",\n  CAST(date_part('year', \"date\") AS INTEGER) AS \"year\",\n  CAST(date_part('decade', \"date\") AS INTEGER) * 10.0 AS \"decade\",\n  CAST(date_part('century', \"date\") AS INTEGER) AS \"century\"\nFROM (\n  SELECT \"generate_series\" AS \"date\"\n  FROM (\nSELECT * \n    FROM generate_series('2000-01-01'::timestamp,\n                         '2030-12-31', '1 day')\n  ) \"q01\"\n) \"q02\"\n\n\n\nctry_pops <-\n  tribble(\n  ~country, ~year_1980,  ~year_1990, ~year_2000, ~year_2010,\n  \"Canada\", 24593, 27791, 31100, 34207,\n  \"Mexico\", 68347, 84634, 99775, 114061,\n  \"United States\", 227225, 249623, 282162, 309326\n)\n\nctry_pops %>%\n  pivot_longer(cols = -country, \n               names_to = \"year\",\n               names_prefix = \"year_\",\n               values_ptypes = integer(),\n               values_to = \"population\")\n\n# A tibble: 12 × 3\n   country       year  population\n   <chr>         <chr>      <int>\n 1 Canada        1980       24593\n 2 Canada        1990       27791\n 3 Canada        2000       31100\n 4 Canada        2010       34207\n 5 Mexico        1980       68347\n 6 Mexico        1990       84634\n 7 Mexico        2000       99775\n 8 Mexico        2010      114061\n 9 United States 1980      227225\n10 United States 1990      249623\n11 United States 2000      282162\n12 United States 2010      309326\n\n\n\nctry_pops_db <- copy_to(pg, ctry_pops)\n\n\nctry_pops_db %>%\n  pivot_longer(cols = -country, \n               names_to = \"year\",\n               names_prefix = \"year_\",\n               values_to = \"population\") %>%\n  show_query()\n\n<SQL>\n(\n  (\n    (\n      SELECT \"country\", '1980' AS \"year\", \"year_1980\" AS \"population\"\n      FROM \"ctry_pops\"\n    )\n    UNION ALL\n    (\n      SELECT \"country\", '1990' AS \"year\", \"year_1990\" AS \"population\"\n      FROM \"ctry_pops\"\n    )\n  )\n  UNION ALL\n  (\n    SELECT \"country\", '2000' AS \"year\", \"year_2000\" AS \"population\"\n    FROM \"ctry_pops\"\n  )\n)\nUNION ALL\n(\n  SELECT \"country\", '2010' AS \"year\", \"year_2010\" AS \"population\"\n  FROM \"ctry_pops\"\n)\n\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_3.html",
    "href": "chapter_3.html",
    "title": "3  Time Series Analysis",
    "section": "",
    "text": "library(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)"
  },
  {
    "objectID": "chapter_3.html#date-datetime-and-time-manipulations",
    "href": "chapter_3.html#date-datetime-and-time-manipulations",
    "title": "3  Time Series Analysis",
    "section": "3.1 Date, Datetime, and Time Manipulations",
    "text": "3.1 Date, Datetime, and Time Manipulations"
  },
  {
    "objectID": "chapter_3.html#time-zone-conversions",
    "href": "chapter_3.html#time-zone-conversions",
    "title": "3  Time Series Analysis",
    "section": "3.2 Time Zone Conversions",
    "text": "3.2 Time Zone Conversions\nTanimura (2021) points out that often “timestamps in the database are not encoded with the time zone, and you will need to consult with the source or developer to figure out how your data was stored.” When pushing data to a PostgreSQL database, I use the timestamp with time zone type as much as possible.\nTanimura (2021) provides the following example, which is interesting because the west coast of the United States would not be on the PST time zone at that time of year. Instead, it would be on PDT.\n\nSELECT '2020-09-01 00:00:00' AT TIME ZONE 'pst';\n\n\n1 records\n\n\ntimezone\n\n\n\n\n2020-08-31 16:00:00\n\n\n\n\n\n\nSELECT '2020-09-01 00:00:00' AT TIME ZONE 'pdt';\n\n\n1 records\n\n\ntimezone\n\n\n\n\n2020-08-31 17:00:00\n\n\n\n\n\nI think most people barely know the difference between PST and PDT and even fewer would know the exact dates that one switches from one to the other. A better approach is to use a time zone that encodes information about when PDT is used and when PST is used. In PostgreSQL, the table pg_timezone_names has information that we need.\n\nSELECT * \nFROM pg_timezone_names\nWHERE name ~ '^US/';\n\n\nDisplaying records 1 - 10\n\n\nname\nabbrev\nutc_offset\nis_dst\n\n\n\n\nUS/Alaska\nAKDT\n-08:00:00\nTRUE\n\n\nUS/Pacific\nPDT\n-07:00:00\nTRUE\n\n\nUS/Eastern\nEDT\n-04:00:00\nTRUE\n\n\nUS/Michigan\nEDT\n-04:00:00\nTRUE\n\n\nUS/Arizona\nMST\n-07:00:00\nFALSE\n\n\nUS/Indiana-Starke\nCDT\n-05:00:00\nTRUE\n\n\nUS/Aleutian\nHDT\n-09:00:00\nTRUE\n\n\nUS/Hawaii\nHST\n-10:00:00\nFALSE\n\n\nUS/East-Indiana\nEDT\n-04:00:00\nTRUE\n\n\nUS/Central\nCDT\n-05:00:00\nTRUE\n\n\n\n\n\n\nSELECT * \nFROM pg_timezone_names\nWHERE abbrev IN ('PDT', 'PST') \nORDER BY name DESC\nLIMIT 5;\n\n\n5 records\n\n\nname\nabbrev\nutc_offset\nis_dst\n\n\n\n\nUS/Pacific\nPDT\n-07:00:00\nTRUE\n\n\nPST8PDT\nPDT\n-07:00:00\nTRUE\n\n\nMexico/BajaNorte\nPDT\n-07:00:00\nTRUE\n\n\nCanada/Pacific\nPDT\n-07:00:00\nTRUE\n\n\nAsia/Manila\nPST\n08:00:00\nFALSE\n\n\n\n\n\nThe following two queries show that US/Pacific is sometimes PDT and sometimes PST.\n\nSELECT \n    '2020-09-01 17:00:01 US/Pacific'::timestamptz AS t1,\n    '2020-09-01 17:00:01 PDT'::timestamptz  AS t2;\n\n\n1 records\n\n\nt1\nt2\n\n\n\n\n2020-09-02 00:00:01\n2020-09-02 00:00:01\n\n\n\n\n\n\nSELECT \n    '2020-12-01 16:00:01 US/Pacific'::timestamptz AS t1,\n    '2020-12-01 16:00:01 PST'::timestamptz  AS t2;\n\n\n1 records\n\n\nt1\nt2\n\n\n\n\n2020-12-02 00:00:01\n2020-12-02 00:00:01\n\n\n\n\n\n\nsql <-\n  \"SELECT     \n    '2020-12-01 16:00:01 US/Pacific'::timestamptz AS t1,\n    '2020-12-01 16:00:01 PST'::timestamptz AS t2\"\n\ntwo_times <- tbl(pg, sql(sql))\ntwo_times\n\n# Source:   SQL [1 x 2]\n# Database: postgres  [igow@localhost:5432/igow]\n  t1                  t2                 \n  <dttm>              <dttm>             \n1 2020-12-02 00:00:01 2020-12-02 00:00:01\n\ntwo_times %>%\n  collect()\n\n# A tibble: 1 × 2\n  t1                  t2                 \n  <dttm>              <dttm>             \n1 2020-12-02 00:00:01 2020-12-02 00:00:01\n\none_time <-\n  two_times %>%\n  select(t1) %>%\n  pull()\n\nprint(one_time, tz = \"UTC\")\n\n[1] \"2020-12-02 00:00:01 UTC\"\n\nprint(one_time, tz = \"US/Pacific\")\n\n[1] \"2020-12-01 16:00:01 PST\"\n\nSys.timezone()\n\n[1] \"Australia/Melbourne\"\n\nprint(one_time, tz = Sys.timezone())\n\n[1] \"2020-12-02 11:00:01 AEDT\"\n\n\nThe above examples illustrate a few key ideas.\nFirst, while we supply the literal form '2020-09-01 17:00:01 US/Pacific'::timestamptz, it seems that once a variable has been encoded as TIMESTAMP WITH TIME ZONE, it behaves as though it is actually being stored as a timestamp in the UTC time zone, just with the displayed time perhaps being different.\nSecond, columns of type TIMESTAMP WITH TIME ZONE come into R with the associated time-zone information, which is what we want (especially if we will later put timestamp data back into PostgreSQL).\nThird, we can see that we can choose to display information in a different time zone without changing the underlying data.\nSome care is needed with timestamp data. I think the AT TIME ZONE queries provided in Tanimura (2021) are actually pretty dangerous, as can be seen in the following query. While we supply 2020-09-01 00:00:00 as UTC and then render it AT TIME ZONE 'PDT', it turns out that the returned value is interpreted as a TIMESTAMP WITHOUT TIME ZONE and subsequent queries lead to confusing behaviour. In the query below, the second application of AT TIME ZONE interprets the TIMESTAMP WITHOUT TIME ZONE as though it came from the stated time zone and the results seem to have AT TIME ZONE doing the opposite of what it did when given a TIMESTAMP WITH TIME ZONE (as in the initial literal '2020-09-01 00:00:01 -0').\n\nWITH q1 AS\n (SELECT '2020-09-01 00:00:01 -0' AT TIME ZONE 'PDT' AS t1)\n \nSELECT t1 AT TIME ZONE 'UTC' AS t2,\n  to_char(t1, 'YYYY-MM-DD HH24:MI:SS TZ'),\n  t1 AT TIME ZONE 'PDT' AS t3,\n  t1 AT TIME ZONE 'Australia/Melbourne' AS t4,\n  pg_typeof(t1)\nFROM q1\n\n\n1 records\n\n\n\n\n\n\n\n\n\nt2\nto_char\nt3\nt4\npg_typeof\n\n\n\n\n2020-08-31 17:00:01\n2020-08-31 17:00:01\n2020-09-01 00:00:01\n2020-08-31 07:00:01\ntimestamp without time zone\n\n\n\n\n\nIt seems that TIMESTAMP WITHOUT TIME ZONE values should be converted to a time zone as quickly as possible to avoid confusion and that great care is needed with AT TIME ZONE given that it does very different things according to the supplied data type.\n\nWITH q1 AS\n (SELECT '2020-09-01 00:00:01 -0'::timestamptz AS t1)\n \nSELECT t1,\n  to_char(t1, 'YYYY-MM-DD HH24:MI:SS TZ'),\n  pg_typeof(t1)\nFROM q1\n\n\n1 records\n\n\n\n\n\n\n\nt1\nto_char\npg_typeof\n\n\n\n\n2020-09-01 00:00:01\n2020-09-01 00:00:01 UTC\ntimestamp with time zone\n\n\n\n\n\nStrange behaviour can result from values stored as TIMESTAMP WITHOUT TIME ZONE. Below we see that t1 is printed as UTC no matter what, while the behaviour of t2 seems easier to understand.\n\nsql <-\n  \"SELECT     \n    '2020-12-01 00:00:01-00' AS t1,\n    '2020-12-01 00:00:01-00'::timestamptz AS t2\"\n\ntwo_times_notz <- tbl(pg, sql(sql))\ntwo_times_notz\n\n# Source:   SQL [1 x 2]\n# Database: postgres  [igow@localhost:5432/igow]\n  t1                     t2                 \n  <chr>                  <dttm>             \n1 2020-12-01 00:00:01-00 2020-12-01 00:00:01\n\n\n\ntwo_times_notz_r <-\n  collect(two_times_notz)\n\n\nprint(two_times_notz_r$t1)\n\n[1] \"2020-12-01 00:00:01-00\"\n\nSys.timezone()\n\n[1] \"Australia/Melbourne\"\n\nprint(two_times_notz_r$t1, tz = Sys.timezone())\n\n[1] \"2020-12-01 00:00:01-00\"\n\nprint(two_times_notz_r$t2)\n\n[1] \"2020-12-01 00:00:01 UTC\"\n\nSys.timezone()\n\n[1] \"Australia/Melbourne\"\n\nprint(two_times_notz_r$t2, tz = Sys.timezone())\n\n[1] \"2020-12-01 11:00:01 AEDT\"\n\n\nAs pointed out by Tanimura (2021), one drawback to storing information as UTC is that localtime information may be lost. But it seems it would be more prudent to store information as TIMESTAMP WITH TIME ZONE and keep local time zone information as a separate column to avoid confusion. For example, if the orders table is stored as TIMESTAMP WITHOUT TIME ZONE based on the local time of the customer, which might be Australia/Melbourne and the shipping table uses TIMESTAMP WITH TIME ZONE, then an analyst of time-to-ship data would be confused by orders apparently being shipped before they are made. If shipping table uses TIMESTAMP WITH TIME ZONE using timestamps in the time zone of the East Bay warehouse (so US/Pacific), things would be even worse.\nI think that fully fleshing out the issues here would require a separate chapter. In fact, nothing in the core part of Chapter 3 of Tanimura (2021) (which focuses on the retail_sales table) really uses timestamp information, so we can put these issues aside for now."
  },
  {
    "objectID": "chapter_3.html#date-and-timestamp-format-conversions",
    "href": "chapter_3.html#date-and-timestamp-format-conversions",
    "title": "3  Time Series Analysis",
    "section": "3.3 Date and Timestamp Format Conversions",
    "text": "3.3 Date and Timestamp Format Conversions\nAs discussed in Tanimura (2021), PostgreSQL has a rich array of functions for converting dates and times and extracting such information as months and days of the week.\n\nSELECT date_trunc('month','2020-10-04 12:33:35 -00'::timestamptz);\n\n\n1 records\n\n\ndate_trunc\n\n\n\n\n2020-10-01\n\n\n\n\n\nOne such function\n\na_time_df <- tbl(pg, sql(\"SELECT '2020-10-04 12:33:35'::timestamp AS a_time\"))\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time))\n\n# Source:   SQL [1 x 2]\n# Database: postgres  [igow@localhost:5432/igow]\n  a_time              a_trunced_time     \n  <dttm>              <dttm>             \n1 2020-10-04 12:33:35 2020-10-01 00:00:00\n\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time)) %>%\n  show_query()\n\n<SQL>\nSELECT *, date_trunc('month', \"a_time\") AS \"a_trunced_time\"\nFROM (SELECT '2020-10-04 12:33:35'::timestamp AS a_time) \"q01\"\n\na_time_df %>%\n  collect()\n\n# A tibble: 1 × 1\n  a_time             \n  <dttm>             \n1 2020-10-04 12:33:35\n\n\n\na_time_df <- tbl(pg, sql(\"SELECT '2020-10-04 12:33:35 US/Pacific'::timestamp with time zone AS a_time\"))\n\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time)) \n\n# Source:   SQL [1 x 2]\n# Database: postgres  [igow@localhost:5432/igow]\n  a_time              a_trunced_time     \n  <dttm>              <dttm>             \n1 2020-10-04 19:33:35 2020-10-01 00:00:00\n\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time)) %>%\n  show_query()\n\n<SQL>\nSELECT *, date_trunc('month', \"a_time\") AS \"a_trunced_time\"\nFROM (SELECT '2020-10-04 12:33:35 US/Pacific'::timestamp with time zone AS a_time) \"q01\"\n\na_time_df %>%\n  collect()\n\n# A tibble: 1 × 1\n  a_time             \n  <dttm>             \n1 2020-10-04 19:33:35\n\n\n\na_time_df %>%\n  mutate(new_time = a_time + sql(\"interval '3 hours'\")) %>%\n  collect()\n\n# A tibble: 1 × 2\n  a_time              new_time           \n  <dttm>              <dttm>             \n1 2020-10-04 19:33:35 2020-10-04 22:33:35"
  },
  {
    "objectID": "chapter_3.html#the-retail-sales-data-set",
    "href": "chapter_3.html#the-retail-sales-data-set",
    "title": "3  Time Series Analysis",
    "section": "3.4 The Retail Sales Data Set",
    "text": "3.4 The Retail Sales Data Set\n\nSELECT sales_month, sales\nFROM retail_sales\nWHERE kind_of_business = 'Retail and food services sales, total'\nORDER BY 1\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nsales\n\n\n\n\n1992-01-01\n146376\n\n\n1992-02-01\n147079\n\n\n1992-03-01\n159336\n\n\n1992-04-01\n163669\n\n\n1992-05-01\n170068\n\n\n1992-06-01\n168663\n\n\n1992-07-01\n169890\n\n\n1992-08-01\n170364\n\n\n1992-09-01\n164617\n\n\n1992-10-01\n173655\n\n\n\n\n\n\nretail_sales <- tbl(pg, \"retail_sales\")\nretail_sales %>%\n  filter(kind_of_business == 'Retail and food services sales, total') %>%\n  select(sales_month, sales) %>%\n  arrange(sales_month) %>%\n  ggplot(aes(x = sales_month, y = sales)) +\n  geom_line()\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year,\n    sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business = 'Retail and food services sales, total'\nGROUP BY 1\n;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nsales\n\n\n\n\n2007\n4439733\n\n\n2005\n4085746\n\n\n1992\n2014102\n\n\n2011\n4598302\n\n\n2014\n5215656\n\n\n2006\n4294359\n\n\n2010\n4284968\n\n\n2001\n3378906\n\n\n2019\n6218002\n\n\n2018\n6001623\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business == 'Retail and food services sales, total') %>%\n  mutate(sales_year = date_part('year', sales_month)) %>%\n  group_by(sales_year) %>%\n  summarize(sales = sum(sales, na.rm = TRUE)) %>%\n  arrange(sales_year) %>%\n  ggplot(aes(x = sales_year, y = sales)) +\n  geom_line()\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year, \n  kind_of_business, sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business IN \n          ('Book stores',\n           'Sporting goods stores',\n           'Hobby, toy, and game stores')\nGROUP BY 1,2\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nkind_of_business\nsales\n\n\n\n\n1992\nSporting goods stores\n15583\n\n\n1992\nHobby, toy, and game stores\n11251\n\n\n1992\nBook stores\n8327\n\n\n1993\nHobby, toy, and game stores\n11651\n\n\n1993\nSporting goods stores\n16791\n\n\n1993\nBook stores\n9108\n\n\n1994\nSporting goods stores\n18825\n\n\n1994\nBook stores\n10107\n\n\n1994\nHobby, toy, and game stores\n12850\n\n\n1995\nHobby, toy, and game stores\n13714\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c('Book stores',\n             'Sporting goods stores',\n             'Hobby, toy, and game stores')) %>%\n  mutate(sales_year = date_part('year', sales_month)) %>%\n  group_by(sales_year, kind_of_business) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  arrange(sales_year) %>%\n  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\nWHERE kind_of_business IN ('Men''s clothing stores','Women''s clothing stores')\nORDER BY 1,2;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nkind_of_business\nsales\n\n\n\n\n1992-01-01\nMen’s clothing stores\n701\n\n\n1992-01-01\nWomen’s clothing stores\n1873\n\n\n1992-02-01\nMen’s clothing stores\n658\n\n\n1992-02-01\nWomen’s clothing stores\n1991\n\n\n1992-03-01\nMen’s clothing stores\n731\n\n\n1992-03-01\nWomen’s clothing stores\n2403\n\n\n1992-04-01\nMen’s clothing stores\n816\n\n\n1992-04-01\nWomen’s clothing stores\n2665\n\n\n1992-05-01\nMen’s clothing stores\n856\n\n\n1992-05-01\nWomen’s clothing stores\n2752\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% c(\"Men's clothing stores\",\n                                 \"Women's clothing stores\")) %>%\n  select(sales_month, kind_of_business, sales) %>%\n  arrange(sales_month) %>%\n  ggplot(aes(x = sales_month, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year,\n  kind_of_business, sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business IN \n        ('Men''s clothing stores',\n        'Women''s clothing stores')\nGROUP BY 1, 2\nORDER BY 1, 2;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nkind_of_business\nsales\n\n\n\n\n1992\nMen’s clothing stores\n10179\n\n\n1992\nWomen’s clothing stores\n31815\n\n\n1993\nMen’s clothing stores\n9962\n\n\n1993\nWomen’s clothing stores\n32350\n\n\n1994\nMen’s clothing stores\n10032\n\n\n1994\nWomen’s clothing stores\n30585\n\n\n1995\nMen’s clothing stores\n9315\n\n\n1995\nWomen’s clothing stores\n28696\n\n\n1996\nMen’s clothing stores\n9546\n\n\n1996\nWomen’s clothing stores\n28238\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  mutate(sales_year = date_part('year', sales_month)) %>%\n  group_by(sales_year, kind_of_business) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  arrange(sales_year) %>%\n  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT date_part('year', sales_month) AS sales_year,\n  sum(CASE WHEN kind_of_business = 'Women''s clothing stores' \n          then sales \n          END) AS womens_sales,\n  sum(CASE WHEN kind_of_business = 'Men''s clothing stores' \n          then sales \n          END) AS mens_sales\nFROM retail_sales\nWHERE kind_of_business IN \n   ('Men''s clothing stores',\n    'Women''s clothing stores')\nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nwomens_sales\nmens_sales\n\n\n\n\n1992\n31815\n10179\n\n\n1993\n32350\n9962\n\n\n1994\n30585\n10032\n\n\n1995\n28696\n9315\n\n\n1996\n28238\n9546\n\n\n1997\n27822\n10069\n\n\n1998\n28332\n10196\n\n\n1999\n29549\n9667\n\n\n2000\n31447\n9507\n\n\n2001\n31453\n8625\n\n\n\n\n\n\npivoted_sales <-\n  retail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  mutate(kind_of_business = if_else(kind_of_business == \"Women's clothing stores\",\n                                    \"womens\", \"mens\"),\n         sales_year = date_part('year', sales_month)) %>%\n  group_by(sales_year, kind_of_business) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  pivot_wider(id_cols = \"sales_year\",\n              names_from = \"kind_of_business\",\n              names_glue = \"{kind_of_business}_{.value}\",\n              values_from = \"sales\")  \n\npivoted_sales %>%\n  show_query()\n\n<SQL>\nSELECT\n  \"sales_year\",\n  MAX(CASE WHEN (\"kind_of_business\" = 'mens') THEN \"sales\" END) AS \"mens_sales\",\n  MAX(CASE WHEN (\"kind_of_business\" = 'womens') THEN \"sales\" END) AS \"womens_sales\"\nFROM (\n  SELECT \"sales_year\", \"kind_of_business\", SUM(\"sales\") AS \"sales\"\n  FROM (\n    SELECT\n      \"sales_month\",\n      \"naics_code\",\n      CASE WHEN (\"kind_of_business\" = 'Women''s clothing stores') THEN 'womens' WHEN NOT (\"kind_of_business\" = 'Women''s clothing stores') THEN 'mens' END AS \"kind_of_business\",\n      \"reason_for_null\",\n      \"sales\",\n      date_part('year', \"sales_month\") AS \"sales_year\"\n    FROM \"retail_sales\"\n    WHERE (\"kind_of_business\" IN ('Men''s clothing stores', 'Women''s clothing stores'))\n  ) \"q01\"\n  GROUP BY \"sales_year\", \"kind_of_business\"\n) \"q02\"\nGROUP BY \"sales_year\"\n\npivoted_sales %>%\n  arrange(sales_year) %>%\n  collect(n = 10) %>%\n  knitr::kable()\n\n\n\n\nsales_year\nmens_sales\nwomens_sales\n\n\n\n\n1992\n10179\n31815\n\n\n1993\n9962\n32350\n\n\n1994\n10032\n30585\n\n\n1995\n9315\n28696\n\n\n1996\n9546\n28238\n\n\n1997\n10069\n27822\n\n\n1998\n10196\n28332\n\n\n1999\n9667\n29549\n\n\n2000\n9507\n31447\n\n\n2001\n8625\n31453\n\n\n\n\n\n\npivoted_sales %>%\n  filter(sales_year <= 2019) %>%\n  group_by(sales_year) %>%\n  mutate(womens_minus_mens = womens_sales - mens_sales,\n         mens_minus_womens = mens_sales - womens_sales) %>%\n  select(sales_year, womens_minus_mens, mens_minus_womens) %>%\n  arrange(sales_year) %>%\n  ggplot(aes(y = womens_minus_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\npivoted_sales %>%\n  filter(sales_year <= 2019) %>%\n  group_by(sales_year) %>%\n  mutate(womens_times_of_mens = womens_sales / mens_sales) %>%\n  arrange(sales_year) %>%\n  ggplot(aes(y = womens_times_of_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\npivoted_sales %>%\n  filter(sales_year <= 2019) %>%\n  group_by(sales_year) %>%\n  mutate(womens_pct_of_mens = (womens_sales / mens_sales - 1) * 100) %>%\n  arrange(sales_year) %>%\n  ggplot(aes(y = womens_pct_of_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  group_by(sales_month) %>%\n  mutate(total_sales = sum(sales)) %>%\n  ungroup() %>%\n  mutate(pct_total_sales = sales * 100 / total_sales) %>%\n  select(sales_month, kind_of_business, pct_total_sales) %>%\n  collect(n = 3)\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n# A tibble: 3 × 3\n  sales_month kind_of_business        pct_total_sales\n  <date>      <chr>                             <dbl>\n1 1992-01-01  Women's clothing stores            72.8\n2 1992-01-01  Men's clothing stores              27.2\n3 1992-02-01  Men's clothing stores              24.8\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  group_by(sales_month) %>%\n  mutate(total_sales = sum(sales)) %>%\n  ungroup() %>%\n  mutate(pct_total_sales = sales * 100 / total_sales) %>%\n  show_query()\n\n<SQL>\nSELECT *, (\"sales\" * 100.0) / \"total_sales\" AS \"pct_total_sales\"\nFROM (\n  SELECT *, SUM(\"sales\") OVER (PARTITION BY \"sales_month\") AS \"total_sales\"\n  FROM \"retail_sales\"\n  WHERE (\"kind_of_business\" IN ('Men''s clothing stores', 'Women''s clothing stores'))\n) \"q01\"\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  group_by(sales_month) %>%\n  mutate(total_sales = sum(sales)) %>%\n  ungroup() %>%\n  mutate(pct_total_sales = sales * 100 / total_sales) %>%\n  ggplot(aes(y = pct_total_sales, x = sales_month, color = kind_of_business)) +\n  geom_line()\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business == \"Women's clothing stores\") %>%\n  mutate(sales_year = date_part('year',sales_month)) %>%\n  group_by(sales_year) %>%\n  summarize(sales = sum(sales, na.rm = TRUE)) %>%\n  ungroup() %>%\n  window_order(sales_year) %>%\n  mutate(index_sales = first(sales),\n         pct_from_index = (sales/index_sales - 1) * 100)\n\n# Source:     SQL [?? x 4]\n# Database:   postgres  [igow@localhost:5432/igow]\n# Ordered by: sales_year\n   sales_year sales index_sales pct_from_index\n        <dbl> <dbl>       <dbl>          <dbl>\n 1       1992 31815       31815           0   \n 2       1993 32350       31815           1.68\n 3       1994 30585       31815          -3.87\n 4       1995 28696       31815          -9.80\n 5       1996 28238       31815         -11.2 \n 6       1997 27822       31815         -12.6 \n 7       1998 28332       31815         -10.9 \n 8       1999 29549       31815          -7.12\n 9       2000 31447       31815          -1.16\n10       2001 31453       31815          -1.14\n# ℹ more rows\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% c(\"Women's clothing stores\",\n                                 \"Men's clothing stores\"),\n         sales_month <= '2019-12-31') %>%\n  mutate(sales_year = date_part('year',sales_month)) %>%\n  group_by(kind_of_business, sales_year) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  group_by(kind_of_business) %>%\n  window_order(sales_year) %>%\n  mutate(index_sales = first(sales),\n         pct_from_index = (sales/index_sales - 1) * 100) %>%\n  ungroup() %>%\n  ggplot(aes(y = pct_from_index, x = sales_year, color = kind_of_business)) +\n  geom_line()\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business == \"Women's clothing stores\") %>%\n  window_order(sales_month) %>%\n  window_frame(-11, 0) %>%\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n()) %>%\n  select(sales_month, moving_avg, records_count) %>%\n  collect(n = 10)\n\n# A tibble: 10 × 3\n   sales_month moving_avg records_count\n   <date>           <dbl>         <int>\n 1 1992-01-01       1873              1\n 2 1992-02-01       1932              2\n 3 1992-03-01       2089              3\n 4 1992-04-01       2233              4\n 5 1992-05-01       2337.             5\n 6 1992-06-01       2351.             6\n 7 1992-07-01       2354.             7\n 8 1992-08-01       2392.             8\n 9 1992-09-01       2411.             9\n10 1992-10-01       2445.            10\n\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_4.html",
    "href": "chapter_4.html",
    "title": "4  Cohorts",
    "section": "",
    "text": "While cohort analysis has some attractive features, I guess that data do not often come in a format that facilitates such analysis. Instead, as is the case with the legislators data set studied in Chapter 4, the data analyst needs to rearrange the data to support cohort analysis.\nI found Chapter 4 a little confusing on a first pass through it.1 The chapter launches into some SQL code intended to create cohorts, but it’s a little unclear why we’re doing what we’re doing, and we quickly see that our cohort analysis does not make sense (e.g., we have more of our original cohort in period 5 than we had in period 4) and we must have done something wrong. I think I see the idea Cathy is going for here: one needs to think carefully about how to arrange the data to avoid subtle mistakes. The challenge I see is that it’s not obvious that everyone would make the same mistake and one is too deep in the weeds of the code to really see the forest for the trees.\nSo before I launch into the code, I will spend a little time thinking about things conceptually. I will start with a different example from that used in Chapter 4, but one that I think brings out some of the issues.\nFor some reason I associate cohort analysis with life expectancy. The people who are born today form a cohort and one might ask: How long do we expect members of this cohort to live? One often hears life expectancy statistics quoted as something like: “In Australia, a boy born in 2018–2020 can expect to live to the age of 81.2 years and a girl would be expected to live to 85.3 years compared to 51.1 for boys and 54.8 years for girls born in in 1891–1900.”2\nThe people who construct the life expectancies for the children must be veritable polymaths. They need to anticipate future developments in medical care and technology. Skin cancer is a significant cause of death in Australia, due to a mismatch between the median complexion and the intensity of the sun. But the analysts calculating life expectancy need to think about how medical technology is likely to affect rates of death from carcinoma in the future. I can imagine whole-body scanners a bit like the scanners in US airports that detect skin cancers before they become problematic. These analysts also need to understand how road safety will evolve. Will children today all be in driverless vehicles in fifty years time and will accidents then be a rarity? And what about war? The data analyst needs to be able to forecast the possibility of World War III breaking out and shortening life spans. Who are these people?\nOf course it seems unlikely these über-analysts exist. Rather they surely do something more prosaic. Here is my guess as to how life expectancies are constructed.3 I guess that the data analyst gathers data on cohorts notionally formed at some point in the past and then looks at survival rates for that cohort over some period, then aggregates those data into a life expectancy.\nFor example, the data analyst might gather data on people who turned 21 in 2018 and then data on whether those people surived to their 22nd birthday. The proportion of such people who make their 22nd birthday could be interpreted as a survival probability \\(p_{21}\\). Repeat that for each age-based cohort to get probabilities \\(\\left\\{p_i: i = 0, 1, \\dots, 119, 120 \\right\\}\\). Now to find the median life expectancy, we could calculate something like this:4\n\\[ \\left\\{j: \\arg \\min_{i} \\left(\\prod_{0}^{i} p_i\\right) \\leq \\frac{1}{2} \\right\\} \\] So we have a (fairly) well-defined procedure here. There are obviously some details to be worked out. For example, do we focus on one year (2018 in this case), or collect data over multiple years. Does it make sense to form cohorts by years? Or would grouping into larger cohorts (e.g., 20–25) make more sense? Do we identify people by birthdays? Or just use some kind of census date? (People who are 21 on 1 July might have just turned 21, or might be about to turn 22.)\nBut what exactly have we calculated? In a sense it’s a nonsensical number. Why would survival rates for 88-year-olds in 2018 be relevant for the life expectancy of newborns today, who will face a very different world when they turn 88 in 2111. First, perhaps the analysts really don’t calculate it in this way (though I’m doubtful they are polymaths). Second, even though it’s a “meaningless” number, it probably still has attractive properties, such as the ability to represent in a one or two numbers a lot about the quality of life in Australia.\nA final note is that it is not clear to me where the “51.1 for boys and 54.8 years for girls born in in 1891–1900” values come from. Are these the equivalent life expectancies calculated using data available around 1900? Or are these the observed lifespans of people born in 1891–1900? If the latter, how accurate were the former as estimates of these values?\n\npg <- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\ndbExecute(pg, \"SET search_path TO sql_book\")\n\n\nSELECT id_bioguide ,min(term_start) AS first_term\nFROM legislators_terms \nGROUP BY 1\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\n\n\n\n\nA000118\n1975-01-14\n\n\nP000281\n1933-03-09\n\n\nK000039\n1933-03-09\n\n\nA000306\n1907-12-02\n\n\nO000095\n1949-01-03\n\n\nB000937\n1913-04-07\n\n\nS000038\n1912-01-01\n\n\nS000657\n1901-12-02\n\n\nP000383\n1873-12-01\n\n\nL000058\n1887-12-05\n\n\n\n\n\n\nWITH first_terms AS (\n      SELECT id_bioguide, min(term_start) AS first_term\n      FROM legislators_terms \n      GROUP BY 1) \nSELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n    count(distinct a.id_bioguide) as cohort_retained\nFROM first_terms a\nJOIN legislators_terms b on a.id_bioguide = b.id_bioguide \nGROUP BY 1\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n3600\n\n\n2\n3619\n\n\n3\n1831\n\n\n4\n3210\n\n\n5\n1744\n\n\n6\n2385\n\n\n7\n1360\n\n\n8\n1607\n\n\n9\n1028\n\n\n\n\n\n\nlegislators_terms <- tbl(pg, \"legislators_terms\")\nfirst_terms <- \n  legislators_terms %>%\n  group_by(id_bioguide) %>%\n  summarize(first_term = min(term_start, na.rm = TRUE))\n\ncohorts <-\n  legislators_terms %>%\n  inner_join(first_terms, by = \"id_bioguide\") %>%\n  mutate(period = date_part('year', age(term_start, first_term))) %>%\n  group_by(period) %>%\n  summarize(cohort_retained = sql(\"count(distinct id_bioguide)\")) \n\ncohorts %>%\n  collect(n = 10)\n\n# A tibble: 10 × 2\n   period cohort_retained\n    <dbl>           <int>\n 1      0           12518\n 2      1            3600\n 3      2            3619\n 4      3            1831\n 5      4            3210\n 6      5            1744\n 7      6            2385\n 8      7            1360\n 9      8            1607\n10      9            1028\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts AS (\n  SELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n      count(distinct a.id_bioguide) as cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  GROUP BY 1)\n  \nSELECT period,\n  first_value(cohort_retained) OVER (ORDER BY period) AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / first_value(cohort_retained) OVER (ORDER BY period) AS pct_retained\nFROM cohorts;\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n3600\n0.2875859\n\n\n2\n12518\n3619\n0.2891037\n\n\n3\n12518\n1831\n0.1462694\n\n\n4\n12518\n3210\n0.2564307\n\n\n5\n12518\n1744\n0.1393194\n\n\n6\n12518\n2385\n0.1905256\n\n\n7\n12518\n1360\n0.1086436\n\n\n8\n12518\n1607\n0.1283751\n\n\n9\n12518\n1028\n0.0821217\n\n\n\n\n\n\nretained_data <-\n  cohorts %>%\n  window_order(period) %>%\n  mutate(cohort_size = first(cohort_retained)) %>%\n  mutate(pct_retained = cohort_retained * 1.0/cohort_size) %>%\n  select(period, cohort_size, cohort_retained, pct_retained) \n\nretained_data %>%\n  collect(n = 10)\n\n# A tibble: 10 × 4\n   period cohort_size cohort_retained pct_retained\n    <dbl>       <int>           <int>        <dbl>\n 1      0       12518           12518       1     \n 2      1       12518            3600       0.288 \n 3      2       12518            3619       0.289 \n 4      3       12518            1831       0.146 \n 5      4       12518            3210       0.256 \n 6      5       12518            1744       0.139 \n 7      6       12518            2385       0.191 \n 8      7       12518            1360       0.109 \n 9      8       12518            1607       0.128 \n10      9       12518            1028       0.0821\n\n\n\nretained_data %>%\n  ggplot(aes(x = period, y = pct_retained)) +\n  geom_line()\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts AS (\n  SELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n      count(distinct a.id_bioguide) as cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  GROUP BY 1),\n  \nretained_data AS (\n  SELECT period,\n    first_value(cohort_retained) OVER (ORDER BY period) AS cohort_size,\n    cohort_retained,\n    cohort_retained * 1.0 / first_value(cohort_retained) OVER (ORDER BY period) AS pct_retained\n  FROM cohorts)\n\nSELECT cohort_size,\n  max(case when period = 0 then pct_retained end) as yr0,\n  max(case when period = 1 then pct_retained end) as yr1,\n  max(case when period = 2 then pct_retained end) as yr2,\n  max(case when period = 3 then pct_retained end) as yr3,\n  max(case when period = 4 then pct_retained end) as yr4\nFROM retained_data\nGROUP BY 1;\n\n\n1 records\n\n\ncohort_size\nyr0\nyr1\nyr2\nyr3\nyr4\n\n\n\n\n12518\n1\n0.2875859\n0.2891037\n0.1462694\n0.2564307\n\n\n\n\n\n\nretained_data %>%\n  select(period, pct_retained) %>%\n  collect() %>%\n  arrange(period) %>%\n  pivot_wider(names_from = period, \n              names_prefix = \"yr\",\n              values_from = pct_retained) %>%\n  collect()\n\n# A tibble: 1 × 57\n    yr0   yr1   yr2   yr3   yr4   yr5   yr6   yr7   yr8    yr9  yr10   yr11\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>\n1     1 0.288 0.289 0.146 0.256 0.139 0.191 0.109 0.128 0.0821 0.112 0.0733\n# ℹ 45 more variables: yr12 <dbl>, yr13 <dbl>, yr14 <dbl>, yr15 <dbl>,\n#   yr16 <dbl>, yr17 <dbl>, yr18 <dbl>, yr19 <dbl>, yr20 <dbl>, yr21 <dbl>,\n#   yr22 <dbl>, yr23 <dbl>, yr24 <dbl>, yr25 <dbl>, yr26 <dbl>, yr27 <dbl>,\n#   yr28 <dbl>, yr29 <dbl>, yr30 <dbl>, yr31 <dbl>, yr32 <dbl>, yr33 <dbl>,\n#   yr34 <dbl>, yr35 <dbl>, yr36 <dbl>, yr37 <dbl>, yr38 <dbl>, yr39 <dbl>,\n#   yr40 <dbl>, yr41 <dbl>, yr42 <dbl>, yr43 <dbl>, yr44 <dbl>, yr45 <dbl>,\n#   yr46 <dbl>, yr47 <dbl>, yr48 <dbl>, yr49 <dbl>, yr50 <dbl>, yr51 <dbl>, …\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT a.id_bioguide, a.first_term, b.term_start, b.term_end,\n  c.date,\n  date_part('year',age(c.date, a.first_term)) AS period\nFROM first_terms a\nJOIN legislators_terms b ON a.id_bioguide = b.id_bioguide \nLEFT JOIN date_dim c ON c.date BETWEEN b.term_start and b.term_end \nand c.month_name = 'December' and c.day_of_month = 31;\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\ndate\nperiod\n\n\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-03\n1993-12-31\n0\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-03\n1994-12-31\n1\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-03\n1993-12-31\n0\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-03\n1994-12-31\n1\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-03\n1987-12-31\n0\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-03\n1988-12-31\n1\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n1983-12-31\n0\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n1984-12-31\n1\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-03\n2007-12-31\n0\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-03\n2008-12-31\n1\n\n\n\n\n\n\nyear_ends <-\n  tbl(pg, sql(\"\n    SELECT generate_series::date AS date\n    FROM generate_series('1770-12-31', '2030-12-31', interval '1 year')\"))\n\ncohorts <-\n  first_terms %>%\n  inner_join(legislators_terms, by = join_by(id_bioguide)) %>%\n  left_join(year_ends, \n            by = join_by(between(y$date, x$term_start, x$term_end))) %>%\n  mutate(period = date_part('year', age(date, first_term))) %>%\n  select(id_bioguide, first_term, term_start, term_end, date, period) \n\ncohorts %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\ndate\nperiod\n\n\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-03\n1993-12-31\n0\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-03\n1994-12-31\n1\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-03\n1993-12-31\n0\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-03\n1994-12-31\n1\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-03\n1987-12-31\n0\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-03\n1988-12-31\n1\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n1983-12-31\n0\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n1984-12-31\n1\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-03\n2007-12-31\n0\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-03\n2008-12-31\n1\n\n\n\n\n\n\nyear_ends <- \n  tibble(date = seq(as.Date(\"1770-12-31\"), \n                    as.Date(\"2030-12-31\"), \n                    by = \"year\")) %>%\n  copy_inline(pg, .)\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n  \nyear_ends AS (\n  SELECT generate_series::date as date\n  FROM generate_series('1770-12-31', '2030-12-31', interval '1 year')\n)\n  \nSELECT \n  coalesce(date_part('year', age(c.date, a.first_term)), 0) AS period,\n  count(DISTINCT a.id_bioguide) AS cohort_retained\nFROM first_terms a\nJOIN legislators_terms b ON a.id_bioguide = b.id_bioguide \nLEFT JOIN year_ends c ON c.date BETWEEN b.term_start AND b.term_end \nGROUP BY 1;\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n12328\n\n\n2\n8166\n\n\n3\n8069\n\n\n4\n5862\n\n\n5\n5795\n\n\n6\n4361\n\n\n7\n4339\n\n\n8\n3521\n\n\n9\n3485\n\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n  \ncohorts AS (\n  SELECT * \n  FROM first_terms a\n  JOIN legislators_terms b USING (id_bioguide)\n  LEFT JOIN date_dim c ON c.date BETWEEN b.term_start AND b.term_end \n    AND c.month_name = 'December' AND c.day_of_month = 31)\n  \nSELECT \n  coalesce(date_part('year', age(date, first_term)), 0) AS period,\n  count(DISTINCT id_bioguide) AS cohort_retained\nFROM cohorts\nGROUP BY 1;\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n12328\n\n\n2\n8166\n\n\n3\n8069\n\n\n4\n5862\n\n\n5\n5795\n\n\n6\n4361\n\n\n7\n4339\n\n\n8\n3521\n\n\n9\n3485\n\n\n\n\n\n\ncohorts_retained <-\n  cohorts %>%\n  mutate(period = coalesce(date_part('year', age(date, first_term)), 0)) %>%\n  select(period, id_bioguide) %>%\n  distinct() %>%\n  group_by(period) %>%\n  summarize(cohort_retained = n()) %>%\n  arrange(period)\n\ncohorts_retained %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n12328\n\n\n2\n8166\n\n\n3\n8069\n\n\n4\n5862\n\n\n5\n5795\n\n\n6\n4361\n\n\n7\n4339\n\n\n8\n3521\n\n\n9\n3485\n\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT id_bioguide, a.first_term, b.term_start,\n  CASE WHEN b.term_type = 'rep' THEN b.term_start + interval '2 years'\n       WHEN b.term_type = 'sen' THEN b.term_start + interval '6 years'\n  END AS term_end\nFROM first_terms a\nJOIN legislators_terms b USING (id_bioguide);\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\n\n\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-06\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-04\n\n\nF000062\n1992-11-10\n1992-11-10\n1998-11-10\n\n\nF000469\n2019-01-03\n2019-01-03\n2021-01-03\n\n\nK000367\n2007-01-04\n2007-01-04\n2013-01-04\n\n\nM000639\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nS000033\n1991-01-03\n1991-01-03\n1993-01-03\n\n\n\n\n\n\nfirst_terms %>%\n  inner_join(legislators_terms, by = join_by(id_bioguide)) %>%\n  mutate(term_end = \n           case_when(term_type == 'rep' ~ term_start + years(2),\n                     term_type == 'sen' ~ term_start + years(6))) %>%\n  select(id_bioguide, first_term, term_start, term_end)  %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\n\n\n\n\nB000944\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nC000127\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nC000141\n1987-01-06\n1987-01-06\n1989-01-06\n\n\nC000174\n1983-01-03\n1983-01-03\n1985-01-03\n\n\nC001070\n2007-01-04\n2007-01-04\n2013-01-04\n\n\nF000062\n1992-11-10\n1992-11-10\n1998-11-10\n\n\nF000469\n2019-01-03\n2019-01-03\n2021-01-03\n\n\nK000367\n2007-01-04\n2007-01-04\n2013-01-04\n\n\nM000639\n1993-01-05\n1993-01-05\n1995-01-05\n\n\nS000033\n1991-01-03\n1991-01-03\n1993-01-03\n\n\n\n\n\n\n\n\n\n\nIn writing this sentence, I am still working through the chapter.↩︎\nSee here for the source for these data.↩︎\nI made no effort to research how they are constructed because (1) I am lazy and (2) my imagined version is probably better for my current purposes.↩︎\nThere are some details I’m glossing over here, such as the fact that there will be some \\(j\\) where the cumulative survival probability is just above one-half, but where that for \\(j + 1\\) is just below one-half, so some interpolation will be required.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Tanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  }
]