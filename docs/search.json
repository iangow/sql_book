[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SQL for Data Analysis using R",
    "section": "",
    "text": "Bachelor of Commerce/Bachelor of Laws at the University of New South Wales.↩︎\nI vaguely recall getting a lower grade than my friend for the group assignment, which was inexplicable since there was no indication who did what. I survived.↩︎\nI recall that partners had the higher-powered laptops—necessary for writing Lotus Notes and reading Powerpoint slides—while analysts like me had older laptops. It didn’t make sense to me at the time, and still doesn’t.↩︎"
  },
  {
    "objectID": "chapter_1.html#why-sql",
    "href": "chapter_1.html#why-sql",
    "title": "1  Analysis with SQL",
    "section": "1.2 Why SQL?",
    "text": "1.2 Why SQL?\n\n1.2.1 What is SQL?\n\n\n1.2.2 Benefits of SQL\n\n\n1.2.3 SQL versus R or Python\n\n\n1.2.4 SQL as Part of the Data Analysis Workflow"
  },
  {
    "objectID": "chapter_1.html#database-types-and-how-to-work-with-them",
    "href": "chapter_1.html#database-types-and-how-to-work-with-them",
    "title": "1  Analysis with SQL",
    "section": "1.3 Database Types and How to Work with Them",
    "text": "1.3 Database Types and How to Work with Them\n\n1.3.1 Row-Store Databases\n\n\n1.3.2 Column-Store Databases\n\n\n1.3.3 Other Types of Data Infrastructure"
  },
  {
    "objectID": "chapter_1.html#conclusion",
    "href": "chapter_1.html#conclusion",
    "title": "1  Analysis with SQL",
    "section": "1.4 Conclusion",
    "text": "1.4 Conclusion"
  },
  {
    "objectID": "chapter_2.html",
    "href": "chapter_2.html",
    "title": "2  Preparing Data for Analysis",
    "section": "",
    "text": "Chapter 2 of Tanimura (2021) provides a good foundation discussion of issues related to preparing data for analysis. While the discussion is couched in terms of SQL, in reality the issues are not specific to SQL or databases. For this reason, I recommend that you read the chapter.\nWhile Chapter 2 of Tanimura (2021) contains many code snippets, few of these seem to be intended for users to run (in part because they assume a database set-up that users would not have). For this reason, I do not attempt to provide dplyr equivalents to the code there except for a couple of exceptions that I discuss below.\nIn my view, some of the material in Chapter 2 of Tanimura (2021) would be better placed later in the book where there is more context for some of the issues discussed. Chapter 2 of Tanimura (2021) contains some example code, but no data, so the reader can merely read. As such, there isn’t much for me to parallel here. Nonetheless, I recommend you go read Chapter 2 before coming back here."
  },
  {
    "objectID": "chapter_2.html#types-of-data",
    "href": "chapter_2.html#types-of-data",
    "title": "2  Preparing Data for Analysis",
    "section": "2.1 Types of Data",
    "text": "2.1 Types of Data\n\n2.1.1 Database Data Types\nR has all the types listed in Chapter 2. PostgreSQL has a richer set of data types than base R has.1\nIn general, data retains the equivalent type when transferred from R to PostgreSQL or vice versa. Sometimes data makes round trips (e.g., data is pulled from PostgreSQL to R for computations and then sent back to R) and it is important to check that data types are retained in the process (e.g., that timestamps don’t shift to a different time zone).\n\n\n2.1.2 Structured versus Unstructured\n\n\n2.1.3 Quantitative versus Qualitative Data\n\n\n2.1.4 First-, Second-, and Third-Party Data\n\n\n2.1.5 Sparse Data"
  },
  {
    "objectID": "chapter_2.html#sql-query-structure",
    "href": "chapter_2.html#sql-query-structure",
    "title": "2  Preparing Data for Analysis",
    "section": "2.2 SQL Query Structure",
    "text": "2.2 SQL Query Structure"
  },
  {
    "objectID": "chapter_2.html#profiling-distributions",
    "href": "chapter_2.html#profiling-distributions",
    "title": "2  Preparing Data for Analysis",
    "section": "2.3 Profiling: Distributions",
    "text": "2.3 Profiling: Distributions\n\n2.3.1 Histograms and Frequencies\n\n\n2.3.2 Binning\n\n\n2.3.3 n-Tiles"
  },
  {
    "objectID": "chapter_2.html#profiling-data-quality",
    "href": "chapter_2.html#profiling-data-quality",
    "title": "2  Preparing Data for Analysis",
    "section": "2.4 Profiling: Data Quality",
    "text": "2.4 Profiling: Data Quality\n\n2.4.1 Detecting Duplicates\n\n\n2.4.2 Deduplication with GROUP BY and DISTINCT"
  },
  {
    "objectID": "chapter_2.html#preparing-data-cleaning",
    "href": "chapter_2.html#preparing-data-cleaning",
    "title": "2  Preparing Data for Analysis",
    "section": "2.5 Preparing: Data Cleaning",
    "text": "2.5 Preparing: Data Cleaning\n\n2.5.1 Cleaning Data with CASE Transformations\n\n\n2.5.2 Type Conversions and Casting\n\n\n2.5.3 Dealing with Nulls: coalesce, nullif, nvl Functions\n\n\n2.5.4 Missing data\nThe SQL in the book generally uses the form x::date rather than the more standard SQL CAST(x AS DATE). In dbplyr, we would use as.Date(x) and dbplyr would translate as CAST(x AS DATE). The following code and output demonstrates how dbplyr translated from dplyr to SQL.\nThe table stored in dates_processed below is equivalent to that created and stored in the database as date_dim in the code supplied with book. This date_dim table is only used in #sec-time-series of the book and we will not even use it there (for reasons to be explained).\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(knitr)\n\n\npg <- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)"
  },
  {
    "objectID": "chapter_2.html#preparing-shaping-data",
    "href": "chapter_2.html#preparing-shaping-data",
    "title": "2  Preparing Data for Analysis",
    "section": "2.6 Preparing: Shaping Data",
    "text": "2.6 Preparing: Shaping Data\n\n2.6.1 For Which Output: BI, Visualization, Statistics, ML\n\n\n2.6.2 Pivoting with CASE Statements\n\n\n2.6.3 Unpivoting with UNION Statements\nA user of dplyr has access to the functions pivot_wider and pivot_longer, which make it much easier to “pivot” and “unpivot” tables than using CASE statements, which could become long and tedious.\nTo illustrate the dplyr way of doing things, I will create ctry_pops to match the data discussed in Chapter 2. First, I create the data set using the tribble() function from dplyr.\n\nctry_pops <-\n  tribble(\n  ~country, ~year_1980,  ~year_1990, ~year_2000, ~year_2010,\n  \"Canada\", 24593, 27791, 31100, 34207,\n  \"Mexico\", 68347, 84634, 99775, 114061,\n  \"United States\", 227225, 249623, 282162, 309326\n)\n\nSecond, I pivot the local data frame using pivot_longer.\n\nctry_pops_long <-\n  ctry_pops %>%\n  pivot_longer(cols = -country, \n               names_to = \"year\",\n               names_prefix = \"year_\",\n               values_ptypes = integer(),\n               values_to = \"population\") \nctry_pops_long %>%\n  kable()\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\nCanada\n1980\n24593\n\n\nCanada\n1990\n27791\n\n\nCanada\n2000\n31100\n\n\nCanada\n2010\n34207\n\n\nMexico\n1980\n68347\n\n\nMexico\n1990\n84634\n\n\nMexico\n2000\n99775\n\n\nMexico\n2010\n114061\n\n\nUnited States\n1980\n227225\n\n\nUnited States\n1990\n249623\n\n\nUnited States\n2000\n282162\n\n\nUnited States\n2010\n309326\n\n\n\n\n\nNext, I copy the data to PostgreSQL, so that it’s a (temporary) table inside the database.2\n\nctry_pops_db <- copy_to(pg, ctry_pops)\nctry_pops_db\n\n# Source:   table<ctry_pops> [3 x 5]\n# Database: postgres  [igow@localhost:5432/igow]\n  country       year_1980 year_1990 year_2000 year_2010\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 Canada            24593     27791     31100     34207\n2 Mexico            68347     84634     99775    114061\n3 United States    227225    249623    282162    309326\n\n\n\nctry_pops_db_long <-\n  ctry_pops_db %>%\n  pivot_longer(cols = -country, \n               names_to = \"year\",\n               names_prefix = \"year_\",\n               values_to = \"population\") \n\nFrom the output below, we can see that dbplyr has taken care of the tedious business of constructing several statements for us.\n\nctry_pops_db_long %>%\n  show_query()\n\n<SQL>\n(\n  (\n    (\n      SELECT \"country\", '1980' AS \"year\", \"year_1980\" AS \"population\"\n      FROM \"ctry_pops\"\n    )\n    UNION ALL\n    (\n      SELECT \"country\", '1990' AS \"year\", \"year_1990\" AS \"population\"\n      FROM \"ctry_pops\"\n    )\n  )\n  UNION ALL\n  (\n    SELECT \"country\", '2000' AS \"year\", \"year_2000\" AS \"population\"\n    FROM \"ctry_pops\"\n  )\n)\nUNION ALL\n(\n  SELECT \"country\", '2010' AS \"year\", \"year_2010\" AS \"population\"\n  FROM \"ctry_pops\"\n)\n\n\nAnd from the following, we can see that the result is the same as it was when using dplyr on a local data frame.\n\nctry_pops_db_long %>%\n  kable()\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\nCanada\n1980\n24593\n\n\nMexico\n1980\n68347\n\n\nUnited States\n1980\n227225\n\n\nCanada\n1990\n27791\n\n\nMexico\n1990\n84634\n\n\nUnited States\n1990\n249623\n\n\nCanada\n2000\n31100\n\n\nMexico\n2000\n99775\n\n\nUnited States\n2000\n282162\n\n\nCanada\n2010\n34207\n\n\nMexico\n2010\n114061\n\n\nUnited States\n2010\n309326\n\n\n\n\n\nAnd we can reverse the pivot_longer() using pivot_wider().\n\nctry_pops_db_long %>%\n  compute() %>%\n  pivot_wider(names_from = year, \n              values_from = population, \n              names_prefix = \"year_\") %>%\n  kable()\n\n\n\n\ncountry\nyear_2010\nyear_1990\nyear_2000\nyear_1980\n\n\n\n\nMexico\n114061\n84634\n99775\n68347\n\n\nCanada\n34207\n27791\n31100\n24593\n\n\nUnited States\n309326\n249623\n282162\n227225\n\n\n\n\n\n\n\n2.6.4 pivot and unpivot Functions\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_3.html",
    "href": "chapter_3.html",
    "title": "3  Time Series Analysis",
    "section": "",
    "text": "This chapter is really two chapters. The first part of the chapter discusses some finer details of dates, date-times, and time stamps. This material is important, but a little technical and bewildering wihtout some concrete use cases.\nThe second part of the chapter is where Tanimura (2021) really starts to take off. This is where it starts playing around with real data. A first-time reader could probably skip ahead to Section 3.2 without much loss."
  },
  {
    "objectID": "chapter_3.html#date-datetime-and-time-manipulations",
    "href": "chapter_3.html#date-datetime-and-time-manipulations",
    "title": "3  Time Series Analysis",
    "section": "3.1 Date, Datetime, and Time Manipulations",
    "text": "3.1 Date, Datetime, and Time Manipulations\n\nINSTALL 'icu';\nLOAD 'icu';\n\n\n3.1.1 Time Zone Conversions\nTanimura (2021) points out that often “timestamps in the database are not encoded with the time zone, and you will need to consult with the source or developer to figure out how your data was stored.” When pushing data to a PostgreSQL database, I use the timestamp with time zone type as much as possible.\nTanimura (2021) provides the following example, which is interesting because the west coast of the United States would not be on the PST time zone at that time of year. Instead, it would be on PDT.\n\nSELECT timestamptz '2020-09-01 00:00:00 -0' AT TIME ZONE 'PST' AS time;\n\n\n1 records\n\n\ntime\n\n\n\n\n2020-08-31 17:00:00\n\n\n\n\n\nIn PostgreSQL, we would get a different answer with the following query, but in DuckDB it seems that PDT is not recognized as a time zone abbreviation at all, so we just get the original UTC timestamp back.\n\nSELECT timestamptz '2020-09-01 00:00:00 -0' AT TIME ZONE 'PDT' AS time;\n\n\n1 records\n\n\ntime\n\n\n\n\n2020-09-01\n\n\n\n\n\nI think most people barely know the difference between PST and PDT and even fewer would know the exact dates that one switches from one to the other. A better approach is to use a time zone that encodes information about when PDT is used and when PST is used. In PostgreSQL and DuckDB, the function pg_timezone_names() returns a table with the information that we need.\nHowever, it seems that there are inconsistencies between PostgreSQL and DuckDB in terms of the abbreviations used. As such it’s probably safer to use the name form (e.g., US/Pacific) whenever possible. Given the widespread confusion about the meaning of terms like EST, EDT, and so on, just use US/Eastern, etc.\n\nSELECT name, abbrev\nFROM pg_timezone_names()\nWHERE regexp_matches(name, '^US/');\n-- use WHERE name ~ '^US/'; in PostgreSQL\n\n\nDisplaying records 1 - 10\n\n\nname\nabbrev\n\n\n\n\nUS/Alaska\nAST\n\n\nUS/Aleutian\nUS/Aleutian\n\n\nUS/Arizona\nPNT\n\n\nUS/Central\nCST\n\n\nUS/East-Indiana\nIET\n\n\nUS/Eastern\nUS/Eastern\n\n\nUS/Hawaii\nUS/Hawaii\n\n\nUS/Indiana-Starke\nUS/Indiana-Starke\n\n\nUS/Michigan\nUS/Michigan\n\n\nUS/Mountain\nNavajo\n\n\n\n\n\nThe following queries demonstrate that daylight savings information is encoded in the database.\n\nSELECT '2020-09-01 17:00:01 US/Pacific'::timestamptz AS t1,\n       '2020-09-02 10:00:01 Australia/Melbourne'::timestamptz  AS t2;\n\n\n1 records\n\n\nt1\nt2\n\n\n\n\n2020-09-02 00:00:01\n2020-09-02 00:00:01\n\n\n\n\n\n\nSELECT '2020-12-01 16:00:01 US/Pacific'::timestamptz AS t1,\n       '2020-12-02 11:00:01 Australia/Melbourne'::timestamptz  AS t2;\n\n\n1 records\n\n\nt1\nt2\n\n\n\n\n2020-12-02 00:00:01\n2020-12-02 00:00:01\n\n\n\n\n\n\nsql <-\n  \"(SELECT     \n    '2020-12-01 16:00:01 US/Pacific'::timestamptz AS t1)\"\n\na_time <- tbl(db, sql(sql))\na_time %>%\n  kable()\n\n\n\n\nt1\n\n\n\n\n2020-12-02 00:00:01\n\n\n\n\n\n\na_time_r <-\n  a_time %>%\n  select(t1) %>%\n  pull()\n\nprint(a_time_r, tz = \"UTC\")\n\n[1] \"2020-12-02 00:00:01 UTC\"\n\nprint(a_time_r, tz = \"US/Pacific\")\n\n[1] \"2020-12-01 16:00:01 PST\"\n\nSys.timezone()\n\n[1] \"Australia/Melbourne\"\n\nprint(a_time_r, tz = Sys.timezone())\n\n[1] \"2020-12-02 11:00:01 AEDT\"\n\n\nThe above examples illustrate a few key ideas.\nFirst, while we supply the literal form '2020-09-01 17:00:01 US/Pacific'::timestamptz, it seems that once a variable has been encoded as TIMESTAMP WITH TIME ZONE, it behaves as though it is actually being stored as a timestamp in the UTC time zone, just with the displayed time perhaps being different.\nSecond, columns of type TIMESTAMP WITH TIME ZONE come into R with the associated time-zone information, which is what we want (especially if we will later put timestamp data back into PostgreSQL).\nThird, we can see that we can choose to display information in a different time zone without changing the underlying data.\nSome care is needed with timestamp data. I think the AT TIME ZONE queries provided in Tanimura (2021) are actually pretty dangerous, as can be seen in the following query. While we supply 2020-09-01 00:00:01 as UTC and then render it AT TIME ZONE 'US/Pacific', it turns out that the returned value is interpreted as a TIMESTAMP WITHOUT TIME ZONE and subsequent queries lead to confusing behaviour. In the query below, the second application of AT TIME ZONE interprets the TIMESTAMP WITHOUT TIME ZONE as though it came from the stated time zone and the results seem to have AT TIME ZONE doing the opposite of what it did when given a TIMESTAMP WITH TIME ZONE (as in the initial literal '2020-09-01 00:00:01 -0').\n\nWITH q1 AS\n (SELECT timestamptz '2020-09-01 00:00:01-00' AT TIME ZONE 'US/Pacific' AS t1,\n         timestamp '2020-09-01 00:00:01' AT TIME ZONE 'US/Pacific' AS t2)\n \nSELECT \n  t1,\n  t1::varchar AS t1_char,\n  t1 AT TIME ZONE 'UTC' AS t3,\n  t2 AT TIME ZONE 'UTC' AS t4,\n  typeof(t1),\n  typeof(t2)\nFROM q1\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\nt1\nt1_char\nt3\nt4\ntypeof(t1)\ntypeof(t2)\n\n\n\n\n2020-08-31 17:00:01\n2020-08-31 17:00:01\n2020-08-31 17:00:01\n2020-09-01 07:00:01\nTIMESTAMP\nTIMESTAMP WITH TIME ZONE\n\n\n\n\n\nIt seems that TIMESTAMP WITHOUT TIME ZONE values should be converted to a time zone as quickly as possible to avoid confusion and that care is needed with AT TIME ZONE given that it does very different (essentially opposite) things according to the supplied data type.\n\nWITH q1 AS\n (SELECT '2020-09-01 00:00:01-00'::timestamptz AS t1)\n \nSELECT t1,\n  t1::varchar AS t2,\n  typeof(t1)\nFROM q1\n\n\n1 records\n\n\nt1\nt2\ntypeof(t1)\n\n\n\n\n2020-09-01 00:00:01\n2020-09-01 10:00:01+10\nTIMESTAMP WITH TIME ZONE\n\n\n\n\n\nStrange behaviour can result from values stored as TIMESTAMP WITHOUT TIME ZONE. Below we see that t1 is printed as UTC no matter what, while the behaviour of t2 seems easier to understand.\n\nsql <-\n  \"(SELECT     \n    '2020-12-01 00:00:01-00' AS t1,\n    '2020-12-01 00:00:01-00'::timestamptz AS t2)\"\n\ntwo_times_notz <- tbl(db, sql(sql))\ntwo_times_notz %>% kable()\n\n\n\n\nt1\nt2\n\n\n\n\n2020-12-01 00:00:01-00\n2020-12-01 00:00:01\n\n\n\n\n\n\ntwo_times_notz_r <-\n  collect(two_times_notz)\n  \nprint(two_times_notz_r$t1)\n\n[1] \"2020-12-01 00:00:01-00\"\n\nSys.timezone()\n\n[1] \"Australia/Melbourne\"\n\nprint(two_times_notz_r$t1, tz = Sys.timezone())\n\n[1] \"2020-12-01 00:00:01-00\"\n\nprint(two_times_notz_r$t2)\n\n[1] \"2020-12-01 00:00:01 UTC\"\n\nSys.timezone()\n\n[1] \"Australia/Melbourne\"\n\nprint(two_times_notz_r$t2, tz = Sys.timezone())\n\n[1] \"2020-12-01 11:00:01 AEDT\"\n\n\nAs pointed out by Tanimura (2021), one drawback to storing information as UTC is that localtime information may be lost. But it seems it would be more prudent to store information as TIMESTAMP WITH TIME ZONE and keep local time zone information as a separate column to avoid confusion. For example, if the orders table is stored as TIMESTAMP WITHOUT TIME ZONE based on the local time of the customer, which might be Australia/Melbourne and the shipping table uses TIMESTAMP WITH TIME ZONE, then an analyst of time-to-ship data would be confused by orders apparently being shipped before they are made. If shipping table uses TIMESTAMP WITH TIME ZONE using timestamps in the time zone of the East Bay warehouse (so US/Pacific), things would be even worse.\nI think that fully fleshing out the issues here would require a separate chapter. In fact, nothing in the core part of Chapter 3 of Tanimura (2021) (which focuses on the retail_sales table) really uses timestamp information, so we can put these issues aside for now.\n\n\n3.1.2 Date and Timestamp Format Conversions\nAs discussed in Tanimura (2021), PostgreSQL has a rich array of functions for converting dates and times and extracting such information as months and days of the week.\n\nSELECT date_trunc('month','2020-10-04 12:33:35 -00'::timestamptz);\n\n\n1 records\n\n\n\n\n\ndate_trunc(‘month’, CAST(‘2020-10-04 12:33:35 -00’ AS TIMESTAMP WITH TIME ZONE))\n\n\n\n\n2020-09-30 14:00:00\n\n\n\n\n\nOne such function\n\na_time_df <- tbl(db, sql(\"(SELECT '2020-10-04 12:33:35'::timestamp AS a_time)\"))\n\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time))\n\n# Source:   SQL [1 x 2]\n# Database: DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.2/:memory:]\n  a_time              a_trunced_time\n  <dttm>              <date>        \n1 2020-10-04 12:33:35 2020-10-01    \n\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time)) %>%\n  show_query()\n\n<SQL>\nSELECT *, date_trunc('month', a_time) AS a_trunced_time\nFROM (SELECT '2020-10-04 12:33:35'::timestamp AS a_time)\n\na_time_df %>%\n  collect()\n\n# A tibble: 1 × 1\n  a_time             \n  <dttm>             \n1 2020-10-04 12:33:35\n\n\n\n\n3.1.3 Date Math\n\n\n3.1.4 Time Math\n\na_time_df <- tbl(db, sql(\"(SELECT '2020-10-04 12:33:35 US/Pacific'::timestamptz AS a_time)\"))\n\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time)) \n\n# Source:   SQL [1 x 2]\n# Database: DuckDB 0.7.1 [igow@Darwin 22.5.0:R 4.2.2/:memory:]\n  a_time              a_trunced_time     \n  <dttm>              <dttm>             \n1 2020-10-04 19:33:35 2020-09-30 14:00:00\n\na_time_df %>% \n  mutate(a_trunced_time = date_trunc('month', a_time)) %>%\n  show_query()\n\n<SQL>\nSELECT *, date_trunc('month', a_time) AS a_trunced_time\nFROM (SELECT '2020-10-04 12:33:35 US/Pacific'::timestamptz AS a_time)\n\na_time_df %>%\n  collect()\n\n# A tibble: 1 × 1\n  a_time             \n  <dttm>             \n1 2020-10-04 19:33:35\n\n\n\na_time_df %>%\n  mutate(new_time = a_time + sql(\"interval '3 hours'\")) %>%\n  collect()\n\n# A tibble: 1 × 2\n  a_time              new_time           \n  <dttm>              <dttm>             \n1 2020-10-04 19:33:35 2020-10-04 22:33:35\n\n\n\n\n3.1.5 Joining Data from Different Sources"
  },
  {
    "objectID": "chapter_3.html#sec-retail-data",
    "href": "chapter_3.html#sec-retail-data",
    "title": "3  Time Series Analysis",
    "section": "3.2 The Retail Sales Data Set",
    "text": "3.2 The Retail Sales Data Set\nAs discussed in Tanimura (2021), the data set used in this chapter comes from the website of the US Census Bureau. The data set is a little messy, but not too large, so we can easily grab it directly from the website and clean it up in much the same way that Cathy has done for us.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Use tmpdir = \".\" or known directory if you have trouble with \n# this part.\nmrtssales <- tempfile(fileext = \".xlsx\")\nurl <- paste0(\"https://www.census.gov/retail/mrts/www/\",\n              \"mrtssales92-present.xlsx\")\ndownload.file(url, mrtssales)\n\nread_tab <- function(year) {\n  \n  # Initially we read all columns as text, as we want to process\n  # more precise than read_excel() would do unsupervised.\n  temp <- read_excel(mrtssales,\n                     range = \"A4:N71\", \n                     sheet = as.character(year),\n                     col_types = \"text\",\n                     col_names = paste0(\"v\", 1:14))\n  \n  # The third row has the dates for columns 3:14\n  names(temp) <- c(\"naics_code\", \"kind_of_business\",\n                   as.character(temp[2, 3:14]))\n  \n  # The actual data are found after row 3\n  temp <- temp[-1:-3, ]\n  \n  # Now pivot the data and convert sales to numeric values.\n  # Also convert sales_month to dates (start of respective month).\n  df <-\n    temp %>%\n    pivot_longer(names_to = \"sales_month\",\n                 values_to = \"sales\",\n                 cols = -1:-2) %>%\n    mutate(sales_month = paste(\"01\", str_remove(sales_month, \"\\\\.\")),\n           sales_month = as.Date(sales_month, \"%d %b %Y\")) %>%\n    mutate(reason_for_null = case_when(sales == \"(NA)\" ~ \"Not Available\",\n                                       sales == \"(S)\" ~ \"Supressed\",\n                                       TRUE ~ NA),\n           sales = case_when(sales == \"(NA)\" ~ NA,\n                             sales == \"(S)\" ~ NA,\n                             TRUE ~ sales)) %>%\n    mutate(sales = as.double(sales)) %>%\n    select(sales_month, naics_code, kind_of_business,\n           reason_for_null, sales)\n  df\n}\n\nretail_sales_local <- bind_rows(lapply(1992:2020, read_tab)) \n\nAlternatively, we could just grab a CSV file from the GitHub repository for Tanimura (2021).\n\nurl <- paste0(\"https://raw.githubusercontent.com/\",\n              \"cathytanimura/sql_book/master/\",\n              \"Chapter%203%3A%20Time%20Series%20Analysis/\",\n              \"us_retail_sales.csv\")\nretail_sales_local <- read_csv(url, show_col_types = FALSE)\n\nNote that there are differences between the two versions of retail_sales_local above, as US Census economic data is continually being revised even after being released.\nNow that we have the data in R, a few questions arise.\nFirst, why would we move it to a database? Second, how can we move it to a database? Related to the previous question will be: which database to we want to move it to?\nTaking these questions in turn, the first one is a good question. With retail_sales as a local data frame, we can run almost all the analyses below with only the slightest modifications. The modifications needed are to replace every instance of window_order() with arrange().1 The “almost all” relates to the moving average, which relies on window_frame() from dbplyr, which has no exact equivalent in dplyr.\nSo the first point is that “almost all” implies an advantage for using dbplyr. On occasion, the SQL engine provided by PostgreSQL will allow us to do some data manipulations more easily than we can in R. But of course, there are many cases when the opposite is true. That said, why not have both? Store data in a database and collect() as necessary to do things that R is better at?\nSecond, performance can be better using SQL. Compiling a version of this chapter using dplyr with a local data frame for the remainder took just under 14 seconds. Using a PostgreSQL backend, it took 8.5 seconds. Adding back in the queries with window_order() that I removed so that I could compile with dplyr and using DuckDB as the backend, the document took 8.3 seconds to compile (this beat out PostgreSQL doing the same in 9.6 seconds). While these differences are not practically very significant, they could be with more demanding tasks. Also, a database will not always beat R or Python for performance, but often will and having the option to use a database backend is a good thing.\nThird, having the data in a database allows you to interrogate the data using SQL. If you are more familiar with SQL, or just know how to do a particular task in SQL, this can be beneficial.2\nFourth, I think there is merit in separating the tasks of acquiring and cleaning data from the task of analysing those data. Many data analysts have a work flow that entails ingesting and cleaning data for each analysis task. My experience is that it is often better to do the ingesting and cleaning once and then reuse the cleaned data in subsequent analyses. A common pattern involves reusing the same data in many analyses and it can be helpful to divide the tasks in a way that using an SQL database encourages. Also the skills in ingesting and cleaning data can be different from those for analysing those data, so sometimes it makes sense for one person to do one task, push the data to a database, and then have someone else do some or all of the analysis.\nRegarding the second question, there are a few options. But for this chapter we will use duckdb\nTo install DuckDB, all we have to do is install.packages(\"duckdb\").\nThen we can create a connection as follows.\n\ndb <- dbConnect(duckdb::duckdb())\n\nHere we use the default of an in-memory database. At the end of this chapter, we discuss how we could store the data in a file (say, legislators.duckdb) if we want persistent storage.\n\nretail_sales <- copy_to(db, retail_sales_local, \"retail_sales\")"
  },
  {
    "objectID": "chapter_3.html#trending-the-data",
    "href": "chapter_3.html#trending-the-data",
    "title": "3  Time Series Analysis",
    "section": "3.3 Trending the Data",
    "text": "3.3 Trending the Data\n\n3.3.1 Simple Trends\n\nSELECT sales_month, sales\nFROM retail_sales\nWHERE kind_of_business = 'Retail and food services sales, total'\nORDER BY 1\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nsales\n\n\n\n\n1992-01-01\n146376\n\n\n1992-02-01\n147079\n\n\n1992-03-01\n159336\n\n\n1992-04-01\n163669\n\n\n1992-05-01\n170068\n\n\n1992-06-01\n168663\n\n\n1992-07-01\n169890\n\n\n1992-08-01\n170364\n\n\n1992-09-01\n164617\n\n\n1992-10-01\n173655\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business == 'Retail and food services sales, total') %>%\n  select(sales_month, sales) %>%\n  ggplot(aes(x = sales_month, y = sales)) +\n  geom_line()\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year,\n    sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business = 'Retail and food services sales, total'\nGROUP BY 1\n;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nsales\n\n\n\n\n1992\n2014102\n\n\n1993\n2153095\n\n\n1994\n2330235\n\n\n1995\n2450628\n\n\n1996\n2603794\n\n\n1997\n2726131\n\n\n1998\n2852956\n\n\n1999\n3086990\n\n\n2000\n3287537\n\n\n2001\n3378906\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business == 'Retail and food services sales, total') %>%\n  mutate(sales_year = year(sales_month)) %>%\n  group_by(sales_year) %>%\n  summarize(sales = sum(sales, na.rm = TRUE)) %>%\n  ggplot(aes(x = sales_year, y = sales)) +\n  geom_line()\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year, \n  kind_of_business, sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business IN \n          ('Book stores',\n           'Sporting goods stores',\n           'Hobby, toy, and game stores')\nGROUP BY 1,2\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nkind_of_business\nsales\n\n\n\n\n1992\nBook stores\n8327\n\n\n1992\nSporting goods stores\n15583\n\n\n1992\nHobby, toy, and game stores\n11251\n\n\n1993\nBook stores\n9108\n\n\n1993\nSporting goods stores\n16791\n\n\n1993\nHobby, toy, and game stores\n11651\n\n\n1994\nBook stores\n10107\n\n\n1994\nSporting goods stores\n18825\n\n\n1994\nHobby, toy, and game stores\n12850\n\n\n1995\nBook stores\n11196\n\n\n\n\n\n\n\n3.3.2 Comparing Components\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c('Book stores',\n             'Sporting goods stores',\n             'Hobby, toy, and game stores')) %>%\n  mutate(sales_year = year(sales_month)) %>%\n  group_by(sales_year, kind_of_business) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT sales_month, kind_of_business, sales\nFROM retail_sales\nWHERE kind_of_business IN ('Men''s clothing stores','Women''s clothing stores')\nORDER BY 1,2;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nkind_of_business\nsales\n\n\n\n\n1992-01-01\nMen’s clothing stores\n701\n\n\n1992-01-01\nWomen’s clothing stores\n1873\n\n\n1992-02-01\nMen’s clothing stores\n658\n\n\n1992-02-01\nWomen’s clothing stores\n1991\n\n\n1992-03-01\nMen’s clothing stores\n731\n\n\n1992-03-01\nWomen’s clothing stores\n2403\n\n\n1992-04-01\nMen’s clothing stores\n816\n\n\n1992-04-01\nWomen’s clothing stores\n2665\n\n\n1992-05-01\nMen’s clothing stores\n856\n\n\n1992-05-01\nWomen’s clothing stores\n2752\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% c(\"Men's clothing stores\",\n                                 \"Women's clothing stores\")) %>%\n  select(sales_month, kind_of_business, sales) %>%\n  ggplot(aes(x = sales_month, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT date_part('year',sales_month) as sales_year,\n  kind_of_business, sum(sales) as sales\nFROM retail_sales\nWHERE kind_of_business IN \n        ('Men''s clothing stores',\n        'Women''s clothing stores')\nGROUP BY 1, 2\nORDER BY 1, 2;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nkind_of_business\nsales\n\n\n\n\n1992\nMen’s clothing stores\n10179\n\n\n1992\nWomen’s clothing stores\n31815\n\n\n1993\nMen’s clothing stores\n9962\n\n\n1993\nWomen’s clothing stores\n32350\n\n\n1994\nMen’s clothing stores\n10032\n\n\n1994\nWomen’s clothing stores\n30585\n\n\n1995\nMen’s clothing stores\n9315\n\n\n1995\nWomen’s clothing stores\n28696\n\n\n1996\nMen’s clothing stores\n9546\n\n\n1996\nWomen’s clothing stores\n28238\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  mutate(sales_year = year(sales_month)) %>%\n  group_by(sales_year, kind_of_business) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  ggplot(aes(x = sales_year, y = sales, color = kind_of_business)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nSELECT date_part('year', sales_month) AS sales_year,\n  sum(CASE WHEN kind_of_business = 'Women''s clothing stores' \n          then sales \n          END) AS womens_sales,\n  sum(CASE WHEN kind_of_business = 'Men''s clothing stores' \n          then sales \n          END) AS mens_sales\nFROM retail_sales\nWHERE kind_of_business IN \n   ('Men''s clothing stores',\n    'Women''s clothing stores')\nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nsales_year\nwomens_sales\nmens_sales\n\n\n\n\n1992\n31815\n10179\n\n\n1993\n32350\n9962\n\n\n1994\n30585\n10032\n\n\n1995\n28696\n9315\n\n\n1996\n28238\n9546\n\n\n1997\n27822\n10069\n\n\n1998\n28332\n10196\n\n\n1999\n29549\n9667\n\n\n2000\n31447\n9507\n\n\n2001\n31453\n8625\n\n\n\n\n\n\npivoted_sales <-\n  retail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  mutate(kind_of_business = if_else(kind_of_business == \"Women's clothing stores\",\n                                    \"womens\", \"mens\"),\n         sales_year = year(sales_month)) %>%\n  group_by(sales_year, kind_of_business) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  pivot_wider(id_cols = \"sales_year\",\n              names_from = \"kind_of_business\",\n              names_glue = \"{kind_of_business}_{.value}\",\n              values_from = \"sales\")  \n\npivoted_sales %>%\n  show_query()\n\n<SQL>\nSELECT\n  sales_year,\n  MAX(CASE WHEN (kind_of_business = 'mens') THEN sales END) AS mens_sales,\n  MAX(CASE WHEN (kind_of_business = 'womens') THEN sales END) AS womens_sales\nFROM (\n  SELECT sales_year, kind_of_business, SUM(sales) AS sales\n  FROM (\n    SELECT\n      sales_month,\n      naics_code,\n      CASE WHEN (kind_of_business = 'Women''s clothing stores') THEN 'womens' WHEN NOT (kind_of_business = 'Women''s clothing stores') THEN 'mens' END AS kind_of_business,\n      reason_for_null,\n      sales,\n      EXTRACT(year FROM sales_month) AS sales_year\n    FROM retail_sales\n    WHERE (kind_of_business IN ('Men''s clothing stores', 'Women''s clothing stores'))\n  ) q01\n  GROUP BY sales_year, kind_of_business\n) q02\nGROUP BY sales_year\n\npivoted_sales %>%\n  arrange(sales_year) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nsales_year\nmens_sales\nwomens_sales\n\n\n\n\n1992\n10179\n31815\n\n\n1993\n9962\n32350\n\n\n1994\n10032\n30585\n\n\n1995\n9315\n28696\n\n\n1996\n9546\n28238\n\n\n1997\n10069\n27822\n\n\n1998\n10196\n28332\n\n\n1999\n9667\n29549\n\n\n2000\n9507\n31447\n\n\n2001\n8625\n31453\n\n\n\n\n\n\npivoted_sales %>%\n  filter(sales_year <= 2019) %>%\n  group_by(sales_year) %>%\n  mutate(womens_minus_mens = womens_sales - mens_sales,\n         mens_minus_womens = mens_sales - womens_sales) %>%\n  select(sales_year, womens_minus_mens, mens_minus_womens) %>%\n  ggplot(aes(y = womens_minus_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\npivoted_sales %>%\n  filter(sales_year <= 2019) %>%\n  group_by(sales_year) %>%\n  mutate(womens_times_of_mens = womens_sales / mens_sales) %>%\n  ggplot(aes(y = womens_times_of_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\npivoted_sales %>%\n  filter(sales_year <= 2019) %>%\n  group_by(sales_year) %>%\n  mutate(womens_pct_of_mens = (womens_sales / mens_sales - 1) * 100) %>%\n  ggplot(aes(y = womens_pct_of_mens, x = sales_year)) +\n  geom_line()\n\n\n\n\n\n\n3.3.3 Percent of Total Calculations\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  group_by(sales_month) %>%\n  mutate(total_sales = sum(sales, na.rm = TRUE))  %>%\n  ungroup() %>%\n  mutate(pct_total_sales = sales * 100 / total_sales) %>%\n  select(sales_month, kind_of_business, pct_total_sales) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nsales_month\nkind_of_business\npct_total_sales\n\n\n\n\n1992-06-01\nMen’s clothing stores\n26.02991\n\n\n1992-06-01\nWomen’s clothing stores\n73.97009\n\n\n1992-08-01\nMen’s clothing stores\n22.62667\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  group_by(sales_month) %>%\n  mutate(total_sales = sum(sales, na.rm = TRUE)) %>%\n  ungroup() %>%\n  mutate(pct_total_sales = sales * 100 / total_sales) %>%\n  show_query()\n\n<SQL>\nSELECT *, (sales * 100.0) / total_sales AS pct_total_sales\nFROM (\n  SELECT *, SUM(sales) OVER (PARTITION BY sales_month) AS total_sales\n  FROM retail_sales\n  WHERE (kind_of_business IN ('Men''s clothing stores', 'Women''s clothing stores'))\n) q01\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% \n           c(\"Men's clothing stores\",\n             \"Women's clothing stores\")) %>%\n  group_by(sales_month) %>%\n  mutate(total_sales = sum(sales, na.rm = TRUE)) %>%\n  ungroup() %>%\n  mutate(pct_total_sales = sales * 100 / total_sales) %>%\n  ggplot(aes(y = pct_total_sales, x = sales_month, color = kind_of_business)) +\n  geom_line()\n\n\n\n\n\n\n3.3.4 Indexing to See Percent Change over Time\n\nretail_sales %>%\n  filter(kind_of_business == \"Women's clothing stores\") %>%\n  mutate(sales_year = year(sales_month)) %>%\n  group_by(sales_year) %>%\n  summarize(sales = sum(sales, na.rm = TRUE)) %>%\n  ungroup() %>%\n  window_order(sales_year) %>%\n  mutate(index_sales = first(sales),\n         pct_from_index = (sales/index_sales - 1) * 100) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nsales_year\nsales\nindex_sales\npct_from_index\n\n\n\n\n1992\n31815\n31815\n0.000000\n\n\n1993\n32350\n31815\n1.681597\n\n\n1994\n30585\n31815\n-3.866101\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business %in% c(\"Women's clothing stores\",\n                                 \"Men's clothing stores\"),\n         sales_month <= '2019-12-31') %>%\n  mutate(sales_year = year(sales_month)) %>%\n  group_by(kind_of_business, sales_year) %>%\n  summarize(sales = sum(sales, na.rm = TRUE), .groups = \"drop\") %>%\n  group_by(kind_of_business) %>%\n  window_order(sales_year) %>%\n  mutate(index_sales = first(sales),\n         pct_from_index = (sales/index_sales - 1) * 100) %>%\n  ungroup() %>%\n  ggplot(aes(y = pct_from_index, x = sales_year, color = kind_of_business)) +\n  geom_line()"
  },
  {
    "objectID": "chapter_3.html#rolling-time-windows",
    "href": "chapter_3.html#rolling-time-windows",
    "title": "3  Time Series Analysis",
    "section": "3.4 Rolling Time Windows",
    "text": "3.4 Rolling Time Windows\n\n3.4.1 Calculating Rolling Time Windows\n\nmvg_avg <-\n  retail_sales %>%\n  filter(kind_of_business == \"Women's clothing stores\") %>%\n  window_order(sales_month) %>%\n  window_frame(-11, 0) %>%\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n()) %>%\n  filter(sales_month >= '1993-01-01') \n\nmvg_avg %>%\n  select(sales_month, moving_avg, records_count) %>%\n  collect(n = 3) %>%\n  kable(digits = 2)\n\n\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n1993-01-01\n2672.08\n12\n\n\n1993-02-01\n2673.25\n12\n\n\n1993-03-01\n2676.50\n12\n\n\n\n\n\n\nmvg_avg %>%\n  ggplot(aes(x = sales_month)) +\n  geom_line(aes(y = sales, colour = \"Sales\")) +\n  geom_line(aes(y = moving_avg, colour = \"Moving average\")) \n\n\n\n\n\nSELECT \n  sales_month,\n  avg(sales) over w AS moving_avg,\n  count(sales) over w AS records_count\nFROM retail_sales\nWHERE kind_of_business = 'Women''s clothing stores'\nWINDOW w AS (order by sales_month \n             rows between 11 preceding and current row)\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n1992-01-01\n1873.000\n1\n\n\n1992-02-01\n1932.000\n2\n\n\n1992-03-01\n2089.000\n3\n\n\n1992-04-01\n2233.000\n4\n\n\n1992-05-01\n2336.800\n5\n\n\n1992-06-01\n2351.333\n6\n\n\n1992-07-01\n2354.429\n7\n\n\n1992-08-01\n2392.250\n8\n\n\n1992-09-01\n2410.889\n9\n\n\n1992-10-01\n2445.300\n10\n\n\n\n\n\n\nretail_sales %>%\n  filter(kind_of_business == \"Women's clothing stores\") %>%\n  window_order(sales_month) %>%\n  window_frame(-11, 0) %>%\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n()) %>%\n  select(sales_month, moving_avg, records_count) %>%\n  collect(n = 10) %>%\n  kable()\n\n\ndate_dim <-\n  tibble(date = seq(as.Date('1993-01-01'), \n                    as.Date('2020-12-01'), \n                    by = \"1 month\")) %>%\n  copy_to(db, ., overwrite = TRUE, name = \"date_dim\")\n\n\nWITH jan_jul AS (\n  SELECT sales_month, sales\n  FROM retail_sales \n  WHERE kind_of_business = 'Women''s clothing stores'\n     AND date_part('month', sales_month) IN (1, 7))\n     \nSELECT a.date, b.sales_month, b.sales\nFROM date_dim a\nINNER JOIN jan_jul b \nON b.sales_month BETWEEN a.date - interval '11 months' AND a.date\nWHERE a.date BETWEEN '1993-01-01' AND '2020-12-01';\n\n\nDisplaying records 1 - 10\n\n\ndate\nsales_month\nsales\n\n\n\n\n1993-01-01\n1992-07-01\n2373\n\n\n1993-02-01\n1992-07-01\n2373\n\n\n1993-03-01\n1992-07-01\n2373\n\n\n1993-04-01\n1992-07-01\n2373\n\n\n1993-05-01\n1992-07-01\n2373\n\n\n1993-06-01\n1992-07-01\n2373\n\n\n1993-01-01\n1993-01-01\n2123\n\n\n1993-02-01\n1993-01-01\n2123\n\n\n1993-03-01\n1993-01-01\n2123\n\n\n1993-04-01\n1993-01-01\n2123\n\n\n\n\n\n\njan_jul <-\n  retail_sales %>%\n  filter(kind_of_business == \"Women's clothing stores\",\n         month(sales_month) %in% c(1, 7)) %>%\n  select(sales_month, sales)\n\ndate_dim %>%\n  mutate(date_start = date - months(11)) %>%\n  inner_join(jan_jul, \n             join_by(between(y$sales_month, x$date_start, x$date))) %>%\n  select(date, sales_month, sales) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\ndate\nsales_month\nsales\n\n\n\n\n1993-01-01\n1992-07-01\n2373\n\n\n1993-02-01\n1992-07-01\n2373\n\n\n1993-03-01\n1992-07-01\n2373\n\n\n\n\n\n\ndate_dim %>%\n  mutate(date_start = date - months(11)) %>%\n  inner_join(jan_jul, \n             join_by(between(y$sales_month, x$date_start, x$date))) %>%\n  group_by(date) %>%\n  summarize(moving_avg = mean(sales, na.rm = TRUE),\n            records = n()) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\ndate\nmoving_avg\nrecords\n\n\n\n\n1993-01-01\n2248\n2\n\n\n1993-02-01\n2248\n2\n\n\n1993-03-01\n2248\n2\n\n\n\n\n\n\nWITH sales_months AS (\n  SELECT distinct sales_month\n  FROM retail_sales\n  WHERE sales_month between '1993-01-01' and '2020-12-01')\n\nSELECT a.sales_month, avg(b.sales) as moving_avg\nFROM sales_months a\nJOIN retail_sales b \non b.sales_month between \n    a.sales_month - interval '11 months' and a.sales_month\n  and b.kind_of_business = 'Women''s clothing stores' \nGROUP BY 1\nORDER BY 1\nLIMIT 3;\n\n\n3 records\n\n\nsales_month\nmoving_avg\n\n\n\n\n1993-01-01\n2672.083\n\n\n1993-02-01\n2673.250\n\n\n1993-03-01\n2676.500\n\n\n\n\n\n\nsales_months <-\n  retail_sales %>%\n  filter(between(sales_month, \n                 as.Date('1993-01-01'), \n                 as.Date('2020-12-01'))) %>%\n  distinct(sales_month)\n\nsales_months %>%\n  mutate(month_start = sales_month - months(11)) %>%\n  inner_join(retail_sales, \n             join_by(between(y$sales_month, x$month_start, x$sales_month)),\n                     suffix = c(\"\", \"_y\")) %>%\n  filter(kind_of_business == \"Women's clothing stores\") %>%\n  group_by(sales_month) %>%\n  summarize(moving_avg = mean(sales, na.rm = TRUE)) %>%\n  arrange(sales_month) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nsales_month\nmoving_avg\n\n\n\n\n1993-01-01\n2672.083\n\n\n1993-02-01\n2673.250\n\n\n1993-03-01\n2676.500\n\n\n\n\n\n\n\n3.4.2 Calculating Cumulative Values\n\nSELECT sales_month, sales,\n  sum(sales) OVER w AS sales_ytd\nFROM retail_sales\nWHERE kind_of_business = 'Women''s clothing stores'\nWINDOW w AS (PARTITION BY date_part('year', sales_month) \n             ORDER BY sales_month)\nLIMIT 3;\n\n\n3 records\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n2017-01-01\n2454\n2454\n\n\n2017-02-01\n2763\n5217\n\n\n2017-03-01\n3485\n8702\n\n\n\n\n\n\nytd_sales <-\n  retail_sales %>%\n  filter(kind_of_business == \"Women's clothing stores\") %>%\n  mutate(year = year(sales_month)) %>%\n  group_by(year) %>%\n  window_order(sales_month) %>%\n  mutate(sales_ytd = cumsum(sales)) %>%\n  ungroup() %>%\n  select(sales_month, sales, sales_ytd) \n\nytd_sales %>%\n  filter(month(sales_month) %in% c(1:3, 12)) %>%\n  collect(n = 6) %>%\n  kable()\n\n\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n2017-01-01\n2454\n2454\n\n\n2019-01-01\n2511\n2511\n\n\n2017-02-01\n2763\n5217\n\n\n2019-02-01\n2680\n5191\n\n\n2017-03-01\n3485\n8702\n\n\n2019-03-01\n3585\n8776\n\n\n\n\n\n\nfactor <- 40/4.5\n\nytd_sales %>%\n  filter(year(sales_month) %in% 2016:2020) %>%\n  ggplot(aes(x = sales_month, y = sales_ytd)) +\n  geom_bar(stat = \"identity\") +\n  geom_line(aes(y = sales * factor, colour = I(\"blue\"))) +\n  scale_y_continuous(\n    \"Sales YTD\", \n    sec.axis = sec_axis(~ . / factor, name = \"Monthly Sales\")\n  )\n\n\n\n\n\nSELECT a.sales_month, a.sales,\n  sum(b.sales) AS sales_ytd\nFROM retail_sales a\nINNER JOIN retail_sales b ON \n date_part('year',a.sales_month) = date_part('year',b.sales_month)\n AND b.sales_month <= a.sales_month\n AND b.kind_of_business = 'Women''s clothing stores'\nWHERE a.kind_of_business = 'Women''s clothing stores'\nGROUP BY 1,2;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n1992-12-01\n4416\n31815\n\n\n1993-12-01\n4170\n32350\n\n\n1992-11-01\n2946\n27399\n\n\n1993-11-01\n2923\n28180\n\n\n1992-10-01\n2755\n24453\n\n\n1993-10-01\n2713\n25257\n\n\n1992-09-01\n2560\n21698\n\n\n1993-09-01\n2622\n22544\n\n\n1992-08-01\n2657\n19138\n\n\n1993-08-01\n2626\n19922\n\n\n\n\n\n\nretail_sales_yr <-\n  retail_sales %>%\n  mutate(year = year(sales_month))\n  \nretail_sales_yr %>%\n  filter(kind_of_business == \"Women's clothing stores\") %>%\n  inner_join(retail_sales_yr, \n             join_by(year, kind_of_business,\n                     sales_month >= sales_month),\n             suffix = c(\"\", \"_y\")) %>%\n  group_by(sales_month, sales) %>%\n  summarize(sales_ytd = sum(sales_y, na.rm = TRUE),\n            .groups = \"drop\") %>%\n  filter(month(sales_month) %in% c(1:3, 12)) %>%\n  collect(n = 6) %>%\n  kable()\n\n\n\n\nsales_month\nsales\nsales_ytd\n\n\n\n\n1992-02-01\n1991\n3864\n\n\n1993-02-01\n2005\n4128\n\n\n1994-02-01\n1970\n3756\n\n\n1992-03-01\n2403\n6267\n\n\n1993-03-01\n2442\n6570\n\n\n1994-03-01\n2560\n6316"
  },
  {
    "objectID": "chapter_3.html#analyzing-with-seasonality",
    "href": "chapter_3.html#analyzing-with-seasonality",
    "title": "3  Time Series Analysis",
    "section": "3.5 Analyzing with Seasonality",
    "text": "3.5 Analyzing with Seasonality\n\nretail_sales %>%\n  filter(kind_of_business %in% c(\"Jewelry stores\",\n                                 \"Book stores\",\n                                 \"Grocery stores\")) %>%\n  ggplot(aes(x = sales_month, y = sales)) +\n  geom_line() +\n  facet_wrap(vars(kind_of_business), nrow = 3, scales = \"free\")\n\n\n\n\n\n3.5.1 Period-over-Period Comparisons: YoY and MoM\n\nSELECT kind_of_business, sales_month, sales,\n  lag(sales_month) OVER w AS prev_month,\n  lag(sales) OVER w AS prev_month_sales\nFROM retail_sales\nWHERE kind_of_business = 'Book stores'\nWINDOW w AS (PARTITION BY kind_of_business ORDER BY sales_month)\n\n\nDisplaying records 1 - 10\n\n\nkind_of_business\nsales_month\nsales\nprev_month\nprev_month_sales\n\n\n\n\nBook stores\n1992-01-01\n790\nNA\nNA\n\n\nBook stores\n1992-02-01\n539\n1992-01-01\n790\n\n\nBook stores\n1992-03-01\n535\n1992-02-01\n539\n\n\nBook stores\n1992-04-01\n523\n1992-03-01\n535\n\n\nBook stores\n1992-05-01\n552\n1992-04-01\n523\n\n\nBook stores\n1992-06-01\n589\n1992-05-01\n552\n\n\nBook stores\n1992-07-01\n592\n1992-06-01\n589\n\n\nBook stores\n1992-08-01\n894\n1992-07-01\n592\n\n\nBook stores\n1992-09-01\n861\n1992-08-01\n894\n\n\nBook stores\n1992-10-01\n645\n1992-09-01\n861\n\n\n\n\n\n\nbooks_w_lag <-\n  retail_sales %>%\n  filter(kind_of_business == 'Book stores') %>%\n  group_by(kind_of_business) %>%\n  window_order(sales_month) %>%\n  mutate(prev_month = lag(sales_month),\n         prev_month_sales = lag(sales)) %>%\n  select(kind_of_business,\n         sales_month, sales,\n         prev_month, prev_month_sales)\n\nbooks_w_lag %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nkind_of_business\nsales_month\nsales\nprev_month\nprev_month_sales\n\n\n\n\nBook stores\n1992-01-01\n790\nNA\nNA\n\n\nBook stores\n1992-02-01\n539\n1992-01-01\n790\n\n\nBook stores\n1992-03-01\n535\n1992-02-01\n539\n\n\n\n\n\n\nbooks_monthly <-\n  books_w_lag %>%\n  mutate(pct_growth = (sales / prev_month_sales - 1) * 100) %>%\n  select(-prev_month, -prev_month_sales)\n\n\nbooks_yearly <- \n  retail_sales %>%\n  filter(kind_of_business == 'Book stores') %>%\n  mutate(sales_year = year(sales_month)) %>%\n  group_by(sales_year) %>%\n  summarize(yearly_sales = sum(sales, na.rm = TRUE),\n            .groups = \"drop\") %>%\n  window_order(sales_year) %>%\n  mutate(prev_year_sales = lag(yearly_sales),\n         pct_growth = (yearly_sales/prev_year_sales - 1) * 100)\n\nbooks_yearly %>%\n  collect(n = 3) %>%\n  kable(digits = 2)\n\n\n\n\nsales_year\nyearly_sales\nprev_year_sales\npct_growth\n\n\n\n\n1992\n8327\nNA\nNA\n\n\n1993\n9108\n8327\n9.38\n\n\n1994\n10107\n9108\n10.97\n\n\n\n\n\n\nbooks_monthly %>%\n  filter(!is.na(pct_growth)) %>%\n  ggplot(aes(x = sales_month, y = pct_growth)) +\n  geom_line()\n\n\n\n\n\n\n3.5.2 Period-over-Period Comparisons: Same Month Versus Last Year\n\nbooks_lagged_year_month <-\n  retail_sales %>%\n  filter(kind_of_business == 'Book stores') %>%\n  mutate(month = month(sales_month)) %>%\n  group_by(month) %>%\n  window_order(sales_month) %>%\n  mutate(prev_year_month = lag(sales_month),\n         prev_year_sales = lag(sales)) %>%\n  ungroup() %>%\n  select(sales_month, sales, prev_year_month, prev_year_sales)\n\nbooks_lagged_year_month %>%\n  filter(month(sales_month) <= 2, \n         year(sales_month) <= 1994) %>%\n  arrange(sales_month) %>%\n  collect(n = 6) %>%\n  kable()\n\n\n\n\nsales_month\nsales\nprev_year_month\nprev_year_sales\n\n\n\n\n1992-01-01\n790\nNA\nNA\n\n\n1992-02-01\n539\nNA\nNA\n\n\n1993-01-01\n998\n1992-01-01\n790\n\n\n1993-02-01\n568\n1992-02-01\n539\n\n\n1994-01-01\n1053\n1993-01-01\n998\n\n\n1994-02-01\n635\n1993-02-01\n568\n\n\n\n\n\n\nbooks_lagged_year_month %>%\n  mutate(dollar_diff = sales - prev_year_sales,\n         pct_diff = dollar_diff/prev_year_sales * 100) %>%\n  select(-prev_year_month, -prev_year_sales) %>%\n  filter(month(sales_month) == 1) %>%\n  collect(n = 3) %>%\n  kable(digits = 2)\n\n\n\n\nsales_month\nsales\ndollar_diff\npct_diff\n\n\n\n\n1992-01-01\n790\nNA\nNA\n\n\n1993-01-01\n998\n208\n26.33\n\n\n1994-01-01\n1053\n55\n5.51\n\n\n\n\n\n\nbooks_lagged_year_month %>%\n  filter(!is.na(prev_year_sales)) %>%\n  mutate(dollar_diff = sales - prev_year_sales,\n         pct_diff = dollar_diff/prev_year_sales * 100) %>%\n  select(sales_month, sales, dollar_diff, pct_diff) %>%\n  pivot_longer(-sales_month) %>%\n  collect() %>%\n  mutate(name = fct_inorder(name)) %>%\n  ggplot(aes(x = sales_month, y = value)) +\n  geom_line() +\n  facet_wrap(. ~ name, nrow = 3, scales = \"free\")\n\n\n\n\nWith PostgreSQL, we would use to_char(sales_month,'Month'); with DuckDB, the equivalent is monthname(sales_month). To use an approach that works with either backend, we draw on arguments to the lubridate function month().\n\nsales_92_94 <-\n  retail_sales %>%\n  filter(kind_of_business == 'Book stores',\n         year(sales_month) %in% 1992:1994) %>%\n  select(sales_month, sales) %>%\n  mutate(month_number = month(sales_month),\n         month_name = month(sales_month, \n                            label = TRUE, abbr = FALSE),\n         year = as.integer(year(sales_month)))\n\nsales_92_94 %>%\n  pivot_wider(id_cols = c(month_number, month_name),\n              names_from = year,\n              names_prefix = \"sales_\",\n              values_from = sales) %>%\n  kable()\n\n\n\n\nmonth_number\nmonth_name\nsales_1992\nsales_1993\nsales_1994\n\n\n\n\n1\nJanuary\n790\n998\n1053\n\n\n2\nFebruary\n539\n568\n635\n\n\n3\nMarch\n535\n602\n634\n\n\n4\nApril\n523\n583\n610\n\n\n5\nMay\n552\n612\n684\n\n\n6\nJune\n589\n618\n724\n\n\n7\nJuly\n592\n607\n678\n\n\n8\nAugust\n894\n983\n1154\n\n\n9\nSeptember\n861\n903\n1022\n\n\n10\nOctober\n645\n669\n732\n\n\n11\nNovember\n642\n692\n772\n\n\n12\nDecember\n1165\n1273\n1409\n\n\n\n\n\n\nsales_92_94 %>%\n  collect() %>%\n  mutate(year = factor(year),\n         month_name = month(month_number, label = TRUE)) %>%\n  ggplot(aes(x = month_name, y = sales, \n             group = year, colour = year)) +\n  geom_line()\n\n\n\n\n\n\n3.5.3 Comparing to Multiple Prior Periods\n\nprev_three <-\n  retail_sales %>%\n  filter(kind_of_business == 'Book stores') %>%\n  mutate(month = month(sales_month)) %>%\n  group_by(month) %>%\n  window_order(sales_month) %>%\n  mutate(prev_sales_1 = lag(sales, 1),\n         prev_sales_2 = lag(sales, 2),\n         prev_sales_3 = lag(sales, 3)) %>%\n  ungroup()\n\nprev_three %>%\n  filter(month == 1) %>%\n  select(sales_month, sales, starts_with(\"prev_sales\")) %>%\n  collect(n = 5) %>%\n  kable()\n\n\n\n\nsales_month\nsales\nprev_sales_1\nprev_sales_2\nprev_sales_3\n\n\n\n\n1992-01-01\n790\nNA\nNA\nNA\n\n\n1993-01-01\n998\n790\nNA\nNA\n\n\n1994-01-01\n1053\n998\n790\nNA\n\n\n1995-01-01\n1308\n1053\n998\n790\n\n\n1996-01-01\n1373\n1308\n1053\n998\n\n\n\n\n\n\nprev_three %>%\n  mutate(avg_prev_three = (prev_sales_1 + \n                         prev_sales_2 + \n                         prev_sales_3)/3,\n         pct_of_prev_3 = 100 * sales/avg_prev_three) %>%\n  select(sales_month, sales, pct_of_prev_3) %>%\n  filter(month(sales_month) == 1,\n         year(sales_month) %in% c(1995:1997, 2017:2019)) %>%\n  collect(n = 10) %>%\n  kable(digits = 2)\n\n\n\n\nsales_month\nsales\npct_of_prev_3\n\n\n\n\n1995-01-01\n1308\n138.12\n\n\n1996-01-01\n1373\n122.63\n\n\n1997-01-01\n1558\n125.17\n\n\n2017-01-01\n1386\n94.63\n\n\n2018-01-01\n1217\n84.95\n\n\n2019-01-01\n1004\n74.74\n\n\n\n\n\n\nprev_three_win <-\n  retail_sales %>%\n  filter(kind_of_business == 'Book stores') %>%\n  mutate(month = month(sales_month)) %>%\n  group_by(month) %>%\n  window_order(sales_month) %>%\n  window_frame(-3, -1) %>%\n  mutate(avg_prev_three = mean(sales, na.rm = TRUE)) %>%\n  ungroup() %>%\n  mutate(pct_of_prev_3 = 100 * sales/avg_prev_three)\n\n\nprev_three_win %>%\n  select(sales_month, sales, pct_of_prev_3) %>%\n  filter(month(sales_month) == 1,\n         year(sales_month) %in% c(1995:1997, 2017:2019)) %>%\n  collect(n = 10) %>%\n  kable(digits = 2)\n\n\n\n\nsales_month\nsales\npct_of_prev_3\n\n\n\n\n1995-01-01\n1308\n138.12\n\n\n1996-01-01\n1373\n122.63\n\n\n1997-01-01\n1558\n125.17\n\n\n2017-01-01\n1386\n94.63\n\n\n2018-01-01\n1217\n84.95\n\n\n2019-01-01\n1004\n74.74"
  },
  {
    "objectID": "chapter_3.html#persistent-storage",
    "href": "chapter_3.html#persistent-storage",
    "title": "3  Time Series Analysis",
    "section": "3.6 Persistent storage",
    "text": "3.6 Persistent storage\nHaving created a database connection, we can write the local data frame to the database using (say) copy_to().\nWe could specify temporary = FALSE if we wanted the data to be there permanently.3\n\n3.6.1 Using PostgreSQL\n\n\n3.6.2 Read-only databases\nIn some cases, you will have access to a database, but no write privileges for that database. In such a case, copy_inline() can be useful.4 Note that it seems you cannot interrogate a table created using copy_inline() using SQL, though it will behave in most respects just like a table created using copy_to() when using dbplyr. It is useful to note that copy_inline() is probably not a good solution if your data are hundreds of thousands of rows or more because the table is effectively turned into literal SQL.\n\nretail_sales_alt <- copy_inline(db, retail_sales_local)\n\n\n\n3.6.3 Closing the database connection\n\ndbDisconnect(db, shutdown=TRUE)\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_4.html",
    "href": "chapter_4.html",
    "title": "4  Cohorts",
    "section": "",
    "text": "Chapter 4 examines the fascinating topic of cohorts, where a cohort is a group of observations (often people) who acquire a shared characteristic at (approximately) the same time. For example, children entering kindergarten in New Zealand in 2017 or the Harvard MBA Class of 2002.\nWhile cohort analysis has some attractive features, I guess that data do not often come in a format that facilitates such analysis. Instead, as is the case with the legislators data set studied in Chapter 4, the data analyst needs to rearrange the data to support cohort analysis.\nI found Chapter 4 a little confusing on a first pass through it.1 The chapter launches into some SQL code intended to create cohorts, but it’s a little unclear why we’re doing what we’re doing, and we quickly see that our cohort analysis does not make sense (e.g., we have more of our original cohort in period 5 than we had in period 4) and we must have done something wrong. I think I see the idea Cathy is going for here: one needs to think carefully about how to arrange the data to avoid subtle mistakes. The challenge I see is that it’s not obvious that everyone would make the same mistake and one is too deep in the weeds of the code to really see the forest for the trees.\nSo before I launch into the code, I will spend a little time thinking about things conceptually. I will start with a different example from that used in Chapter 4, but one that I think brings out some of the issues.\nFor some reason I associate cohort analysis with life expectancy. The people who are born today form a cohort and one might ask: How long do we expect members of this cohort to live? One often hears life expectancy statistics quoted as something like: “In Australia, a boy born in 2018–2020 can expect to live to the age of 81.2 years and a girl would be expected to live to 85.3 years compared to 51.1 for boys and 54.8 years for girls born in in 1891–1900.”2\nThe people who construct the life expectancies for the children must be veritable polymaths. They need to anticipate future developments in medical care and technology. Skin cancer is a significant cause of death in Australia, due to a mismatch between the median complexion and the intensity of the sun. But the analysts calculating life expectancy need to think about how medical technology is likely to affect rates of death from carcinoma in the future. I can imagine whole-body scanners a bit like the scanners in US airports that detect skin cancers before they become problematic. These analysts also need to understand how road safety will evolve. Will children today all be in driverless vehicles in fifty years time and will accidents then be a rarity? And what about war? The data analyst needs to be able to forecast the possibility of World War III breaking out and shortening life spans. Who are these people?\nOf course it seems unlikely these über-analysts exist. Rather they surely do something more prosaic. Here is my guess as to how life expectancies are constructed.3 I guess that the data analyst gathers data on cohorts notionally formed at some point in the past and then looks at survival rates for that cohort over some period, then aggregates those data into a life expectancy.\nFor example, the data analyst might gather data on people who turned 21 in 2018 and then data on whether those people surived to their 22nd birthday. The proportion of such people who make their 22nd birthday could be interpreted as a survival probability \\(p_{21}\\). Repeat that for each age-based cohort to get probabilities \\(\\left\\{p_i: i = 0, 1, \\dots, 119, 120 \\right\\}\\). Now to find the median life expectancy, we could calculate something like this:4\n\\[ \\left\\{j: \\arg \\min_{i} \\left(\\prod_{0}^{i} p_i\\right) \\leq \\frac{1}{2} \\right\\} \\] So we have a (fairly) well-defined procedure here. There are obviously some details to be worked out. For example, do we focus on one year (2018 in this case)? Or collect data over multiple years? Does it make sense to form cohorts by years? Or would grouping into larger cohorts (e.g., 20–25) make more sense? Do we identify people by birthdays? Or just use some kind of census date? (People who are 21 on 1 July might have just turned 21, or might be about to turn 22.)\nBut what exactly have we calculated? In a sense it’s a nonsensical number. Why would survival rates for 88-year-olds in 2018 be relevant for the life expectancy of newborns today, who will face a very different world when they turn 88 in 2111. First, perhaps the analysts really don’t calculate it in this way (though I’m doubtful they are polymaths). Second, even though it’s a “meaningless” number, it probably still has attractive properties, such as the ability to represent in a one or two numbers a lot about the quality of life in Australia.\nA final note is that it is not clear to me where the “51.1 for boys and 54.8 years for girls born in in 1891–1900” values come from. Are these the equivalent life expectancies calculated using data available around 1900? Or are these the observed lifespans of people born in 1891–1900? If the latter, how accurate were the former as estimates of these values?"
  },
  {
    "objectID": "chapter_4.html#the-legislators-data-set",
    "href": "chapter_4.html#the-legislators-data-set",
    "title": "4  Cohorts",
    "section": "4.2 The Legislators Data Set",
    "text": "4.2 The Legislators Data Set\nNow that we understand cohorts, let’s move onto the legislators data set.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(duckdb)\n\nIf you have the tables legislators and legislators_terms in a PostgreSQL database, then you could connect to that database.\n\ndb <- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)\n\nOtherwise, the easiest way to get the legislators data set is to get the data from the GitHub site provided with Tanimura (2021).\n\nurl <- paste0(\"https://raw.githubusercontent.com/cathytanimura/\",\n              \"sql_book/master/Chapter%204%3A%20Cohorts/\")\nlegislators_df <- read_csv(paste0(url, \"legislators.csv\"),\n                           show_col_types = FALSE)\nlegislators_terms_df <- read_csv(paste0(url, \"legislators_terms.csv\"),\n                                 show_col_types = FALSE)\n\n\ndb <- dbConnect(duckdb())\nlegislators <- \n  copy_to(db, legislators_df, name = \"legislators\")\nlegislators_terms <- \n  copy_to(db, legislators_terms_df, name = \"legislators_terms\")\n\n\nSELECT id_bioguide, min(term_start) AS first_term\nFROM legislators_terms \nGROUP BY 1\nORDER BY 1\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\n\n\n\n\nA000001\n1951-01-03\n\n\nA000002\n1947-01-03\n\n\nA000003\n1817-12-01\n\n\nA000004\n1843-12-04\n\n\nA000005\n1887-12-05\n\n\nA000006\n1868-01-01\n\n\nA000007\n1875-12-06\n\n\nA000008\n1857-12-07\n\n\nA000009\n1973-01-03\n\n\nA000010\n1954-01-01\n\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1) \n\nSELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n  COUNT(DISTINCT a.id_bioguide) AS cohort_retained\nFROM first_terms a\nINNER JOIN legislators_terms b\nUSING (id_bioguide)\nGROUP BY 1\nORDER BY 1\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n3600\n\n\n2\n3619\n\n\n3\n1831\n\n\n4\n3210\n\n\n5\n1744\n\n\n6\n2385\n\n\n7\n1360\n\n\n8\n1607\n\n\n9\n1028\n\n\n\n\n\n\nfirst_terms <- \n  legislators_terms %>%\n  group_by(id_bioguide) %>%\n  summarize(first_term = min(term_start, na.rm = TRUE))\n\ncohorts <-\n  legislators_terms %>%\n  inner_join(first_terms, by = \"id_bioguide\") %>%\n  mutate(period = date_part('year', age(term_start, first_term))) %>%\n  group_by(period) %>%\n  summarize(cohort_retained = sql(\"count(distinct id_bioguide)\")) \n\ncohorts %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nperiod\ncohort_retained\n\n\n\n\n1\n3600\n\n\n2\n3619\n\n\n6\n2385\n\n\n5\n1744\n\n\n10\n1398\n\n\n4\n3210\n\n\n3\n1831\n\n\n14\n715\n\n\n12\n1185\n\n\n13\n859\n\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts AS (\n  SELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n    COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b \n  USING (id_bioguide)\n  GROUP BY 1)\n  \nSELECT period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / \n    first_value(cohort_retained) OVER w AS prop_retained\nFROM cohorts\nWINDOW w AS (ORDER BY period);\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_size\ncohort_retained\nprop_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n3600\n0.2875859\n\n\n2\n12518\n3619\n0.2891037\n\n\n3\n12518\n1831\n0.1462694\n\n\n4\n12518\n3210\n0.2564307\n\n\n5\n12518\n1744\n0.1393194\n\n\n6\n12518\n2385\n0.1905256\n\n\n7\n12518\n1360\n0.1086436\n\n\n8\n12518\n1607\n0.1283751\n\n\n9\n12518\n1028\n0.0821217\n\n\n\n\n\n\nretained_data <-\n  cohorts %>%\n  window_order(period) %>%\n  mutate(cohort_size = first(cohort_retained)) %>%\n  mutate(pct_retained = cohort_retained * 1.0/cohort_size) %>%\n  select(period, cohort_size, cohort_retained, pct_retained) \n\nretained_data %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n3600\n0.2875859\n\n\n2\n12518\n3619\n0.2891037\n\n\n3\n12518\n1831\n0.1462694\n\n\n4\n12518\n3210\n0.2564307\n\n\n5\n12518\n1744\n0.1393194\n\n\n6\n12518\n2385\n0.1905256\n\n\n7\n12518\n1360\n0.1086436\n\n\n8\n12518\n1607\n0.1283751\n\n\n9\n12518\n1028\n0.0821217\n\n\n\n\n\n\nretained_data %>%\n  ggplot(aes(x = period, y = pct_retained)) +\n  geom_line()\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts AS (\n  SELECT date_part('year', age(b.term_start, a.first_term)) AS period,\n      count(distinct a.id_bioguide) as cohort_retained\n  FROM first_terms a\n  JOIN legislators_terms b on a.id_bioguide = b.id_bioguide \n  GROUP BY 1),\n  \nretained_data AS (\n  SELECT period,\n    first_value(cohort_retained) OVER (ORDER BY period) AS cohort_size,\n    cohort_retained,\n    cohort_retained * 1.0 / first_value(cohort_retained) OVER (ORDER BY period) AS pct_retained\n  FROM cohorts)\n\nSELECT cohort_size,\n  max(case when period = 0 then pct_retained end) as yr0,\n  max(case when period = 1 then pct_retained end) as yr1,\n  max(case when period = 2 then pct_retained end) as yr2,\n  max(case when period = 3 then pct_retained end) as yr3,\n  max(case when period = 4 then pct_retained end) as yr4\nFROM retained_data\nGROUP BY 1;\n\n\n1 records\n\n\ncohort_size\nyr0\nyr1\nyr2\nyr3\nyr4\n\n\n\n\n12518\n1\n0.2875859\n0.2891037\n0.1462694\n0.2564307\n\n\n\n\n\n\nretained_data %>%\n  select(period, pct_retained) %>%\n  filter(period <= 4) %>%\n  collect() %>%\n  arrange(period) %>%\n  pivot_wider(names_from = period, \n              names_prefix = \"yr\",\n              values_from = pct_retained) %>%\n  collect() %>%\n  kable()\n\n\n\n\nyr0\nyr1\nyr2\nyr3\nyr4\n\n\n\n\n1\n0.2875859\n0.2891037\n0.1462694\n0.2564307"
  },
  {
    "objectID": "chapter_4.html#adjusting-time-series-to-increase-retention-accuracy",
    "href": "chapter_4.html#adjusting-time-series-to-increase-retention-accuracy",
    "title": "4  Cohorts",
    "section": "4.3 Adjusting Time Series to Increase Retention Accuracy",
    "text": "4.3 Adjusting Time Series to Increase Retention Accuracy\n\ndate_dim <-\n  tibble(date = seq(as.Date('1770-12-31'), \n                    as.Date('2020-12-31'), \n                    by = \"1 year\")) %>%\n  copy_to(db, ., overwrite = TRUE, name = \"date_dim\")\n\n\nWITH\n\nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT a.id_bioguide, a.first_term, b.term_start, b.term_end,\n  c.date,\n  date_part('year', age(c.date, a.first_term)) AS period\nFROM first_terms a\nJOIN legislators_terms b USING (id_bioguide)\nLEFT JOIN date_dim c ON c.date BETWEEN b.term_start and b.term_end;\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\ndate\nperiod\n\n\n\n\nT000165\n2020-05-19\n2020-05-19\n2021-01-03\n2020-12-31\n0\n\n\nG000061\n2020-05-19\n2020-05-19\n2021-01-03\n2020-12-31\n0\n\n\nM000687\n1987-01-06\n2020-05-05\n2021-01-03\n2020-12-31\n33\n\n\nM001210\n2019-09-17\n2019-09-17\n2021-01-03\n2020-12-31\n1\n\n\nM001210\n2019-09-17\n2019-09-17\n2021-01-03\n2019-12-31\n0\n\n\nB001311\n2019-09-17\n2019-09-17\n2021-01-03\n2020-12-31\n1\n\n\nB001311\n2019-09-17\n2019-09-17\n2021-01-03\n2019-12-31\n0\n\n\nK000395\n2019-06-03\n2019-06-03\n2021-01-03\n2020-12-31\n1\n\n\nK000395\n2019-06-03\n2019-06-03\n2021-01-03\n2019-12-31\n0\n\n\nS001217\n2019-01-08\n2019-01-08\n2025-01-03\n2020-12-31\n1\n\n\n\n\n\n\nyear_ends <-\n  tibble(date = seq(as.Date('1770-12-31'), \n                    as.Date('2030-12-31'), \n                    \"1 year\")) %>%\n  copy_to(db, ., names = \"year_ends\",\n          overwrite = TRUE)\n  \ncohorts <-\n  first_terms %>%\n  inner_join(legislators_terms, by = join_by(id_bioguide)) %>%\n  left_join(year_ends, \n            by = join_by(between(y$date, x$term_start, x$term_end))) %>%\n  mutate(period = date_part('year', age(date, first_term))) %>%\n  select(id_bioguide, first_term, term_start, term_end, date, period) \n\ncohorts %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\ndate\nperiod\n\n\n\n\nT000165\n2020-05-19\n2020-05-19\n2021-01-03\n2020-12-31\n0\n\n\nG000061\n2020-05-19\n2020-05-19\n2021-01-03\n2020-12-31\n0\n\n\nM000687\n1987-01-06\n2020-05-05\n2021-01-03\n2020-12-31\n33\n\n\nM001210\n2019-09-17\n2019-09-17\n2021-01-03\n2020-12-31\n1\n\n\nM001210\n2019-09-17\n2019-09-17\n2021-01-03\n2019-12-31\n0\n\n\nB001311\n2019-09-17\n2019-09-17\n2021-01-03\n2020-12-31\n1\n\n\nB001311\n2019-09-17\n2019-09-17\n2021-01-03\n2019-12-31\n0\n\n\nK000395\n2019-06-03\n2019-06-03\n2021-01-03\n2020-12-31\n1\n\n\nK000395\n2019-06-03\n2019-06-03\n2021-01-03\n2019-12-31\n0\n\n\nS001217\n2019-01-08\n2019-01-08\n2025-01-03\n2024-12-31\n5\n\n\n\n\n\n\nyear_ends <- \n  tibble(date = seq(as.Date(\"1770-12-31\"), \n                    as.Date(\"2030-12-31\"), \n                    by = \"year\")) %>%\n  copy_inline(db, .)\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n  \nyear_ends AS (\n  SELECT generate_series::date as date\n  FROM generate_series('1770-12-31'::TIMESTAMP, \n                       '2030-12-31'::TIMESTAMP, \n                       interval '1 year')\n  \n)\n  \nSELECT \n  coalesce(date_part('year', age(c.date, a.first_term)), 0) AS period,\n  count(DISTINCT a.id_bioguide) AS cohort_retained\nFROM first_terms a\nJOIN legislators_terms b ON a.id_bioguide = b.id_bioguide \nLEFT JOIN year_ends c ON c.date BETWEEN b.term_start AND b.term_end \nGROUP BY 1\nORDER BY 1;\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n12328\n\n\n2\n8166\n\n\n3\n8069\n\n\n4\n5862\n\n\n5\n5795\n\n\n6\n4361\n\n\n7\n4339\n\n\n8\n3521\n\n\n9\n3485\n\n\n\n\n\n\nWITH\n\nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) as first_term\n  FROM legislators_terms \n  GROUP BY 1),\n\ncohorts_retained AS (        \n    SELECT coalesce(date_part('year', age(c.date,a.first_term)),0) AS period,\n    count(distinct a.id_bioguide) as cohort_retained\n    FROM first_terms a\n    JOIN legislators_terms b USING (id_bioguide)\n    LEFT JOIN date_dim c ON c.date BETWEEN b.term_start AND b.term_end\n    GROUP BY 1)\n    \nSELECT period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0/first_value(cohort_retained) over w as pct_retained\nFROM cohorts_retained\nWINDOW w AS (ORDER BY period);\n\n\nDisplaying records 1 - 10\n\n\nperiod\ncohort_size\ncohort_retained\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12518\n12328\n0.9848219\n\n\n2\n12518\n8162\n0.6520211\n\n\n3\n12518\n8065\n0.6442722\n\n\n4\n12518\n5853\n0.4675667\n\n\n5\n12518\n5786\n0.4622144\n\n\n6\n12518\n4360\n0.3482985\n\n\n7\n12518\n4338\n0.3465410\n\n\n8\n12518\n3513\n0.2806359\n\n\n9\n12518\n3476\n0.2776801\n\n\n\n\n\n\ncohorts_retained <-\n  cohorts %>%\n  mutate(period = coalesce(date_part('year', age(date, first_term)), 0)) %>%\n  select(period, id_bioguide) %>%\n  distinct() %>%\n  group_by(period) %>%\n  summarize(cohort_retained = n()) \n\npct_retained <-\n  cohorts_retained %>%\n  window_order(period) %>%\n  mutate(cohort_size = first(cohort_retained),\n         cohort_retained = as.double(cohort_retained),\n         pct_retained = cohort_retained/cohort_size) \n\npct_retained %>%\n  arrange(period) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nperiod\ncohort_retained\ncohort_size\npct_retained\n\n\n\n\n0\n12518\n12518\n1.0000000\n\n\n1\n12328\n12518\n0.9848219\n\n\n2\n8166\n12518\n0.6523406\n\n\n3\n8069\n12518\n0.6445918\n\n\n4\n5862\n12518\n0.4682857\n\n\n5\n5795\n12518\n0.4629334\n\n\n6\n4361\n12518\n0.3483783\n\n\n7\n4339\n12518\n0.3466209\n\n\n8\n3521\n12518\n0.2812750\n\n\n9\n3485\n12518\n0.2783991\n\n\n\n\npct_retained %>%\n  ggplot(aes(x = period, y = pct_retained)) + \n  geom_line()\n\n\n\n\n\nWITH \nfirst_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1)\n  \nSELECT id_bioguide, a.first_term, b.term_start,\n  CASE WHEN b.term_type = 'rep' THEN b.term_start + interval '2 years'\n       WHEN b.term_type = 'sen' THEN b.term_start + interval '6 years'\n  END AS term_end\nFROM first_terms a\nJOIN legislators_terms b USING (id_bioguide);\n\n\nDisplaying records 1 - 10\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\n\n\n\n\nF000062\n1992-11-10\n2019-01-03\n2025-01-03\n\n\nT000464\n2007-01-04\n2019-01-03\n2025-01-03\n\n\nA000360\n2003-01-07\n2015-01-06\n2021-01-06\n\n\nG000359\n1995-01-04\n2015-01-06\n2021-01-06\n\n\nR000307\n1981-01-05\n2015-01-06\n2021-01-06\n\n\nA000055\n1997-01-07\n2019-01-03\n2021-01-03\n\n\nT000474\n2015-01-06\n2019-01-03\n2021-01-03\n\n\nC001047\n2001-01-03\n2015-01-06\n2021-01-06\n\n\nH001052\n2011-01-05\n2019-01-03\n2021-01-03\n\n\nH001038\n2005-01-04\n2019-01-03\n2021-01-03\n\n\n\n\n\n\nfirst_terms %>%\n  inner_join(legislators_terms, by = join_by(id_bioguide)) %>%\n  mutate(term_end = \n           case_when(term_type == 'rep' ~ term_start + years(2),\n                     term_type == 'sen' ~ term_start + years(6))) %>%\n  select(id_bioguide, first_term, term_start, term_end)  %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nid_bioguide\nfirst_term\nterm_start\nterm_end\n\n\n\n\nF000062\n1992-11-10\n2019-01-03\n2025-01-03\n\n\nT000464\n2007-01-04\n2019-01-03\n2025-01-03\n\n\nA000360\n2003-01-07\n2015-01-06\n2021-01-06\n\n\nG000359\n1995-01-04\n2015-01-06\n2021-01-06\n\n\nR000307\n1981-01-05\n2015-01-06\n2021-01-06\n\n\nA000055\n1997-01-07\n2019-01-03\n2021-01-03\n\n\nT000474\n2015-01-06\n2019-01-03\n2021-01-03\n\n\nC001047\n2001-01-03\n2015-01-06\n2021-01-06\n\n\nH001052\n2011-01-05\n2019-01-03\n2021-01-03\n\n\nH001038\n2005-01-04\n2019-01-03\n2021-01-03\n\n\n\n\n\n\nWITH first_terms AS (\n  SELECT id_bioguide, min(term_start) AS first_term\n  FROM legislators_terms \n  GROUP BY 1),\n                \nfirst_centuries AS (\n  SELECT \n    date_part('century', a.first_term) AS first_century,\n    coalesce(date_part('year', age(c.date, a.first_term)), 0) AS period,\n    COUNT(DISTINCT a.id_bioguide) AS cohort_retained\n  FROM first_terms AS a\n  INNER JOIN legislators_terms b \n  USING (id_bioguide)\n  LEFT JOIN date_dim c \n  ON c.date BETWEEN b.term_start AND b.term_end \n  GROUP BY 1,2)\n              \nSELECT first_century, period,\n  first_value(cohort_retained) OVER w AS cohort_size,\n  cohort_retained,\n  cohort_retained * 1.0 / first_value(cohort_retained) OVER w AS prop_retained\nFROM first_centuries AS aa\nWINDOW w AS (PARTITION BY first_century ORDER BY period)\nORDER BY 1, 2;\n\n\nDisplaying records 1 - 10\n\n\nfirst_century\nperiod\ncohort_size\ncohort_retained\nprop_retained\n\n\n\n\n18\n0\n368\n368\n1.0000000\n\n\n18\n1\n368\n360\n0.9782609\n\n\n18\n2\n368\n242\n0.6576087\n\n\n18\n3\n368\n233\n0.6331522\n\n\n18\n4\n368\n149\n0.4048913\n\n\n18\n5\n368\n144\n0.3913043\n\n\n18\n6\n368\n99\n0.2690217\n\n\n18\n7\n368\n101\n0.2744565\n\n\n18\n8\n368\n73\n0.1983696\n\n\n18\n9\n368\n70\n0.1902174\n\n\n\n\n\n\ndbDisconnect(db, shutdown = TRUE)\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_5.html",
    "href": "chapter_5.html",
    "title": "5  Text Analysis",
    "section": "",
    "text": "I would actually reframe this question as “why store text data in a database?” and offer different reasons from those offered in Tanimura (2021). To structure my answer I will use a representative textual analysis problem (really set of problems) I’ve managed in the past.\nPublic companies routinely communicate with investors or their representatives through conference calls. Most public companies hold conference calls when they announce their earnings results for a quarter or year. The typical earnings conference call starts with a presentation of results by management, typically the CEO or CEO, followed by the “Q&A” portion of the call during which call participants can ask questions of management Apart from representatives of the company, the typical participant in a conference call is an equity analyst. Equity analysts typically cover a relatively small numbers of companies, typically in a single industry, and provide insights and investment recommendations and related to their covered companies and industries.\nAnalyst recommendations usually come from valuation analyses that draw on projections of future financial performance. An analyst’s valuation model is usually constructed using a spreadsheet and to some extent an analyst’s questions on a conference call will seek information that can be used for model inputs.\nTranscripts of conference calls are collected by a number of data providers, who presumably supply them to various users, including investors and academic researchers. I have used transcripts of conference calls in my own research. The data provider in my case provided a continuous stream of transcripts in XML files. Each call is contained in its own XML file with a file name that indicates the unique identifier of the call. Some elements of the call are contained in structured XML, but the bulk of the data in a call are in a single unstructured XML field.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\n\n\npg <- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)\n\n\nufo <- tbl(pg, \"ufo\")\n\nufo %>%\n  mutate(length = length(sighting_report)) %>%\n  ggplot(aes(x = length)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\nufo <- \n  tbl(pg, \"ufo\") %>% \n  mutate(id = row_number())\n\nregex <- paste0(\"Occurred :\\\\s*(.*)\\\\s*\",\n                \"Reported:\\\\s*(.* [AP]M).*?\\\\s*\",\n                \"Posted:\\\\s*(.*)\\\\s*\",\n                \"Location:\\\\s*(.*)\\\\s*\",\n                \"Shape:\\\\s*(.*)\\\\s*\",\n                \"Duration:\\\\s*(.*)\\\\s*\")\n\nregex2 <- paste0(\"^(.*?)\",\n                 \"(?:\\\\s*\\\\(Entered as :\\\\s*(.*)\\\\))?\\\\s*$\")\n\nufo_extracted <- \n  ufo %>%\n  mutate(matches = regexp_matches(sighting_report, regex)) %>%\n  mutate(occurred_plus = matches[[1]],\n         reported = matches[[2]],\n         posted = matches[[3]],\n         location = matches[[4]],\n         shape = matches[[5]],\n         duration = matches[[6]]) %>%\n  select(id, occurred_plus:duration) %>%\n  mutate(occurred_plus = regexp_matches(occurred_plus, regex2)) %>%\n  mutate(occurred_raw = occurred_plus[[1]],\n         entered = occurred_plus[[2]]) %>%\n  select(-occurred_plus) %>%\n  mutate(location_clean = regexp_replace(location, \n                                         \"(outside of|close to)\", \"near\")) %>%\n  mutate(occurred = case_when(occurred_raw == '' ~ NA,\n                              length(occurred_raw) < 8 ~ NA,\n                              TRUE ~ as.POSIXct(occurred_raw)),\n         reported = case_when(reported == '' ~ NA,\n                              length(reported) < 8 ~ NA,\n                              TRUE ~ as.POSIXct(reported)),\n         posted = if_else(posted == '', NA, as.Date(posted)),\n         shape = initcap(shape)) %>%\n  collect()\n\n\nufo_extracted %>%\n  ggplot(aes(y = fct_rev(fct_infreq(shape)))) +\n  geom_bar() +\n  ylab(\"Shape\")\n\n\n\n\n\nufo_extracted %>%\n  filter(!is.na(occurred_raw)) %>%\n  count(occurred_raw) %>%\n  arrange(desc(n)) %>%\n  slice_head(n = 10) %>%\n  mutate(occurred_raw = fct_rev(fct_inorder(as.character(occurred_raw)))) %>%\n  ggplot(aes(y = occurred_raw, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nufo_extracted %>%\n  count(duration) %>%\n  arrange(desc(n)) %>%\n  slice_head(n = 10) %>%\n  mutate(duration = fct_rev(fct_inorder(duration))) %>%\n  ggplot(aes(y = duration, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nufo_extracted %>%\n  count(location) %>%\n  arrange(desc(n)) %>%\n  slice_head(n = 10) %>%\n  mutate(location = fct_rev(fct_inorder(location))) %>%\n  ggplot(aes(y = location, x = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_6.html",
    "href": "chapter_6.html",
    "title": "6  Anomaly Detection",
    "section": "",
    "text": "library(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\nlibrary(duckdb)\nThe ntile() offered by dbplyr works a little differently from other window functions. Rather than preceding the mutate with a window_order(), we need to specify an order_by argument.1\nBelow Cathy uses INNER JOIN with ON 1 = 1.\nInstead, I use CROSS JOIN (this is cross_join in dplyr).\nThe output in the book differs from one gets from running the code, so I add !(mag %in% c(-9, -9.99)) to get closer to the book’s output.\nNote that in constructing mag_stats, I follow the book in using avg(mag) and stddev_pop(mag). In practice, I would probably lean more to using R-compatible mean(mag, na.rm = TRUE) and sd(mag, na.rm = TRUE), respectively. This makes little differ in practice—the only difference is that sd is translated into stddev_samp instead of stddev_pop, which is barely different in this case—but I believe it is helpful to be consistent where possible. Often I find myself moving the data processing from PosrtgreSQL to R or vice versa and this is much easier if the dbplyr code is consistent with the dplyr equivalent."
  },
  {
    "objectID": "chapter_6.html#graphing-to-find-anomalies-visually",
    "href": "chapter_6.html#graphing-to-find-anomalies-visually",
    "title": "6  Anomaly Detection",
    "section": "6.1 Graphing to find anomalies visually",
    "text": "6.1 Graphing to find anomalies visually\n\nearthquakes %>%\n  filter(!is.na(mag)) %>%\n  ggplot(aes(x = mag)) +\n  geom_histogram(breaks = seq(-10, 10, 0.1))\n\n\n\n\n\nearthquakes %>%\n  filter(!is.na(mag),\n         between(mag, 7.2, 9.5)) %>%\n  ggplot(aes(x = mag)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\nearthquakes %>%\n  filter(!is.na(mag),\n         between(mag, 7.2, 9.5)) %>%\n  ggplot(aes(x = mag)) +\n  geom_bar() +\n  scale_x_binned(breaks = seq(7.2, 9.5, 0.1))\n\n\n\n\n\nearthquakes %>%\n  filter(!is.na(mag), !is.na(depth)) %>%\n  distinct(mag, depth) %>%\n  ggplot(aes(x = mag, y = depth)) +\n  geom_point(size = 0.1, colour = \"blue\")\n\n\n\n\n\nearthquakes %>%\n  filter(!is.na(mag), !is.na(depth)) %>%\n  filter(between(mag, 4, 7), depth <= 50) %>%\n  ggplot(aes(x = mag, y = depth)) +\n  geom_count(color = \"blue\")\n\n\n\n\n\njapan_quakes <-\n  earthquakes %>%\n  filter(!is.na(mag), !is.na(depth)) %>%\n  filter(grepl(\"Japan\", place)) \n\njapan_quakes %>%\n  ggplot(aes(y = mag)) +\n  geom_boxplot(width = 0.5)\n\n\n\n\n\njapan_quakes %>%\n  summarize(p25 = quantile(mag, probs = 0.25, na.rm = TRUE),\n            p50 = quantile(mag, probs = 0.50, na.rm = TRUE),\n            p75 = quantile(mag, probs = 0.75, na.rm = TRUE)) %>%\n  mutate(iqr = (p75 - p25) * 1.5,\n         lower_whisker = p25 - (p75 - p25) * 1.5,\n         upper_whisker = p75 + (p75 - p25) * 1.5) %>%\n  kable()\n\n\n\n\np25\np50\np75\niqr\nlower_whisker\nupper_whisker\n\n\n\n\n4.2\n4.4\n4.6\n0.6\n3.6\n5.2\n\n\n\n\n\n\njapan_quakes %>%\n  select(mag, time) %>%\n  collect() %>%\n  mutate(year = as.factor(year(time))) %>%\n  ggplot(aes(y = mag, x = year, group = year)) +\n  geom_boxplot()"
  },
  {
    "objectID": "chapter_6.html#forms-of-anomalies",
    "href": "chapter_6.html#forms-of-anomalies",
    "title": "6  Anomaly Detection",
    "section": "6.2 Forms of Anomalies",
    "text": "6.2 Forms of Anomalies\n\n6.2.1 Anomalous Values\n\nearthquakes %>%\n  filter(mag >= 1.08) %>%\n  group_by(mag) %>%\n  summarize(count = n()) %>%\n  arrange(mag) %>%\n  collect(n = 5) %>%\n  kable()\n\n\n\n\nmag\ncount\n\n\n\n\n1.08\n319\n\n\n1.09\n318\n\n\n1.10\n1918\n\n\n1.11\n314\n\n\n1.12\n358\n\n\n\n\n\n\nearthquakes %>%\n  filter(depth > 600) %>%\n  group_by(net) %>%\n  summarize(count = n()) %>%\n  arrange(net) %>%\n  collect(n = 5) %>%\n  kable()\n\n\n\n\nnet\ncount\n\n\n\n\nus\n44\n\n\n\n\n\n\nearthquakes %>%\n  filter(depth > 600) %>%\n  group_by(place) %>%\n  summarize(count = n()) %>%\n  arrange(place) %>%\n  collect(n = 5) %>%\n  kable()\n\n\n\n\nplace\ncount\n\n\n\n\nBanda Sea\n1\n\n\nFiji region\n10\n\n\nMindanao, Philippines\n1\n\n\nMoro Gulf, Mindanao, Philippines\n24\n\n\nNew Britain region, Papua New Guinea\n1\n\n\n\n\n\n\nearthquakes %>%\n  filter(depth > 600) %>%\n  mutate(place_name = case_when(grepl(' of ', place) ~\n                                  split_part(place, ' of ', 2L),\n                                TRUE ~ place)) %>%\n  group_by(place_name) %>%\n  summarize(count = n()) %>%\n  arrange(desc(count)) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nplace_name\ncount\n\n\n\n\nMoro Gulf, Mindanao, Philippines\n24\n\n\nFiji region\n10\n\n\nthe Fiji Islands\n4\n\n\n\n\n\n\nearthquakes %>%\n  summarize(distinct_types = n_distinct(type),\n            distinct_lower = n_distinct(lower(type))) %>%\n  kable()\n\n\n\n\ndistinct_types\ndistinct_lower\n\n\n\n\n9\n9\n\n\n\n\n\n\n\n6.2.2 Anomalous Counts or Frequencies\nWhy use date_trunc('year',time)::date as earthquake_year?\n\nSELECT EXTRACT(year FROM time) AS earthquake_year, \n  COUNT(*) AS earthquakes\nFROM earthquakes\nGROUP BY 1\nORDER BY 1;\n\n\n1 records\n\n\nearthquake_year\nearthquakes\n\n\n\n\n2010\n1e+05\n\n\n\n\n\n\nSELECT *\nFROM earthquakes\nLIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime\nlatitude\nlongitude\ndepth\nmag\nmagtype\nnst\ngap\ndmin\nrms\nnet\nid\nupdated\nplace\ntype\nhorizontalerror\ndeptherror\nmagerror\nmagnst\nstatus\nlocationsource\nmagsource\n\n\n\n\n2010-01-01 00:10:43\n61.03220\n-151.6587\n88.400\n1.60\nml\nNA\nNA\nNA\n0.77\nak\nak0101nll17\n2018-07-06 19:44:32\nSouthern Alaska\nearthquake\nNA\n0.00\nNA\nNA\nreviewed\nak\nak\n\n\n2010-01-01 00:16:49\n48.20317\n-121.6768\n2.948\n2.20\nmd\n16\n79\n0.16790\n0.22\nuw\nuw10782553\n2016-09-01 18:42:22\nWashington\nearthquake\n1.004\n12.60\n0.070\n9\nreviewed\nuw\nuw\n\n\n2010-01-01 00:16:54\n36.03200\n-117.7822\n1.103\n1.10\nml\n19\n71\n0.01280\n0.22\nci\nci14566836\n2016-03-10 22:55:21\n15km E of Coso Junction, CA\nearthquake\n0.400\n0.62\n0.164\n15\nreviewed\nci\nci\n\n\n2010-01-01 00:29:29\n60.14700\n-141.0550\n0.000\n1.40\nml\nNA\nNA\nNA\n0.51\nak\nak0101npmji\n2018-07-06 19:44:32\nSouthern Alaska\nice quake\nNA\n0.00\nNA\nNA\nreviewed\nak\nak\n\n\n2010-01-01 00:30:16\n33.47467\n-116.4268\n4.375\n0.75\nml\n13\n210\n0.12000\n0.11\nci\nci14566844\n2016-03-10 11:39:44\n24km SSW of La Quinta, CA\nearthquake\n0.560\n31.61\n0.146\n19\nreviewed\nci\nci\n\n\n2010-01-01 00:31:40\n63.19860\n-149.6839\n86.700\n1.00\nml\nNA\nNA\nNA\n0.36\nak\nak0101nq2s7\n2018-07-06 19:44:42\nCentral Alaska\nearthquake\nNA\n0.20\nNA\nNA\nreviewed\nak\nak\n\n\n2010-01-01 00:34:29\n36.00400\n-117.7892\n1.991\n0.46\nml\n8\n118\n0.02675\n0.11\nci\nci14566852\n2016-03-10 05:47:44\n13km NE of Little Lake, CA\nearthquake\n0.490\n1.12\n0.201\n6\nreviewed\nci\nci\n\n\n2010-01-01 00:55:05\n53.92600\n-163.9887\n22.400\n2.40\nml\nNA\nNA\nNA\n0.31\nak\nak0101nv5aw\n2018-07-06 19:44:32\nUnimak Island region, Alaska\nearthquake\nNA\n0.10\nNA\nNA\nreviewed\nak\nak\n\n\n2010-01-01 00:57:31\n63.25890\n-151.1658\n6.200\n0.60\nml\nNA\nNA\nNA\n0.19\nak\nak0101nvmrg\n2018-07-06 19:44:43\nCentral Alaska\nearthquake\nNA\n0.20\nNA\nNA\nreviewed\nak\nak\n\n\n2010-01-01 01:09:53\n38.75517\n-122.7172\n1.604\n0.20\nmd\n9\n67\n0.01532\n0.08\nnc\nnc71328690\n2017-01-21 16:40:03\nNorthern California\nearthquake\n0.500\n1.19\n0.175\n4\nreviewed\nnc\nnc\n\n\n\n\n\n\nearthquakes %>%\n  mutate(earthquake_year = as.character(year(time))) %>%\n  group_by(earthquake_year) %>%\n  summarize(earthquakes = n()) %>%\n  ggplot(aes(x = earthquake_year, y = earthquakes)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n2\n\nearthquakes %>%\n  mutate(earthquake_year = as.character(year(time))) %>%\n  select(earthquake_year) %>%\n  ggplot(aes(x = earthquake_year)) +\n  geom_bar()\n\n\nearthquakes %>%\n  mutate(earthquake_month = floor_date(time, \"month\")) %>%\n  group_by(earthquake_month) %>%\n  summarize(earthquakes = n(), .groups = \"drop\") %>%\n  ggplot(aes(x = earthquake_month, y = earthquakes)) +\n  geom_line()\n\n\n\n\nFrom the book, “it turns out that the increase in earthquakes starting in 2017 can be at least partially explained by the status field. The status indicates whether the event has been reviewed by a human (‘reviewed’) or was directly posted by a system without review (‘automatic’).” This can be seen in the following plot.3\n\nearthquakes %>%\n  mutate(earthquake_month = floor_date(time, \"month\")) %>%\n  group_by(earthquake_month, status) %>%\n  summarize(earthquakes = n(), .groups = \"drop\") %>%\n  ggplot(aes(x = earthquake_month, y = earthquakes, color = status)) +\n  geom_line()\n\n\n\n\n\nearthquakes %>%\n  filter(mag >= 6) %>%\n  group_by(place) %>%\n  summarize(earthquakes = n(), .groups = \"drop\") %>%\n  arrange(desc(earthquakes)) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nplace\nearthquakes\n\n\n\n\noffshore Bio-Bio, Chile\n9\n\n\nVanuatu\n8\n\n\nSolomon Islands\n7\n\n\n\n\n\nFor the next query, it seems easy enough to just put the result in a plot.\n\nearthquakes %>%\n  filter(mag >= 6) %>%\n  mutate(place = if_else(grepl(' of ', place),\n                         split_part(place, ' of ', 2L), \n                         place)) %>%\n  count(place, name = \"earthquakes\") %>%\n  arrange(desc(earthquakes)) %>%\n  collect(n = 10) %>%\n  ggplot(aes(y = fct_inorder(place), \n             x = earthquakes)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n6.2.3 Anomalies from the Absence of Data"
  },
  {
    "objectID": "chapter_6.html#handling-anomalies",
    "href": "chapter_6.html#handling-anomalies",
    "title": "6  Anomaly Detection",
    "section": "6.3 Handling Anomalies",
    "text": "6.3 Handling Anomalies\n\n6.3.1 Investigation\n\n\n6.3.2 Removal\n\nearthquakes %>%\n  filter(!mag %in% c(-9,-9.99)) %>%\n  select(time, mag, type) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\ntime\nmag\ntype\n\n\n\n\n2010-01-01 00:10:43\n1.60\nearthquake\n\n\n2010-01-01 00:16:49\n2.20\nearthquake\n\n\n2010-01-01 00:16:54\n1.10\nearthquake\n\n\n2010-01-01 00:29:29\n1.40\nice quake\n\n\n2010-01-01 00:30:16\n0.75\nearthquake\n\n\n2010-01-01 00:31:40\n1.00\nearthquake\n\n\n2010-01-01 00:34:29\n0.46\nearthquake\n\n\n2010-01-01 00:55:05\n2.40\nearthquake\n\n\n2010-01-01 00:57:31\n0.60\nearthquake\n\n\n2010-01-01 01:09:53\n0.20\nearthquake\n\n\n\n\n\n\nearthquakes %>%\n  summarize(avg_mag = avg(mag),\n            avg_mag_adjusted = avg(if_else(mag > -9, mag, NA))) %>%\n  kable()\n\n\n\n\navg_mag\navg_mag_adjusted\n\n\n\n\n1.711903\n1.713898\n\n\n\n\n\n\nearthquakes %>%\n  filter(place == 'Yellowstone National Park, Wyoming') %>%\n  summarize(avg_mag = avg(mag),\n            avg_mag_adjusted = avg(if_else(mag > -9, mag, NA))) %>%\n  kable()\n\n\n\n\navg_mag\navg_mag_adjusted\n\n\n\n\n0.8140104\n0.8740535\n\n\n\n\n\n\n\n6.3.3 Replacement with Alternate Values\n\nearthquakes %>%\n  mutate(event_type = if_else(type == 'earthquake', type, 'Other')) %>%\n  count(event_type) %>%\n  kable()\n\n\n\n\nevent_type\nn\n\n\n\n\nearthquake\n98375\n\n\nOther\n1625\n\n\n\n\n\n\nextremes <-\n  earthquakes %>%\n  summarize(p95 = quantile(mag, probs = 0.95, na.rm = TRUE),\n            p05 = quantile(mag, probs = 0.05, na.rm = TRUE))\n\nextremes %>% kable()\n\n\n\n\np95\np05\n\n\n\n\n4.5\n0.31\n\n\n\n\n\nNote that this SQL from the book4\n\nCASE \n  WHEN mag > p95 THEN p95\n  WHEN mag < p05 THEN p05\n  ELSE mag\nEND AS mag_winsorized\n\ncan be replaced with a single line:\n\nLEAST(GREATEST(mag, p05), p95) AS mag_winsorized\n\nThe R equivalents of LEAST and GREATEST are pmin and pmax, respectively. And dbplyr will translate pmin and pmax for us, so we can get winsorized data as follows.\n\nearthquakes_wins <-\n  earthquakes %>%\n  mutate(mag = if_else(mag %in% c(-9.99, -9), NA, mag)) %>%\n  filter(!is.na(mag)) %>%\n  cross_join(extremes) %>%\n  mutate(mag_winsorized = pmin(pmax(mag, p05), p95)) %>%\n  select(time, place, mag, mag_winsorized) \n\nearthquakes_wins %>%\n  arrange(desc(mag)) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2010-02-27 06:34:11\noffshore Bio-Bio, Chile\n8.8\n4.5\n\n\n2010-04-06 22:15:01\nnorthern Sumatra, Indonesia\n7.8\n4.5\n\n\n2010-07-23 22:51:11\nMoro Gulf, Mindanao, Philippines\n7.6\n4.5\n\n\n\n\nearthquakes_wins %>%\n  filter(mag == mag_winsorized) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2010-01-01 00:10:43\nSouthern Alaska\n1.6\n1.6\n\n\n2010-01-01 00:16:49\nWashington\n2.2\n2.2\n\n\n2010-01-01 00:16:54\n15km E of Coso Junction, CA\n1.1\n1.1\n\n\n\n\nearthquakes_wins %>%\n  arrange(mag) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\ntime\nplace\nmag\nmag_winsorized\n\n\n\n\n2010-01-24 19:16:52\nNevada\n-1.6\n0.31\n\n\n2010-06-07 14:18:32\nMount St. Helens area, Washington\n-1.6\n0.31\n\n\n2010-06-21 14:29:38\nMount St. Helens area, Washington\n-1.6\n0.31\n\n\n\n\n\n\n\n6.3.4 Rescaling\nIn the book, it says WHERE depth >= 0.05, but I need to use WHERE depth > 0.05 to match the results there.\n\nquake_depths <-\n  earthquakes %>%\n  filter(depth > 0.05) %>%\n  mutate(depth = round(depth, 1)) %>%\n  select(depth)\n\nquake_depths %>%\n  mutate(log_depth = log(depth, base = 10)) %>%\n  count(depth, log_depth) %>%\n  arrange(depth) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\ndepth\nlog_depth\nn\n\n\n\n\n0.1\n-1.0000000\n610\n\n\n0.2\n-0.6989700\n467\n\n\n0.3\n-0.5228787\n398\n\n\n\n\nquake_depths %>%\n  ggplot(aes(x = depth)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\nquake_depths %>%\n  ggplot(aes(x = log(depth, base = 10))) +\n  geom_histogram(binwidth = 0.1)"
  },
  {
    "objectID": "chapter_7.html",
    "href": "chapter_7.html",
    "title": "7  Experiment Analysis",
    "section": "",
    "text": "I would reframe this discussion. We might be using a database because the results of an experiment are stored there.\nAn alternative approach might have the results of an experiment are extracted and exported to a CSV or Excel file and attached to an email and sent to you for analysis.1 But extracted from where? If the data are in a database, it would be better to cut out the middleman and just get the data directly.\nThe limitations of SQL essentially vanish when we access the data using dbplyr. R can do any statistical calculation we can think of, so we have no need of an online statistical calculator (though such calculators can help us frame our analyses and check our results).\nCathy does say that “many databases allow developers to extend SQL functionality with user-defined functions (UDFs) … but they are beyond the scope of this book.” For PostgreSQL there are PL/Python and PL/R, which allow creation of functions in Python and R, respectively. When I first started to use PostgreSQL, these extensions seemed pretty exciting and, because I was maintaining my own databases, I could use them. But over time, I found maintaining UDFs to be more effort than could be justified and I no longer use them. Instead, if I need to do analysis in Python or R, I will extract data from the database, do the analysis, then write data back to the database. While this likely partly reflects the kinds of analysis I do, I think that UDFs are likely to be off limits to most users because of the additional complexity of turning code into UDFs and because many users would lack sufficient database privileges to create these.\n\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\nlibrary(flextable)\nlibrary(janitor)"
  },
  {
    "objectID": "chapter_7.html#the-data-set",
    "href": "chapter_7.html#the-data-set",
    "title": "7  Experiment Analysis",
    "section": "7.2 The Data Set",
    "text": "7.2 The Data Set\nAs discussed in Tanimura (2021), there are four tables used in this chapter. We can get these data from the GitHub site associated with Tanimura (2021).\n\nurl <- paste0(\"https://raw.githubusercontent.com/\",\n              \"cathytanimura/sql_book/master/\",\n              \"Chapter%207%3A%20Experiment%20Analysis/\")\n\ngame_users <- read_csv(paste0(url, \"game_users.csv\"),\n                        col_types = \"iDc\")\n\ngame_actions <- read_csv(paste0(url, \"game_actions.csv\"),\n                         col_types = \"icD\")\n\ngame_purchases <- read_csv(paste0(url, \"game_purchases.csv\"),\n                         col_types = \"iDd\")\n\nexp_assignment <- read_csv(paste0(url, \"exp_assignment.csv\"),\n                           col_types = \"ciDc\")\n\nHaving obtained these four data tables, we replace the tibble version of each with an equivalent stored in an in-memory DuckDB database. Note that all the code below works if you skip the following five lines of code. However, in compiling a version of this chapter, I could do it in about 24 seconds without these lines (i.e., using local data frames) and in about 4 seconds with these lines (i.e., using DuckDB).2\n\ndb <- dbConnect(duckdb::duckdb())\ngame_users <- copy_to(db, game_users, \"game_users\")\ngame_actions <- copy_to(db, game_actions, \"game_actions\")\ngame_purchases <- copy_to(db, game_purchases, \"game_purchases\")\nexp_assignment <- copy_to(db, exp_assignment, \"exp_assignment\")\n\n\ndb <- dbConnect(RPostgres::Postgres())\ngame_users <- copy_to(db, game_users, \"game_users\")\ngame_actions <- copy_to(db, game_actions, \"game_actions\")\ngame_purchases <- copy_to(db, game_purchases, \"game_purchases\")\nexp_assignment <- copy_to(db, exp_assignment, \"exp_assignment\")"
  },
  {
    "objectID": "chapter_7.html#types-of-experiments",
    "href": "chapter_7.html#types-of-experiments",
    "title": "7  Experiment Analysis",
    "section": "7.3 Types of Experiments",
    "text": "7.3 Types of Experiments\nI would reword the first paragraph here to the following for clarity (edits in italics):\n\nThere is a wide range of experiments, If you can change something that a user, customer, constituent, or other entity experiences, you can in theory test the effect of that change on some outcome.\n\n\n7.3.1 Experiments with Binary Outcomes: The Chi-Squared Test\nTo better match the approach of the book, I essentially create the contingency table in the database. An alternative approach would have been to collect() after summarize() and then do the statistical analysis in R. In fact, many of the functions in R are better set-up for this approach. However, this means bring more data into R and doing more calculation in R. If the experiment is very large or you would rather the database server do more of the work, then the approach below may be preferred.\n\ncont_tbl <-\n  exp_assignment %>%\n  left_join(game_actions, by = \"user_id\") %>%\n  group_by(variant, user_id) %>%\n  summarize(completed = \n              coalesce(any(action == \"onboarding complete\", na.rm = TRUE),\n                       FALSE),\n            .groups = \"drop\") %>%\n  count(variant, completed) %>%\n  mutate(completed = if_else(completed, \"Yes\", \"No\")) %>%\n  pivot_wider(names_from = completed, values_from = n) %>%\n  collect()\n\nUsing the packages janitor and flextable, I mimic the nicely formatted output shown in the book:\n\ncont_tbl %>%\n  adorn_totals(c(\"row\", \"col\")) %>%\n  mutate(`% Complete` = prettyNum(Yes/Total * 100, digits = 4)) %>%\n  flextable() %>%\n  add_header_row(values=c(\"\", \"Completed onboarding\", \"\"),\n                 colwidths = c(1, 2, 2))\n\n\nCompleted onboardingvariantNoYesTotal% Completecontrol13629362682.465239e-319Infvariant 111995382802.483915e-319InfTotal25624745484.949154e-319Inf\n\n\nNow I can do the Chi-squared test. I need to turn variant into the row names of the contingency table so that we want a simple \\(2 \\times 2\\) numeric table as the input to our statistical test and I use column_to_rownames() to this end. I then pipe the result into the base R function chisq.test(). I specified correct = FALSE so that my result matched what I got from the online calculator I found. I then display the Chi-squared statistic and the \\(p\\)-value.\n\nres <- \n  cont_tbl %>%\n  column_to_rownames(var = \"variant\") %>%\n  chisq.test(correct = FALSE)\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\nres$statistic\n\nX-squared \n      NaN \n\nres$p.value\n\n[1] NaN\n\n\n\n\n7.3.2 Experiments with Continuous Outcomes: The t-Test\n\namounts <- \n  exp_assignment %>%\n  filter(exp_name == 'Onboarding') %>%\n  left_join(game_purchases, by = \"user_id\") %>%\n  group_by(variant, user_id) %>%\n  summarize(amount = sum(coalesce(amount, 0), na.rm = TRUE),\n            .groups = \"drop\")\n\nt_test_stats <-\n  amounts %>%\n  group_by(variant) %>%\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) %>%\n  collect()\n\nt_test_stats %>%\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n50275\n3.688\n19.22\n\n\ncontrol\n49897\n3.781\n18.94\n\n\n\n\n\nWe can make a small function that we can pass a data frame to as df. The calculations assume that df contains two rows (one for each group) and columns named mean, sd, and n for the mean, standard deviation, and number of observations, respectively, in each group.\n\nt_test <- function(df) {\n  mean_diff = abs(df$mean[1] - df$mean[2])\n  se_diff <- sqrt(sum(df$sd^2 / df$n))\n  t_stat <- mean_diff / se_diff\n  p <- pt(t_stat, df = sum(df$n))\n  p_val <- 2 * min(p, 1 - p)\n  return(list(\"statistic\" = t_stat, \"p-value\" = p_val))\n}\n\nt_test(t_test_stats)\n\n$statistic\n[1] 0.7771625\n\n$`p-value`\n[1] 1\n\n\nThese values line up with those obtained from the online calculator I found.\nAn alternative approach would be to collect() the underlying data and do the \\(t\\)-test in R.\n\nt_test_data <-\n  amounts %>%\n  select(variant, amount) %>%\n  collect()\n\nt.test(formula = amount ~ variant, data = t_test_data)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by variant\nt = 0.77655, df = 100165, p-value = 0.4374\nalternative hypothesis: true difference in means between group control and group variant 1 is not equal to 0\n95 percent confidence interval:\n -0.1426893  0.3299478\nsample estimates:\n  mean in group control mean in group variant 1 \n               3.781218                3.687589 \n\n\n\nt_test_stats_2 <-\n  amounts %>%\n  inner_join(game_actions, by = \"user_id\") %>%\n  filter(action == \"onboarding complete\") %>%\n  group_by(variant) %>%\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) %>%\n  collect()\n\nt_test_stats_2 %>%\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n38280\n4.843\n21.899\n\n\ncontrol\n36268\n5.202\n22.049\n\n\n\n\n\n\nt_test(t_test_stats_2)\n\n$statistic\n[1] 2.230487\n\n$`p-value`\n[1] 1"
  },
  {
    "objectID": "chapter_7.html#challenges-with-experiments-and-options-for-rescuing-flawed-experiments",
    "href": "chapter_7.html#challenges-with-experiments-and-options-for-rescuing-flawed-experiments",
    "title": "7  Experiment Analysis",
    "section": "7.4 Challenges with Experiments and Options for Rescuing Flawed Experiments",
    "text": "7.4 Challenges with Experiments and Options for Rescuing Flawed Experiments\n\n7.4.1 Variant Assignment\n\n\n7.4.2 Outliers\n\nexp_assignment %>%\n  left_join(game_purchases, by = \"user_id\", keep = TRUE,\n            suffix = c(\"\", \".y\")) %>%\n  inner_join(game_actions, by = \"user_id\") %>%\n  filter(action == \"onboarding complete\",\n         exp_name == 'Onboarding') %>%\n  group_by(variant) %>%\n  summarize(total_cohorted = n_distinct(user_id),\n            purchasers = n_distinct(user_id.y),\n            .groups = \"drop\") %>%\n  mutate(pct_purchased = purchasers * 100.0 / total_cohorted) %>%\n  kable(digits = 2)\n\n\n\n\nvariant\ntotal_cohorted\npurchasers\npct_purchased\n\n\n\n\ncontrol\n36268\n4988\n13.75\n\n\nvariant 1\n38280\n4981\n13.01\n\n\n\n\n\n\n\n7.4.3 Time Boxing\n\namounts_boxed <- \n  exp_assignment %>%\n  filter(exp_name == 'Onboarding') %>%\n  mutate(exp_end = exp_date + days(7)) %>%\n  left_join(game_purchases, \n            by = join_by(user_id, exp_end >= purch_date)) %>%\n  group_by(variant, user_id) %>%\n  summarize(amount = sum(coalesce(amount, 0), na.rm = TRUE),\n            .groups = \"drop\")\n\n\nt_test_stats_boxed <-\n  amounts_boxed %>%\n  group_by(variant) %>%\n  summarize(n = n(),\n            mean = mean(amount, na.rm = TRUE),\n            sd = sd(amount, na.rm = TRUE)) %>%\n  collect()\n\nt_test_stats_boxed %>%\n  kable(digits = 3)\n\n\n\n\nvariant\nn\nmean\nsd\n\n\n\n\nvariant 1\n50275\n1.352\n5.613\n\n\ncontrol\n49897\n1.369\n5.766\n\n\n\n\n\n\nt_test(t_test_stats_boxed)\n\n$statistic\n[1] 0.4949663\n\n$`p-value`\n[1] 1\n\n\n\n\n7.4.4 Pre-Post Analysis\nThe original SQL query is something like this, but we make some tweaks to get this to work more naturally using dbplyr.\n\nSELECT \n  CASE WHEN a.created BETWEEN '2020-01-13' AND '2020-01-26' THEN 'pre'\n     WHEN a.created BETWEEN '2020-01-27' AND '2020-02-09' THEN 'post'\n     END AS variant,\n  count(distinct a.user_id) AS cohorted,\n  count(distinct b.user_id) AS opted_in,\n  count(distinct b.user_id) * 1.0 / count(DISTINCT a.user_id) AS pct_optin,\n  count(distinct a.created) AS days\nFROM game_users a\nLEFT JOIN game_actions b ON a.user_id = b.user_id \n  AND b.action = 'email_optin'\nWHERE a.created BETWEEN '2020-01-13' AND '2020-02-09'\nGROUP BY 1\n;\n\nThe first tweak is to translate the b.action = 'email_optin' part of the join into a filter, which we embed in the left_join() portion of our code (mirroring how it works in the SQL). The second tweak is to move the pct_optin out of the grouped aggregation portion of the query, as in the SQL it is referring to what we will later have as opted_in and cohorted. (In fact, I don’t bother with `pct_optin at all, as it’s easy to calculate when we use it in making a table.)\nNote that COUNT(DISTINCT col) becomes n_distinct(col). We use keep = TRUE and suffix = c(\"\", \".y\") to store what is b.user_id in the SQL as user_id.y.\nWe calculate not_opted_in so that our table is better prepared for the chisq.test() we will pass it to later on.\n\nopt_in_df <-\n  game_users %>%\n  filter(between(created, \n                 as.Date('2020-01-13'), \n                 as.Date('2020-02-09'))) %>%\n  left_join(game_actions %>% \n              filter(action == 'email_optin'), by = \"user_id\",\n            keep = TRUE,\n            suffix = c(\"\", \".y\")) %>%\n  mutate(variant = if_else(created <= '2020-01-26', 'pre', 'post')) %>%\n  group_by(variant) %>%\n  summarize(cohorted = n_distinct(user_id),\n            opted_in = n_distinct(user_id.y),\n            days = n_distinct(created),\n            .groups = \"drop\") %>%\n  mutate(not_opted_in = cohorted - opted_in) %>%\n  collect()\n\n\nopt_in_df %>%\n  select(variant, days, opted_in, not_opted_in, cohorted) %>%\n  adorn_totals(c(\"row\")) %>%\n  mutate(`% opted in` = prettyNum(100.0 * opted_in/cohorted, digits = 4)) %>%\n  rename(Yes = opted_in, No = not_opted_in, Total = cohorted) %>%\n  flextable() %>%\n  add_header_row(values=c(\"\", \"Opted in\", \"\"),\n                 colwidths = c(2, 2, 2))\n\n\nOpted invariantdaysYesNoTotal% opted inpost1411220163972761740.63pre1414489101732466258.75Total2825709265705227949.18\n\n\n\nres <- \n  opt_in_df %>% \n  select(-cohorted, -days) %>%\n  column_to_rownames(var = \"variant\") %>%\n  chisq.test(correct = FALSE)\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\nres$statistic\n\nX-squared \n      NaN \n\nres$p.value\n\n[1] NaN\n\n\n\n\n7.4.5 Natural experiment analysis\n\nby_country <- \n  game_users %>%\n  filter(country %in% c('United States', 'Canada')) %>%\n  left_join(game_purchases, by = \"user_id\",\n            keep = TRUE,\n            suffix = c(\"\", \".y\")) %>%\n  group_by(country) %>%\n  summarize(cohorted = n_distinct(user_id),\n            opted_in = n_distinct(user_id.y),\n            .groups = \"drop\") %>%\n  mutate(pct_purchased = 1.0 * opted_in/cohorted) %>%\n  collect()\n\nby_country %>% kable()\n\n\n\n\ncountry\ncohorted\nopted_in\npct_purchased\n\n\n\n\nCanada\n20179\n5011\n0.2483275\n\n\nUnited States\n45012\n4958\n0.1101484\n\n\n\n\n\n\nres <- \n  by_country %>% \n  select(-pct_purchased) %>%\n  column_to_rownames(var = \"country\") %>%\n  chisq.test(correct = FALSE)\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\nres$statistic\n\nX-squared \n      NaN \n\nres$p.value\n\n[1] NaN\n\n\nTo avoid a warning, we disconnect from the database once we’re done with it.\n\ndbDisconnect(db, shutdown = TRUE)\n\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "chapter_8.html",
    "href": "chapter_8.html",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "",
    "text": "library(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(knitr)\n\n\npg <- dbConnect(RPostgres::Postgres(), \n                bigint = \"integer\",\n                check_interrupts = TRUE)"
  },
  {
    "objectID": "chapter_8.html#code-organization",
    "href": "chapter_8.html#code-organization",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.2 Code Organization",
    "text": "8.2 Code Organization\n\nearthquakes <- tbl(pg, \"earthquakes\")\nlegislators_terms <- tbl(pg, \"legislators_terms\")\nvideogame_sales <- tbl(pg, \"videogame_sales\")\n\n\nearthquakes %>%\n  filter(date_part('year', time) >= 2019,\n         between(mag, 0, 1)) %>%\n  mutate(place = case_when(grepl('CA', place) ~ 'California',\n                           grepl('AK', place) ~ 'Alaska',\n                           TRUE ~ trim(split_part(place, ',', 2L)))) %>%\n  count(place, type, mag) %>%\n  arrange(desc(n)) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nplace\ntype\nmag\nn\n\n\n\n\nAlaska\nearthquake\n1.00\n4824\n\n\nAlaska\nearthquake\n0.90\n4153\n\n\nAlaska\nearthquake\n0.80\n3219\n\n\nCalifornia\nearthquake\n0.56\n2631\n\n\nAlaska\nearthquake\n0.70\n2061\n\n\nNevada\nearthquake\n1.00\n1688\n\n\nNevada\nearthquake\n0.80\n1681\n\n\nNevada\nearthquake\n0.90\n1669\n\n\nAlaska\nearthquake\n0.60\n1464\n\n\nCalifornia\nearthquake\n0.55\n1444\n\n\n\n\n\n\n8.2.1 Commenting\n\n\n8.2.2 Capitalization, Indentation, Parentheses, and Other Formatting Tricks\n\n\n8.2.3 Storing Code"
  },
  {
    "objectID": "chapter_8.html#organizing-computations",
    "href": "chapter_8.html#organizing-computations",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.3 Organizing Computations",
    "text": "8.3 Organizing Computations\n\n8.3.1 Understanding Order of SQL Clause Evaluation\nIn general, I think the order of evaluation is more intuitive when writing SQL using dbplyr. It is perhaps more confusing to someone who has written a lot of SQL without thinking about the order things are evaluated (e.g., WHERE comes early in evaluation, but relatively late in the SQL).\nIn dplyr the meaning of filter() is usually clear by where it is placed. Some times it is translated into WHERE and some times it is HAVING. In the example below, a WHERE-like filter would be placed just after legislators_terms (much as it is evaluated by the SQL query engine), while in this case we have filter being translated into HAVING because it is using the result of GROUP BY query. One hardly need give this much thought.\n\nterms_by_states <-\n  legislators_terms %>%\n  group_by(state) %>%\n  summarize(terms = n()) %>%\n  filter(terms >= 1000) %>%\n  arrange(desc(terms))\n\nterms_by_states %>%\n  show_query()\n\n<SQL>\nSELECT \"state\", COUNT(*) AS \"terms\"\nFROM \"legislators_terms\"\nGROUP BY \"state\"\nHAVING (COUNT(*) >= 1000.0)\nORDER BY \"terms\" DESC\n\nterms_by_states %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nstate\nterms\n\n\n\n\nNY\n4159\n\n\nPA\n3252\n\n\nOH\n2239\n\n\nCA\n2121\n\n\nIL\n2011\n\n\nTX\n1692\n\n\nMA\n1667\n\n\nVA\n1648\n\n\nNC\n1351\n\n\nMI\n1284\n\n\n\n\n\nThe way dplyr code is written makes it easy to look at the output each step in the series of pipes. In the query below, we can easily highlight the code up to the end of any pipe and evaluate it to see if it is doing what we want and expect it be doing. In this specific case, I find the dplyr code to be more intuitive than the SQL provided in the book, which uses an `a\n\nlegislators_terms %>%\n  group_by(state) %>%\n  summarize(terms = n(), .groups = \"drop\") %>%\n  mutate(avg_terms = mean(terms, na.rm = TRUE)) %>%\n  collect(n = 10) %>%\n  kable(digits = 2)\n\n\n\n\nstate\nterms\navg_terms\n\n\n\n\nDK\n16\n746.83\n\n\nND\n170\n746.83\n\n\nNV\n177\n746.83\n\n\nOH\n2239\n746.83\n\n\nGU\n24\n746.83\n\n\nNY\n4159\n746.83\n\n\nHI\n122\n746.83\n\n\nIN\n1177\n746.83\n\n\nNE\n379\n746.83\n\n\nWV\n428\n746.83\n\n\n\n\n\n\nlegislators_terms %>%\n  group_by(state) %>%\n  summarize(terms = n(), .groups = \"drop\") %>%\n  window_order(desc(terms)) %>%\n  mutate(rank = row_number()) %>%\n  arrange(rank) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nstate\nterms\nrank\n\n\n\n\nNY\n4159\n1\n\n\nPA\n3252\n2\n\n\nOH\n2239\n3\n\n\nCA\n2121\n4\n\n\nIL\n2011\n5\n\n\nTX\n1692\n6\n\n\nMA\n1667\n7\n\n\nVA\n1648\n8\n\n\nNC\n1351\n9\n\n\nMI\n1284\n10\n\n\n\n\n\n\n\n8.3.2 Subqueries\nIn writing dbplyr code, it is more natural to think in terms of CTEs, even though the code you write will generally be translated into SQL using subqueries.\nThe query in the book written with LATERAL seems much more confusing to me than the following. (Also, adding EXPLAIN to each query suggests that LATERAL is more complicated for PostgreSQL.) I rewrote the LATERAL query using CTEs and got the following, which seems closer to the second SQL query included in the book.\n\nWITH \n\ncurrent_legislators AS (\n  SELECT distinct id_bioguide, party\n  FROM legislators_terms\n  WHERE term_end > '2020-06-01'),\n  \nparty_changers AS (\n  SELECT b.id_bioguide, min(term_start) as first_term\n  FROM legislators_terms b\n  INNER JOIN current_legislators AS a\n  ON b.id_bioguide = a.id_bioguide AND b.party <> a.party\n  GROUP BY 1)\n\nSELECT date_part('year', first_term) as first_year, party,\n  count(id_bioguide) as legislators\nFROM current_legislators\nINNER JOIN party_changers\nUSING (id_bioguide)\nGROUP BY 1, 2;\n\n\n3 records\n\n\nfirst_year\nparty\nlegislators\n\n\n\n\n1979\nRepublican\n1\n\n\n2011\nLibertarian\n1\n\n\n2015\nDemocrat\n1\n\n\n\n\n\nTranslating the CTE version into dbplyr is a piece of cake.\n\ncurrent_legislators <-\n  legislators_terms %>%\n  filter(term_end > '2020-06-01') %>%\n  distinct(id_bioguide, party)\n\nparty_changers <-\n  legislators_terms %>%\n  inner_join(current_legislators, \n             join_by(id_bioguide)) %>%\n  filter(party.x != party.y) %>%\n  group_by(id_bioguide) %>%\n  summarize(first_term = min(term_start, na.rm = TRUE), .groups = \"drop\")\n\ncurrent_legislators %>%\n  inner_join(party_changers, by = \"id_bioguide\") %>%\n  mutate(first_year = date_part('year', first_term)) %>%\n  group_by(first_year, party) %>%\n  summarize(legislators = n(), .groups = \"drop\") %>%\n  collect() %>%\n  kable()\n\n\n\n\nfirst_year\nparty\nlegislators\n\n\n\n\n1979\nRepublican\n1\n\n\n2011\nLibertarian\n1\n\n\n2015\nDemocrat\n1\n\n\n\n\n\n\n\n8.3.3 Temporary Tables\nCreating temporary tables with dbplyr is easy: simply append compute() at the end of the table definition. Generally, dbplyr will take care of details that likely do not matter much, such as choosing a name for the table (we don’t care because we can refer to the table below as current_legislators regardless of the name chosen for it).\n\ncurrent_legislators %>%\n  show_query()\n\n<SQL>\nSELECT DISTINCT \"id_bioguide\", \"party\"\nFROM \"legislators_terms\"\nWHERE (\"term_end\" > '2020-06-01')\n\ncurrent_legislators <-\n  legislators_terms %>%\n  filter(term_end > '2020-06-01') %>%\n  distinct(id_bioguide, party) %>%\n  compute()\n\ncurrent_legislators %>%\n  show_query()\n\n<SQL>\nSELECT *\nFROM \"dbplyr_001\"\n\n\nCreating temporary tables can lead to significant performance gains in some situations, as the query optimizer has a simpler object to work with. Note that dbplyr allows the creating of an index with temporary tables, which can improve performance even further.\nNot that some database administrators do not allow users to create temporary tables. In such cases, you can often use collect() followed by copy_inline() effectively. (This is also useful when you have data outside the database—so collect() is not relevant—but want to merge it with data in the database.1)\n\n\n8.3.4 Common Table Expressions\nWe have been using them throughout the book already. For the sake of completeness, I rewrite the query given in the book here.\n\nfirst_term <- \n  legislators_terms %>%\n  group_by(id_bioguide) %>%\n  summarize(first_term = min(term_start, na.rm = TRUE),\n            .groups = \"drop\")\n    \nfirst_term %>%\n  inner_join(legislators_terms, by = \"id_bioguide\") %>%\n  mutate(periods = date_part('year', age(term_start, first_term))) %>%\n  group_by(periods) %>%\n  summarize(cohort_retained = n_distinct(id_bioguide)) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nperiods\ncohort_retained\n\n\n\n\n0\n12518\n\n\n1\n3600\n\n\n2\n3619\n\n\n\n\n\n\n\n8.3.5 grouping sets\nI don’t think there is a “pure” dbplyr way of doing these. However, not all is lost for the dedicated dbplyr user, as the following examples demonstrate.\n\nglobal_sales <-\n  tbl(pg, sql(\"\n    SELECT platform, genre, publisher,\n      sum(global_sales) as global_sales\n    FROM videogame_sales\n    GROUP BY grouping sets (platform, genre, publisher)\"))\n\nglobal_sales %>%\n  arrange(desc(global_sales)) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nplatform\ngenre\npublisher\nglobal_sales\n\n\n\n\nNA\nNA\nNintendo\n1786.56\n\n\nNA\nAction\nNA\n1751.18\n\n\nNA\nSports\nNA\n1330.93\n\n\nPS2\nNA\nNA\n1255.64\n\n\nNA\nNA\nElectronic Arts\n1110.32\n\n\nNA\nShooter\nNA\n1037.37\n\n\nX360\nNA\nNA\n979.96\n\n\nPS3\nNA\nNA\n957.84\n\n\nNA\nRole-Playing\nNA\n927.37\n\n\nWii\nNA\nNA\n926.71\n\n\n\n\n\n\nglobal_sales_cube <-\n  tbl(pg, sql(\"\n    SELECT coalesce(platform, 'All') as platform,\n      coalesce(genre,'All') AS genre,\n      coalesce(publisher,'All') AS publisher,\n      sum(global_sales) AS global_sales\n    FROM videogame_sales\n    GROUP BY cube (platform, genre, publisher)\"))\n\nglobal_sales_cube %>%\n  arrange(platform, genre, publisher) %>%\n  collect(n = 10) %>%\n  kable()\n\n\n\n\nplatform\ngenre\npublisher\nglobal_sales\n\n\n\n\n2600\nAction\n20th Century Fox Video Games\n1.72\n\n\n2600\nAction\nActivision\n4.64\n\n\n2600\nAction\nAll\n29.34\n\n\n2600\nAction\nAnswer Software\n0.50\n\n\n2600\nAction\nAtari\n7.68\n\n\n2600\nAction\nAvalon Interactive\n0.17\n\n\n2600\nAction\nBomb\n0.22\n\n\n2600\nAction\nCBS Electronics\n0.31\n\n\n2600\nAction\nCPG Products\n0.54\n\n\n2600\nAction\nColeco\n1.26"
  },
  {
    "objectID": "chapter_8.html#managing-data-set-size-and-privacy-concerns",
    "href": "chapter_8.html#managing-data-set-size-and-privacy-concerns",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.4 Managing Data Set Size and Privacy Concerns",
    "text": "8.4 Managing Data Set Size and Privacy Concerns\n\n8.4.1 Sampling with %, mod\nOne can also use random() to similar effect.\n\n\n8.4.2 Reducing Dimensionality\n\nlegislators_terms %>%\n  mutate(state_group = \n           case_when(state %in% c('CA', 'TX', 'FL', 'NY', 'PA') ~ state, \n                     TRUE ~ 'Other')) %>%\n  group_by(state_group) %>%\n  summarize(terms = n()) %>%\n  arrange(desc(terms)) %>%\n  collect(n = 6) %>%\n  kable()\n\n\n\n\nstate_group\nterms\n\n\n\n\nOther\n31980\n\n\nNY\n4159\n\n\nPA\n3252\n\n\nCA\n2121\n\n\nTX\n1692\n\n\nFL\n859\n\n\n\n\n\n\ntop_states <-\n  legislators_terms %>%\n  group_by(state) %>%\n  summarize(n_reps = n_distinct(id_bioguide), .groups = \"drop\") %>%\n  window_order(desc(n_reps)) %>%\n  mutate(rank = row_number())\n\nlegislators_terms %>%\n  inner_join(top_states, by = \"state\") %>%\n  mutate(state_group = case_when(rank <= 5 ~ state,\n                                 TRUE ~ 'Other')) %>%\n  group_by(state_group) %>%\n  summarize(terms = n_distinct(id_bioguide)) %>%\n  arrange(desc(terms)) %>%\n  collect(n = 6) %>%\n  kable()\n\n\n\n\nstate_group\nterms\n\n\n\n\nOther\n8317\n\n\nNY\n1494\n\n\nPA\n1075\n\n\nOH\n694\n\n\nIL\n509\n\n\nVA\n451\n\n\n\n\n\nNote that the CASE WHEN in the SQL in the book can be significantly simplified.\n\nWITH num_terms AS (\n  SELECT id_bioguide, count(term_id) as terms\n    FROM legislators_terms\n    GROUP BY 1)\n    \nSELECT terms >= 2 AS two_terms_flag,\n  count(*) as legislators\nFROM num_terms\nGROUP BY 1;\n\n\n2 records\n\n\ntwo_terms_flag\nlegislators\n\n\n\n\nFALSE\n4139\n\n\nTRUE\n8379\n\n\n\n\n\n\nnum_terms <- \n  legislators_terms %>%\n  group_by(id_bioguide) %>%\n  summarize(terms = n(), .groups = \"drop\")\n\nnum_terms %>%\n  mutate(two_terms_flag = terms >= 2) %>%\n  count(two_terms_flag) %>%\n  collect() %>%\n  kable()\n\n\n\n\ntwo_terms_flag\nn\n\n\n\n\nFALSE\n4139\n\n\nTRUE\n8379\n\n\n\n\n\nNow we can reuse the num_terms lazy table we created above.\n\nnum_terms %>%\n  mutate(terms_level = case_when(terms >= 10 ~ '10+',\n                                 terms >= 2 ~ '2 - 9',\n                                 TRUE ~ '1')) %>%\n  count(terms_level) %>%\n  collect() %>%\n  kable()\n\n\n\n\nterms_level\nn\n\n\n\n\n1\n4139\n\n\n2 - 9\n7496\n\n\n10+\n883\n\n\n\n\n\n\n\n8.4.3 PII and Data Privacy"
  },
  {
    "objectID": "chapter_8.html#conclusion",
    "href": "chapter_8.html#conclusion",
    "title": "8  Creating Complex Data Sets for Analysis",
    "section": "8.5 Conclusion",
    "text": "8.5 Conclusion"
  },
  {
    "objectID": "chapter_9.html",
    "href": "chapter_9.html",
    "title": "9  Conclusion",
    "section": "",
    "text": "library(tidyverse)\nlibrary(DBI)\nlibrary(knitr)\nlibrary(dbplyr)"
  },
  {
    "objectID": "chapter_9.html#funnel-analysis",
    "href": "chapter_9.html#funnel-analysis",
    "title": "9  Conclusion",
    "section": "9.1 Funnel Analysis",
    "text": "9.1 Funnel Analysis\nThe issue described in this section of Tanimura (2021) is more difficult to get to using dbplyr. This seems the natural translation of the ideas represented in the SQL there.\n\nusers %>%\n  left_join(step_one, by = \"user_id\",\n            keep = TRUE, suffix = c(\"\", \"_1\")) %>%\n  left_join(step_two, by = \"user_id\",\n            keep = TRUE, suffix = c(\"\", \"_2\")) %>%\n  summarize(all_users = n_distinct(user_id),\n            step_one_users = n_distinct(user_id_1),\n            step_two_users = n_distinct(user_id_2)) %>%\n  mutate(pct_step_one = step_one_users/all_users,\n         pct_one_to_two = step_two_users / step_one_users)\n\nThis yields the second of the two options provided, which seems to be the implicitly preferred one if we want to capture all step_two_users—even if some customers go directly to step_two—though it does make pct_one_to_two a bit more difficult to interpret in such cases."
  },
  {
    "objectID": "chapter_9.html#churn-lapse-and-other-definitions-of-departure",
    "href": "chapter_9.html#churn-lapse-and-other-definitions-of-departure",
    "title": "9  Conclusion",
    "section": "9.2 Churn, Lapse, and Other Definitions of Departure",
    "text": "9.2 Churn, Lapse, and Other Definitions of Departure\nThis query seems easier to follow with a separate window specification (WINDOW w AS) and using a CTE.\n\nWITH gap_intervals AS (\n  SELECT id_bioguide, term_start,\n    lag(term_start) OVER w AS prev,\n    age(term_start, lag(term_start) OVER w) as gap_interval\n  FROM legislators_terms\n  WHERE term_type = 'rep'\n  WINDOW w AS (partition BY id_bioguide \n               ORDER BY term_start))\n\nSELECT avg(gap_interval) AS avg_gap\nFROM gap_intervals\nWHERE gap_interval IS NOT NULL;\n\n\n1 records\n\n\navg_gap\n\n\n\n\n2 years 2 mons 17 days 15:41:54.83805\n\n\n\n\n\nTranslating this query into dplyr is straightforward, with again most of the work being done in creation of gap_intervals. Note that we can refer to prev created in an earlier part of the mutate call in creating gap_interval. It seems that dbplyr detects this kind of situation and cleverly puts the calculation of prev into a subquery.\n\ngap_intervals <-\n  legislators_terms %>%\n  filter(term_type == 'rep') %>%\n  group_by(id_bioguide) %>%\n  window_order(term_start) %>%\n  mutate(prev = lag(term_start),\n         gap_interval = age(term_start, prev)) %>%\n  ungroup() %>%\n  select(id_bioguide, term_start, prev, gap_interval)\n\nThis makes it effectively a single line of code to transform the data into the average shown above.\n\ngap_intervals %>%\n  summarize(avg_gap = mean(gap_interval, na.rm = TRUE)) %>%\n  kable()\n\n\n\n\navg_gap\n\n\n\n\n2 years 2 mons 17 days 15:41:54.83805\n\n\n\n\n\nWith gap_intervals in our pocket, the following query is much simpler than the SQL shown in the book.\n\ngap_months <-\n  gap_intervals %>%\n  mutate(gap_months = year(gap_interval) * 12 + month(gap_interval)) %>%\n  count(gap_months, name = \"instances\") %>%\n  arrange(gap_months) %>%\n  ungroup()\n\ngap_months %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\ngap_months\ninstances\n\n\n\n\n1\n25\n\n\n2\n4\n\n\n3\n2\n\n\n\n\n\nThe following plot better matches what is shown in the book, where it seems the caption is incorrect.\n\ngap_months %>%\n  filter(between(gap_months, 14, 49)) %>%\n  ggplot(aes(x = gap_months, y = instances)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nlatest_date <- \n  legislators_terms %>% \n  summarize(max = max(term_start)) %>%\n  pull()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\nintervals <-\n  legislators_terms %>%\n  filter(term_type == 'rep') %>%\n  group_by(id_bioguide) %>%\n  summarize(max_date = max(term_start), .groups = \"drop\") %>%\n  mutate(interval_since_last = age(latest_date, max_date))\n\nintervals %>%\n  mutate(years_since_last = year(interval_since_last)) %>%\n  count(years_since_last) %>%\n  arrange(years_since_last) %>%\n  collect(n = 3) %>%\n  kable()\n\n\n\n\nyears_since_last\nn\n\n\n\n\n0\n6\n\n\n1\n440\n\n\n2\n1\n\n\n\n\n\n\nintervals %>%\n  mutate(months_since_last = year(interval_since_last) * 12 +\n           month(interval_since_last)) %>%\n  count(months_since_last, name = \"reps\") %>%\n  mutate(status = case_when(months_since_last <= 23 ~ 'Current',\n                            months_since_last <= 48 ~ 'Lapsed',\n                            TRUE ~ 'Churned')) %>%\n  group_by(status) %>%\n  summarize(total_reps = sum(reps, na.rm = TRUE)) %>%\n  kable()\n\n\n\n\nstatus\ntotal_reps\n\n\n\n\nChurned\n10685\n\n\nCurrent\n446\n\n\nLapsed\n105"
  },
  {
    "objectID": "chapter_9.html#basket-analysis",
    "href": "chapter_9.html#basket-analysis",
    "title": "9  Conclusion",
    "section": "9.3 Basket Analysis",
    "text": "9.3 Basket Analysis\nThis seems like a case where the programmability of R and the SQL-generation capability of dbplyr to address this more comprehensively. To make this tangible, let’s make some data and put it in the database:\n\npurchases <- tribble(\n  ~customer_id, ~product,\n  1, \"bananas\",\n  2, \"apples\",\n  2, \"oranges\",\n  3, \"apples\",\n  3, \"oranges\",\n  3, \"bananas\",\n  4, \"bananas\",\n  4, \"apples\",\n  4, \"oranges\",\n  4, \"passionfruit\") %>%\n  mutate(customer_id = as.integer(customer_id)) %>%\n  copy_inline(pg, .)\n\nNow, let’s make a function that performs as much of the analysis in the database as possible. The only data we bring into R relates to the distinct number of products purchased by customers. So let’s say some customers purchase one product, others two, three, or four, while some customers purchase six products. In this case n_prods = c(1, 2, 3, 4, 6) and that is the only data we need in R from the database. If p = 2 (i.e., we’re interested in product pairs), we then create all the combinations of 2 products for each value in n_prods. For a customer with one product, there is no pair. For a customer with two products, there is one pair. For a customer with three products, there are two pairs: {1, 2} and {2, 3}. These combinations are created in R and the resulting table (combos) is passed back to the database.\nFrom there, everything is processed in the database.\nIn creating cust_prods, each unique product purchased by each customer is given a within-customer ID (prod_num) using the row_number() window function, and note is made of the number of unique products purchased by the customer (n_products). We then join cust_prods with combos using (customer_id, n_products) to create base, which contains customer IDs, as well as the “baskets” of products represented in the form of prod_num values stored in p_1, p_2, etc.\nNext, we want to add the p actual products in each basket from cust_prods to the data stored in base. For example, we want to turn p_1 of 1 into product_1 of \"apples\". The get_prod() function does this for one product at a time and we use Reduce from base R to repeatedly apply this idea p times.\nFinally, we collapse the product_i fields into a single basket field analogous to what is done in Tanimura (2021).\n\nget_baskets <- function(df, p) {\n  \n  get_combos <- function(n, p) {\n    if (n < p) return(NULL)\n    combos <- t(matrix(combn(sort(n), p), nrow = p))\n    colnames(combos) <- paste0(\"p_\", 1:p)\n    combos <- as_tibble(combos)\n    combos$row_num <- 1:nrow(combos)\n    cross_join(tibble(n_products = n), combos)\n  }\n\n  n_prods <- \n    df %>%\n    distinct(customer_id, product) %>%\n    group_by(customer_id) %>%\n    summarize(n_products = n()) %>%\n    select(n_products) %>%\n    distinct() %>%\n    pull()\n\n  combos <-\n    n_prods %>% \n    lapply(get_combos, p = p) %>%\n    bind_rows() %>%\n    copy_inline(pg, .)\n\n  cust_prods <-\n    df %>% \n    group_by(customer_id) %>% \n    window_order(product) %>% \n    mutate(prod_num = row_number()) %>%\n    ungroup() %>%\n    group_by(customer_id) %>% \n    mutate(n_products = n()) %>%\n    ungroup()\n\n  base <- \n    cust_prods %>%\n    select(customer_id, n_products) %>%\n    distinct() %>%\n    inner_join(combos, by = join_by(n_products)) %>%\n    select(-n_products)\n\n  get_prod <- function(i) {\n    cust_prods %>%\n      rename_with(function(x) paste0(\"p_\", i), \"prod_num\") %>%\n      inner_join(base) %>%\n      rename_with(function(x) paste0(\"product_\", i), \"product\") %>%\n      select(customer_id, row_num, starts_with(\"product_\"))\n  }\n\n  Reduce(inner_join, lapply(1:p, get_prod)) %>%\n    arrange(customer_id, row_num) %>%\n    mutate(basket = \n             sql(paste0(\"concat(\", paste0(\"product_\", 1:p, collapse = \", ', ', \"), \")\"))) %>%\n    compute() %>%\n    select(customer_id, basket)\n}\n\nNow we can apply our function to our data to get two-item baskets.\n\npairs <- get_baskets(purchases, 2) \n\npairs %>% kable()\n\n\n\n\ncustomer_id\nbasket\n\n\n\n\n2\napples, oranges\n\n\n3\napples, bananas\n\n\n3\napples, oranges\n\n\n3\nbananas, oranges\n\n\n4\napples, bananas\n\n\n4\napples, oranges\n\n\n4\napples, passionfruit\n\n\n4\nbananas, oranges\n\n\n4\nbananas, passionfruit\n\n\n4\noranges, passionfruit\n\n\n\n\n\nAnd we can identify the most popular baskets easily:\n\npairs %>%\n  count(basket) %>%\n  arrange(desc(n)) %>%\n  kable()\n\n\n\n\nbasket\nn\n\n\n\n\napples, oranges\n3\n\n\nbananas, oranges\n2\n\n\napples, bananas\n2\n\n\napples, passionfruit\n1\n\n\noranges, passionfruit\n1\n\n\nbananas, passionfruit\n1\n\n\n\n\n\nBut the function also works for three-item baskets …\n\nget_baskets(purchases, 3) %>% kable()\n\n\n\n\ncustomer_id\nbasket\n\n\n\n\n3\napples, bananas, oranges\n\n\n4\napples, bananas, oranges\n\n\n4\napples, bananas, passionfruit\n\n\n4\napples, oranges, passionfruit\n\n\n4\nbananas, oranges, passionfruit\n\n\n\n\n\n… and four-item baskets too.\n\nget_baskets(purchases, 4) %>% kable()\n\n\n\n\ncustomer_id\nbasket\n\n\n\n\n4\napples, bananas, oranges, passionfruit"
  },
  {
    "objectID": "chapter_9.html#resources",
    "href": "chapter_9.html#resources",
    "title": "9  Conclusion",
    "section": "9.4 Resources",
    "text": "9.4 Resources"
  },
  {
    "objectID": "chapter_9.html#final-thoughts",
    "href": "chapter_9.html#final-thoughts",
    "title": "9  Conclusion",
    "section": "9.5 Final Thoughts",
    "text": "9.5 Final Thoughts\n\n\n\n\nTanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Tanimura, C. 2021. SQL for Data Analysis. O’Reilly Media. https://books.google.com.au/books?id=ojhCEAAAQBAJ."
  }
]